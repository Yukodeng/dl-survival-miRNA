{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pycox: DeepSurv Stratified by Batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nfs/dengy/dl-survival-miRNA'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# os.chdir(\"dl-survival-miRNA\") \n",
    "# os.chdir(\"../\")\n",
    "from pss.pycox.models import CoxPH, CoxPHStratified, StratifiedDataset\n",
    "from pss.pycox.evaluation.eval_surv import EvalSurv\n",
    "from pss.utils import load_simulate_survival_data\n",
    "from pss.run_models import DeepSurvPipeline, train_over_subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Test: Debugging*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Network code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "def cox_ph_loss_sorted(log_h: Tensor, events: Tensor, eps: float = 1e-7) -> Tensor:\n",
    "    \"\"\"Requires the input to be sorted by descending duration time.\n",
    "    See DatasetDurationSorted.\n",
    "\n",
    "    We calculate the negative log of $(\\frac{h_i}{\\sum_{j \\in R_i} h_j})^d$,\n",
    "    where h = exp(log_h) are the hazards and R is the risk set, and d is event.\n",
    "\n",
    "    We just compute a cumulative sum, and not the true Risk sets. This is a\n",
    "    limitati`on, but simple and fast.\n",
    "    \"\"\"\n",
    "    if events.dtype is torch.bool:\n",
    "        events = events.float()\n",
    "    events = events.view(-1)\n",
    "    log_h = log_h.view(-1)\n",
    "    if events.sum() == 0:\n",
    "        return log_h.sum() * 0.0  # update 08/11/25: safe dummy loss\n",
    "    \n",
    "    gamma = log_h.max()\n",
    "    log_cumsum_h = log_h.sub(gamma).exp().cumsum(0).add(eps).log().add(gamma)\n",
    "    return - log_h.sub(log_cumsum_h).mul(events).sum().div(events.sum())\n",
    "\n",
    "\n",
    "def cox_ph_loss(log_h: Tensor, durations: Tensor, events: Tensor, eps: float = 1e-7) -> Tensor:\n",
    "    \"\"\"Loss for CoxPH model. If data is sorted by descending duration, see `cox_ph_loss_sorted`.\n",
    "\n",
    "    We calculate the negative log of $(\\frac{h_i}{\\sum_{j \\in R_i} h_j})^d$,\n",
    "    where h = exp(log_h) are the hazards and R is the risk set, and d is event.\n",
    "\n",
    "    We just compute a cumulative sum, and not the true Risk sets. This is a\n",
    "    limitation, but simple and fast.\n",
    "    \"\"\"\n",
    "    idx = durations.sort(descending=True)[1]\n",
    "    events = events[idx]\n",
    "    log_h = log_h[idx]\n",
    "    return cox_ph_loss_sorted(log_h, events, eps)\n",
    "\n",
    "\n",
    "####### [UPDATE] 07/07/2025\n",
    "def stratified_cox_ph_loss(log_h: Tensor, durations: Tensor, events: Tensor, batch_indices: Tensor, eps: float = 1e-7) -> Tensor:\n",
    "    \"\"\"\n",
    "    Stratified CoxPH loss that computes partial likelihood across batches.\n",
    "\n",
    "    Arguments:\n",
    "        log_h {torch.Tensor} -- Log hazard predictions for each instance.\n",
    "        durations {torch.Tensor} -- Duration times for each instance.\n",
    "        events {torch.Tensor} -- Event indicators (1 if event, 0 if censored).\n",
    "        batch_indices {numpy array} -- Batch labels for each instance.\n",
    "        eps {float} -- Small epsilon for numerical stability.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor -- The total stratified negative log partial likelihood.\n",
    "    \"\"\"\n",
    "    device = batch_indices.device\n",
    "    unique_batches = torch.unique(batch_indices)\n",
    "    losses = torch.zeros(len(unique_batches), device=device)\n",
    "    n_valid_batch = 0\n",
    "        \n",
    "    for i, batch in enumerate(unique_batches):\n",
    "        # Select data for the current batch\n",
    "        mask = (batch_indices == batch)\n",
    "        if mask.sum() == 0 or events[mask].sum() == 0:\n",
    "            continue  # skip empty batch (added 08/11/25) or batch with no events\n",
    "        \n",
    "        # Sort by descending durations\n",
    "        idx = torch.argsort(durations[mask], descending=True)\n",
    "        \n",
    "        events_batch = events[mask][idx]\n",
    "        log_h_batch = log_h[mask][idx]\n",
    "        if events_batch.sum() == 0:\n",
    "            continue \n",
    "        \n",
    "        losses[i] = cox_ph_loss_sorted(log_h_batch, events_batch, eps)\n",
    "        n_valid_batch += 1\n",
    "        \n",
    "    if n_valid_batch == 0:\n",
    "        return log_h.sum() * 0.0\n",
    "    # print(n_valid_batch)\n",
    "    return losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch tensor\n",
    "batch_indices = torch.tensor([1, 2, 2, 2, 2, 2, 3, 3, 4, 4], dtype=torch.float32)\n",
    "durations = torch.tensor([169.5, 0.6, 12.3, 1.5, 3.8, 0.1, 0.1, 0.1, 0.6, 0.1], dtype=torch.float32)\n",
    "events = torch.tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.float32)\n",
    "log_h = torch.tensor([-4.1238, 2.1188, -1.5863, -1.2239, 0.9088, 5.6637, 1.2920, 4.5356, 1.5392, 5.0004], dtype=torch.float32)\n",
    "\n",
    "# Test out ufnction\n",
    "device = batch_indices.device\n",
    "unique_batches = torch.unique(batch_indices)\n",
    "losses = torch.zeros(len(unique_batches), device=device)\n",
    "\n",
    "for i, batch in enumerate(unique_batches):\n",
    "    # i = 1\n",
    "    # batch = 1\n",
    "    # print(i)\n",
    "    mask = (batch_indices == batch)\n",
    "    if mask.sum() == 0:\n",
    "        print(f\"batch {batch} is empty\")\n",
    "        continue\n",
    "    idx = torch.argsort(durations[mask], descending=True)\n",
    "    # idx = durations[mask].sort(descending=True)[1]\n",
    "    log_h_batch = log_h[mask][idx]\n",
    "    events_batch = events[mask][idx]\n",
    "    \n",
    "    print(events_batch)\n",
    "    if events_batch.sum() == 0:\n",
    "        print(f\"batch {int(batch)} has no events\")\n",
    "        continue\n",
    "    \n",
    "    losses[i] = cox_ph_loss_sorted(log_h_batch, events_batch, eps=1e-7)\n",
    "    \n",
    "losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_cox_ph_loss(log_h, durations, events, batch_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dimensions: (90000, 541)\n",
      "Testing data dimensions:  (10000, 541)\n"
     ]
    }
   ],
   "source": [
    "batchNormType='BE00Asso00_normTMM'\n",
    "dataType = 'linear-moderate'\n",
    "keywords = ['061825']\n",
    "test_size=10000\n",
    "random_state=42\n",
    "time_col='time'\n",
    "status_col='status'\n",
    "batch_col='batch.id'\n",
    "\n",
    "train_df, test_df = load_simulate_survival_data(batchNormType=batchNormType,\n",
    "                                                dataName=dataType,\n",
    "                                                keywords=keywords, \n",
    "                                                keep_batch=True)\n",
    "\n",
    "print(f\"Training data dimensions: {train_df.shape}\")\n",
    "print(f\"Testing data dimensions:  {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([1000, 538])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "def _preprocess_data(df, mapper=None, fit_scaler=True):\n",
    "    survival_cols = [time_col, status_col]\n",
    "    covariate_cols = [col for col in df.columns if col not in survival_cols]\n",
    "    # Transform features (miRNA expression)\n",
    "    if fit_scaler or mapper is None:\n",
    "        standardize = [([col], StandardScaler()) for col in covariate_cols]\n",
    "        mapper = DataFrameMapper(standardize)\n",
    "        x = mapper.fit_transform(df[covariate_cols]).astype('float32')\n",
    "    else:\n",
    "        x = mapper.transform(df[covariate_cols]).astype('float32')\n",
    "    # Prepare labels (survival data)\n",
    "    y = (df[time_col].values, df[status_col].values)\n",
    "    \n",
    "    return x, y, mapper\n",
    "\n",
    "train_sub,_ = train_test_split(train_df,\n",
    "                            train_size=2000, \n",
    "                            shuffle=True, random_state=42,\n",
    "                            stratify=train_df[[status_col, batch_col]])\n",
    "test_sub, _ = train_test_split(test_df,\n",
    "                            train_size=1000, \n",
    "                            shuffle=True, random_state=42,\n",
    "                            stratify=test_df[[status_col, batch_col]])\n",
    "\n",
    "batch_ids_train = train_sub[batch_col].to_numpy().reshape(-1)\n",
    "batch_ids_test = test_sub[[batch_col]].to_numpy().reshape(-1)\n",
    "\n",
    "train_sub = train_sub.drop(columns=[batch_col])\n",
    "test_sub = test_sub.drop(columns=[batch_col])\n",
    "\n",
    "x_train, y_train, mapper = _preprocess_data(train_sub, fit_scaler=True)\n",
    "x_test, y_test, _ = _preprocess_data(test_sub, mapper=mapper, fit_scaler=False)\n",
    "\n",
    "durations_train, events_train = y_train[0], y_train[1]\n",
    "durations_test, events_test = y_test[0], y_test[1]\n",
    "\n",
    "# Prepare data \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = torch.from_numpy(x_train).to(device)\n",
    "x_test = torch.from_numpy(x_test).to(device)\n",
    "\n",
    "durations_train = torch.from_numpy(durations_train).float().to(device)\n",
    "durations_test = torch.from_numpy(durations_test).float().to(device)\n",
    "events_train = torch.from_numpy(events_train).float().to(device)\n",
    "events_test = torch.from_numpy(events_test).float().to(device)\n",
    "y_train = (durations_train, events_train)\n",
    "y_test = (durations_test, events_test)\n",
    "        \n",
    "batch_ids_train = torch.from_numpy(batch_ids_train).long().to(device)\n",
    "batch_ids_test = torch.from_numpy(batch_ids_test).long().to(device)\n",
    "\n",
    "print(device)\n",
    "print(x_test.shape)           # Should be [n_samples, n_features]\n",
    "# print(y_test.shape)           # Should be [n_samples, n_features]\n",
    "print(durations_test.shape)   # Should be [n_samples]\n",
    "print(events_test.shape)      # Should be [n_samples]\n",
    "print(batch_ids_test.shape)   # Should be [n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.shape\n",
    "input_size = x_train.shape[1]\n",
    "output_size = 1\n",
    "num_nodes = [32,16]            # Default # layers & nodes\n",
    "dropout = 0.2                    # Default dropout rate\n",
    "learning_rate = 1e-3      # Default learning rate\n",
    "batch_size = 128               # Default batch size\n",
    "epochs = 500                      # Default number of epochs\n",
    "batch_norm = True             # Default batch normalization\n",
    "output_bias = True           # Default output bias\n",
    "weight_decay = 1e-4         # Default weight decay\n",
    "activation = torch.nn.ReLU\n",
    "\n",
    "net = tt.practical.MLPVanilla(\n",
    "    in_features=input_size,\n",
    "    out_features=output_size,\n",
    "    num_nodes=num_nodes,\n",
    "    dropout=dropout, \n",
    "    batch_norm=batch_norm,\n",
    "    activation=activation,\n",
    "    output_bias=output_bias\n",
    ").to(device)\n",
    "optimizer = tt.optim.Adam(weight_decay=weight_decay, lr=learning_rate)\n",
    "\n",
    "# Get default early stopping settings if not defined \n",
    "patience = 30\n",
    "min_delta = 1e-3\n",
    "callbacks = [tt.callbacks.EarlyStopping(patience=patience, min_delta=min_delta)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoxPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoxPH model\n",
    "model = CoxPH(net, optimizer=optimizer)\n",
    "log = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks, \n",
    "    verbose=True,\n",
    "    val_data=(x_test, y_test),\n",
    "    val_batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Evaluation ====================\n",
    "_ = model.compute_baseline_hazards(input=x_train, target=(durations_train, events_train))\n",
    "\n",
    "# Convert torch tensors back to numpy objects for evaluation\n",
    "x_train_np = x_train.detach().cpu().numpy()\n",
    "x_test  = x_test.detach().cpu().numpy()\n",
    "durations_train = durations_train.detach().cpu().numpy()\n",
    "durations_test  = durations_test.detach().cpu().numpy()\n",
    "events_train    = events_train.detach().cpu().numpy()\n",
    "events_test     = events_test.detach().cpu().numpy()\n",
    "\n",
    "# Initialize EvalSurv objects \n",
    "tr_surv  = model.predict_surv_df(x_train)\n",
    "te_surv = model.predict_surv_df(x_test)\n",
    "tr_ev = EvalSurv(tr_surv, durations_train, events_train, censor_surv='km')\n",
    "te_ev = EvalSurv(te_surv, durations_test, events_test, censor_surv='km')\n",
    "\n",
    "# Concordance index ----------------\n",
    "tr_c_index  = tr_ev.concordance_td() \n",
    "te_c_index = te_ev.concordance_td() \n",
    "\n",
    "tr_c_index, te_c_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified CoxPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = StratifiedDataset(x_train, durations_train, events_train, batch_ids_train)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_dataset = torch.utils.data.TensorDataset(x_test, durations_test, events_test, batch_ids_test)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # # Access batches\n",
    "# # for idx, (inputs, durations, events, batch_ids) in enumerate(train_loader):\n",
    "# #     print(f\"Batch {idx + 1}:\")\n",
    "# #     print(\"batch ids:\", batch_ids)\n",
    "# #     # print(\"Time:\", durations)\n",
    "# #     print(\"Events:\", events)\n",
    "# #     print()\n",
    "    \n",
    "# ## Test\n",
    "# for idx, (inputs, durations, events, batch_ids) in enumerate(test_loader):\n",
    "#     print(f\"Batch {idx + 1}:\")\n",
    "#     print(\"batch ids:\", batch_ids)\n",
    "#     # print(\"Time:\", durations)\n",
    "#     print(\"Events:\", events)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL batch total events: 95 | per-stratum: {1: 11, 2: 10, 3: 10, 4: 11, 5: 6, 6: 7, 7: 7, 8: 9, 9: 7, 10: 17}\n",
      "VAL batch total events: 96 | per-stratum: {1: 13, 2: 9, 3: 11, 4: 8, 5: 7, 6: 6, 7: 9, 8: 11, 9: 10, 10: 12}\n",
      "VAL batch total events: 103 | per-stratum: {1: 8, 2: 10, 3: 6, 4: 12, 5: 14, 6: 10, 7: 11, 8: 11, 9: 15, 10: 6}\n",
      "VAL batch total events: 93 | per-stratum: {1: 13, 2: 11, 3: 11, 4: 7, 5: 4, 6: 10, 7: 11, 8: 6, 9: 8, 10: 12}\n",
      "VAL batch total events: 94 | per-stratum: {1: 13, 2: 15, 3: 12, 4: 10, 5: 10, 6: 12, 7: 4, 8: 6, 9: 7, 10: 5}\n",
      "VAL batch total events: 96 | per-stratum: {1: 8, 2: 9, 3: 7, 4: 13, 5: 15, 6: 8, 7: 11, 8: 5, 9: 12, 10: 8}\n",
      "VAL batch total events: 94 | per-stratum: {1: 8, 2: 13, 3: 4, 4: 13, 5: 6, 6: 7, 7: 16, 8: 10, 9: 9, 10: 8}\n",
      "VAL batch total events: 77 | per-stratum: {1: 11, 2: 3, 3: 6, 4: 10, 5: 8, 6: 12, 7: 6, 8: 9, 9: 5, 10: 7}\n"
     ]
    }
   ],
   "source": [
    "# Get default early stopping settings if not defined \n",
    "patience = 30\n",
    "min_delta = 1e-3\n",
    "callbacks = [tt.callbacks.EarlyStopping(patience=patience, min_delta=min_delta)]\n",
    "\n",
    "\n",
    "# train_dataset = torch.utils.data.TensorDataset(x_train, durations_train, events_train, batch_ids_train)\n",
    "# test_dataset   = torch.utils.data.TensorDataset(x_test, durations_test, events_test, batch_ids_test)\n",
    "train_dataset = StratifiedDataset(x_train, durations_train, events_train, batch_ids_train)\n",
    "test_dataset = StratifiedDataset(x_test, durations_test, events_test, batch_ids_test)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for xb, db, eb, bb in test_loader:\n",
    "    print(\"VAL batch total events:\", int(eb.sum().item()),\n",
    "          \"| per-stratum:\", {int(s): int(eb[bb==s].sum().item()) for s in bb.unique().tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 20.6112,\tval_loss: 19.5040\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 18.8728,\tval_loss: 18.8106\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 18.3032,\tval_loss: 18.4668\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 17.5007,\tval_loss: 18.1772\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 17.1657,\tval_loss: 17.9118\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 16.7562,\tval_loss: 17.6523\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 16.2896,\tval_loss: 17.4696\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 15.7985,\tval_loss: 17.2974\n",
      "8:\t[0s / 0s],\t\ttrain_loss: 15.4151,\tval_loss: 17.2267\n",
      "9:\t[0s / 0s],\t\ttrain_loss: 14.9886,\tval_loss: 16.8337\n",
      "10:\t[0s / 0s],\t\ttrain_loss: 14.4737,\tval_loss: 16.5885\n",
      "11:\t[0s / 0s],\t\ttrain_loss: 13.8276,\tval_loss: 16.5437\n",
      "12:\t[0s / 0s],\t\ttrain_loss: 13.7400,\tval_loss: 16.2938\n",
      "13:\t[0s / 1s],\t\ttrain_loss: 13.0749,\tval_loss: 15.9844\n",
      "14:\t[0s / 1s],\t\ttrain_loss: 12.8311,\tval_loss: 15.8871\n",
      "15:\t[0s / 1s],\t\ttrain_loss: 12.5133,\tval_loss: 15.6785\n",
      "16:\t[1s / 2s],\t\ttrain_loss: 12.2128,\tval_loss: 15.5450\n",
      "17:\t[0s / 3s],\t\ttrain_loss: 11.9292,\tval_loss: 15.4642\n",
      "18:\t[0s / 3s],\t\ttrain_loss: 11.5381,\tval_loss: 15.5818\n",
      "19:\t[0s / 3s],\t\ttrain_loss: 11.4502,\tval_loss: 15.2526\n",
      "20:\t[0s / 3s],\t\ttrain_loss: 11.4160,\tval_loss: 15.0398\n",
      "21:\t[0s / 3s],\t\ttrain_loss: 11.2613,\tval_loss: 14.8947\n",
      "22:\t[0s / 3s],\t\ttrain_loss: 11.2909,\tval_loss: 14.8877\n",
      "23:\t[0s / 3s],\t\ttrain_loss: 10.8115,\tval_loss: 14.8926\n",
      "24:\t[0s / 3s],\t\ttrain_loss: 10.5065,\tval_loss: 15.0167\n",
      "25:\t[0s / 3s],\t\ttrain_loss: 10.4131,\tval_loss: 14.7054\n",
      "26:\t[0s / 3s],\t\ttrain_loss: 10.6001,\tval_loss: 14.7447\n",
      "27:\t[0s / 3s],\t\ttrain_loss: 10.3592,\tval_loss: 14.5732\n",
      "28:\t[0s / 3s],\t\ttrain_loss: 10.2577,\tval_loss: 14.7732\n",
      "29:\t[0s / 3s],\t\ttrain_loss: 10.2236,\tval_loss: 14.3227\n",
      "30:\t[0s / 4s],\t\ttrain_loss: 9.5694,\tval_loss: 14.4882\n",
      "31:\t[0s / 4s],\t\ttrain_loss: 9.7481,\tval_loss: 14.5800\n",
      "32:\t[0s / 4s],\t\ttrain_loss: 9.6616,\tval_loss: 14.3989\n",
      "33:\t[0s / 4s],\t\ttrain_loss: 9.5791,\tval_loss: 14.4871\n",
      "34:\t[0s / 4s],\t\ttrain_loss: 9.5159,\tval_loss: 14.5330\n",
      "35:\t[0s / 4s],\t\ttrain_loss: 9.4044,\tval_loss: 14.7256\n",
      "36:\t[0s / 4s],\t\ttrain_loss: 9.3585,\tval_loss: 14.4294\n",
      "37:\t[0s / 4s],\t\ttrain_loss: 9.3047,\tval_loss: 14.3946\n",
      "38:\t[0s / 4s],\t\ttrain_loss: 9.0912,\tval_loss: 14.4839\n",
      "39:\t[0s / 4s],\t\ttrain_loss: 9.4056,\tval_loss: 14.4072\n",
      "40:\t[0s / 4s],\t\ttrain_loss: 9.0725,\tval_loss: 14.1628\n",
      "41:\t[0s / 4s],\t\ttrain_loss: 9.0367,\tval_loss: 14.4431\n",
      "42:\t[0s / 4s],\t\ttrain_loss: 9.2167,\tval_loss: 14.5521\n",
      "43:\t[0s / 4s],\t\ttrain_loss: 8.5082,\tval_loss: 14.3585\n",
      "44:\t[0s / 4s],\t\ttrain_loss: 9.0316,\tval_loss: 14.2568\n",
      "45:\t[0s / 5s],\t\ttrain_loss: 9.2235,\tval_loss: 14.3435\n",
      "46:\t[0s / 5s],\t\ttrain_loss: 8.9989,\tval_loss: 14.0253\n",
      "47:\t[0s / 5s],\t\ttrain_loss: 8.8814,\tval_loss: 14.2406\n",
      "48:\t[0s / 5s],\t\ttrain_loss: 8.6188,\tval_loss: 14.1227\n",
      "49:\t[0s / 5s],\t\ttrain_loss: 8.8582,\tval_loss: 14.2725\n",
      "50:\t[0s / 5s],\t\ttrain_loss: 8.6881,\tval_loss: 14.1832\n",
      "51:\t[0s / 5s],\t\ttrain_loss: 8.9002,\tval_loss: 14.3184\n",
      "52:\t[0s / 5s],\t\ttrain_loss: 8.5333,\tval_loss: 14.4059\n",
      "53:\t[0s / 5s],\t\ttrain_loss: 8.3425,\tval_loss: 14.4463\n",
      "54:\t[0s / 5s],\t\ttrain_loss: 8.3878,\tval_loss: 14.4199\n",
      "55:\t[0s / 5s],\t\ttrain_loss: 8.5439,\tval_loss: 14.5957\n",
      "56:\t[0s / 5s],\t\ttrain_loss: 8.6749,\tval_loss: 14.4996\n",
      "57:\t[0s / 5s],\t\ttrain_loss: 8.5236,\tval_loss: 14.6952\n",
      "58:\t[0s / 5s],\t\ttrain_loss: 8.3917,\tval_loss: 14.5778\n",
      "59:\t[0s / 6s],\t\ttrain_loss: 8.1516,\tval_loss: 14.3792\n",
      "60:\t[0s / 6s],\t\ttrain_loss: 8.0394,\tval_loss: 14.5075\n",
      "61:\t[0s / 6s],\t\ttrain_loss: 8.3509,\tval_loss: 14.7048\n",
      "62:\t[0s / 6s],\t\ttrain_loss: 8.2421,\tval_loss: 14.5012\n",
      "63:\t[0s / 6s],\t\ttrain_loss: 8.4988,\tval_loss: 14.5069\n",
      "64:\t[0s / 6s],\t\ttrain_loss: 8.1722,\tval_loss: 14.6033\n",
      "65:\t[0s / 6s],\t\ttrain_loss: 8.2929,\tval_loss: 14.7130\n",
      "66:\t[0s / 6s],\t\ttrain_loss: 8.0431,\tval_loss: 14.6207\n",
      "67:\t[0s / 6s],\t\ttrain_loss: 8.3263,\tval_loss: 14.5298\n",
      "68:\t[0s / 6s],\t\ttrain_loss: 8.1450,\tval_loss: 14.7182\n",
      "69:\t[0s / 6s],\t\ttrain_loss: 8.2177,\tval_loss: 14.7160\n",
      "70:\t[0s / 6s],\t\ttrain_loss: 8.0446,\tval_loss: 14.7786\n",
      "71:\t[0s / 6s],\t\ttrain_loss: 8.1454,\tval_loss: 14.6037\n",
      "72:\t[0s / 6s],\t\ttrain_loss: 7.8258,\tval_loss: 14.5364\n",
      "73:\t[0s / 6s],\t\ttrain_loss: 8.0864,\tval_loss: 14.4806\n",
      "74:\t[0s / 7s],\t\ttrain_loss: 7.8876,\tval_loss: 14.5541\n",
      "75:\t[0s / 7s],\t\ttrain_loss: 7.9933,\tval_loss: 14.6749\n",
      "76:\t[0s / 7s],\t\ttrain_loss: 7.9791,\tval_loss: 14.6880\n",
      "Training time: 7.2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Stratified CoxPH model\n",
    "model = CoxPHStratified(net, optimizer=optimizer)\n",
    "model.metrics = {'val_loss': model.loss}\n",
    "start = time.time() # Record iteration start time\n",
    "log = model.fit_dataloader(\n",
    "    train_loader,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=True,\n",
    "    val_dataloader=test_loader  # optional for now\n",
    ")\n",
    "stop = time.time() # Record time when training finished\n",
    "duration = round(stop - start, 2)\n",
    "print(f\"Training time: {duration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Stratified C-index* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9417363122787157 0.8178225734837048\n"
     ]
    }
   ],
   "source": [
    "# ==================== Evaluation ====================\n",
    "# Convert torch tensors back to numpy objects for evaluation\n",
    "durations_train_np = durations_train.detach().cpu().numpy()\n",
    "durations_test_np  = durations_test.detach().cpu().numpy()\n",
    "events_train_np    = events_train.detach().cpu().numpy()\n",
    "events_test_np     = events_test.detach().cpu().numpy()\n",
    "\n",
    "# Compute baseline hazards (per-batch)\n",
    "baseline_hazards_strata = model.compute_baseline_hazards(input=x_train, target=(durations_train, events_train), batch_ids=batch_ids_train)\n",
    "\n",
    "# Initialize EvalSurv objects \n",
    "tr_surv  = model.predict_surv_df(x_train, batch_ids = batch_ids_train)\n",
    "te_surv = model.predict_surv_df(x_test, batch_ids = batch_ids_test)\n",
    "tr_ev = EvalSurv(tr_surv, durations_train_np, events_train_np, censor_surv='km')\n",
    "te_ev = EvalSurv(te_surv, durations_test_np, events_test_np, censor_surv='km')\n",
    "\n",
    "# Concordance index ----------------\n",
    "tr_strat_c_index  = tr_ev.stratified_concordance_td(batch_indices=batch_ids_train) \n",
    "te_strat_c_index = te_ev.stratified_concordance_td(batch_indices=batch_ids_test) \n",
    "\n",
    "print(tr_strat_c_index, te_strat_c_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *One-batch C-index* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9387092920643851 0.8268118138566364\n"
     ]
    }
   ],
   "source": [
    "baseline_hazards_1batch = model.compute_baseline_hazards(input=x_train, target=(durations_train, events_train))\n",
    "\n",
    "# Initialize EvalSurv objects \n",
    "tr_surv  = model.predict_surv_df(x_train, baseline_hazards_=baseline_hazards_1batch)\n",
    "te_surv = model.predict_surv_df(x_test, baseline_hazards_=baseline_hazards_1batch)\n",
    "tr_ev = EvalSurv(tr_surv, durations_train_np, events_train_np, censor_surv='km')\n",
    "te_ev = EvalSurv(te_surv, durations_test_np, events_test_np, censor_surv='km')\n",
    "\n",
    "# Concordance index (non-stratified) ----------------\n",
    "tr_c_index, _  = tr_ev.concordance_td() \n",
    "te_c_index, _ = te_ev.concordance_td() \n",
    "print(tr_c_index, te_c_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual test\n",
    "from pycox.evaluation.concordance import concordance_td\n",
    "from pycox.evaluation import ipcw\n",
    "\n",
    "batch_indices = batch_ids_train.detach().cpu().numpy() if not isinstance(batch_ids_train, np.ndarray) else batch_ids_train\n",
    "batches = np.unique(batch_indices)\n",
    "c_index_ls , n_pairs_ls = np.zeros(len(batches)), np.zeros(len(batches))\n",
    "\n",
    "for i, batch in enumerate(batches):\n",
    "    # Filter data by batch\n",
    "    mask = (batch_indices == batch)\n",
    "    if mask.sum() == 0:\n",
    "        continue  # skip empty batch\n",
    "    batch_durations = durations_train[mask]\n",
    "    batch_events = events_train[mask]\n",
    "    batch_surv = tr_ev.surv.iloc[:, mask]\n",
    "    if batch_events.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    # Compute concordance for the current batch\n",
    "    c_index_batch, n_pairs_batch = concordance_td(\n",
    "        batch_durations, batch_events, batch_surv.values,\n",
    "        tr_ev.idx_at_times(batch_durations), method='adj_antolini'\n",
    "    )\n",
    "    print(n_pairs_batch)\n",
    "    n_pairs_ls[i] = n_pairs_batch\n",
    "    # n_events_ls[i] = batch_events.sum()\n",
    "    c_index_ls[i] = c_index_batch\n",
    "    \n",
    "print(\"Final score: %f\" % (np.sum(c_index_ls*n_pairs_ls) / np.sum(n_pairs_ls) if np.sum(n_pairs_ls) > 0 else float('nan')))\n",
    "\n",
    "for e, c in zip(n_pairs_ls, c_index_ls):\n",
    "    print(f\"{int(e)} comparable pairs: {round(c,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Test: Stratified Integrated Brier score*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated Brier score -----------\n",
    "min_surv = np.ceil(max(np.min(durations_train), np.min(durations_test)))\n",
    "max_surv = np.floor(min(np.max(durations_train), np.max(durations_test)))\n",
    "times = np.linspace(min_surv, max_surv, 20)\n",
    "\n",
    "tr_brier  = tr_ev.integrated_brier_score(time_grid=times) \n",
    "te_brier =  te_ev.integrated_brier_score(time_grid=times)\n",
    "\n",
    "print(tr_brier, te_brier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_strat_brier  = tr_ev.stratified_integrated_brier_score(time_grid=times, batch_indices=batch_ids_train) \n",
    "te_strat_brier =  te_ev.stratified_integrated_brier_score(time_grid=times, batch_indices=batch_ids_test)\n",
    "print(tr_strat_brier, te_strat_brier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tr_ev.surv.values.shape) \n",
    "# print(tr_ev.censor_surv.surv.values.shape)\n",
    "# print(tr_ev.index_surv.shape)\n",
    "# print(tr_ev.censor_surv.index_surv.shape) \n",
    "# print(tr_ev.steps)\n",
    "# print(tr_ev.censor_surv.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices = batch_ids_train.detach().numpy() if not isinstance(batch_ids_train, np.ndarray) else batch_ids_train\n",
    "batches = np.unique(batch_indices)\n",
    "brier_ls, n_events_ls = np.zeros(len(batches)), np.zeros(len(batches))\n",
    "\n",
    "for i, batch in enumerate(batches):\n",
    "    # Filter data by batch\n",
    "    mask = (batch_indices == batch)\n",
    "    if mask.sum() == 0:\n",
    "        continue  # skip empty batch\n",
    "    batch_durations = durations_train[mask]\n",
    "    batch_events = events_train[mask]\n",
    "    batch_surv = tr_ev.surv.iloc[:, mask]\n",
    "    if batch_events.sum() == 0:\n",
    "        continue\n",
    "    batch_surv_values = tr_ev.surv.values[:, mask]\n",
    "    batch_censor_surv_values = tr_ev.censor_surv.surv.values[:, mask] \n",
    "    # batch_index_surv = tr_ev.index_surv[mask]\n",
    "    # batch_censor_index_surv = tr_ev.censor_surv.index_surv[mask]\n",
    "    \n",
    "    # Compute integrated brier score for the current batch\n",
    "    brier_batch = ipcw.integrated_brier_score(times, batch_durations, batch_events, \n",
    "                                    batch_surv_values, batch_censor_surv_values, \n",
    "                                    tr_ev.index_surv, tr_ev.censor_surv.index_surv, np.inf, \n",
    "                                    tr_ev.steps, tr_ev.censor_surv.steps)\n",
    "    n_events_ls[i] = batch_events.sum()\n",
    "    brier_ls[i] = brier_batch\n",
    "    \n",
    "print(\"Final score: %f\\n\" % (np.sum(brier_ls*n_events_ls) / np.sum(n_events_ls) if np.sum(n_events_ls) > 0 else float('nan')))\n",
    "\n",
    "for e, c in zip(n_events_ls, brier_ls):\n",
    "    print(f\"{int(e)} events: {round(c,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "batchNormType='BE00Asso00_normNone'\n",
    "dataName='nl-shiftquad'\n",
    "keywords = ['061825']\n",
    "test_size=10000\n",
    "random_state=42\n",
    "time_col='time'\n",
    "status_col='status'\n",
    "batch_col='batch.id'\n",
    "\n",
    "train_df, test_df = load_simulate_survival_data(batchNormType=batchNormType,\n",
    "                                                dataName=dataName,\n",
    "                                                keywords=keywords, \n",
    "                                                keep_batch=True)\n",
    "\n",
    "print(f\"Training data dimensions: {train_df.shape}\")\n",
    "print(f\"Testing data dimensions:  {test_df.shape}\")\n",
    "plot_simulation_data(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"num_nodes\": {\"type\": \"categorical\", \"choices\": [[64,64], [32,32], [16,16]]},\n",
    "    \"dropout\": {\"type\": \"float\", \"low\": 0.1, \"high\": 0.5},\n",
    "    \"weight_decay\": {\"type\": \"float\", \"low\": 1e-5, \"high\": 1e-2, \"log\": True},\n",
    "    \"learning_rate\": {\"type\": \"float\", \"low\": 1e-4, \"high\": 1e-2, \"log\": True},\n",
    "    \"batch_size\": {\"type\": \"categorical\", \"choices\": [128, 64, 32, 16]}\n",
    "}\n",
    "\n",
    "ds = DeepSurvPipeline(\n",
    "    train_df, test_df, \n",
    "    batchNormType=batchNormType, \n",
    "    dataName=dataName,\n",
    "    hyperparameters=hyperparameters,\n",
    "    is_stratified=True\n",
    ")\n",
    "\n",
    "# optuna.logging.disable_default_handler()\n",
    "stratified_results = ds.train_over_subsets(subset_sizes=[2000],#subset_sizes, \n",
    "                                runs_per_size=[5],#runs_per_size, \n",
    "                                splits_per_size=[10],#splits_per_size,\n",
    "                                trials_per_size=[5],#trails_per_size,\n",
    "                                is_tune=True, \n",
    "                                is_save=False, \n",
    "                                n_jobs=-1,\n",
    "                                trial_threshold=5                               \n",
    ")\n",
    "stratified_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== Archive ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "folder = 'linear'\n",
    "keywords = ['moderate', \"latest\", 'RW']\n",
    "\n",
    "train_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, test_size=0.2)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_cols = ['time', 'status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df, val_df = train_test_split(train_df, \n",
    "                                test_size=0.2,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                stratify=train_df['status'])\n",
    "\n",
    "# Transform data\n",
    "covariate_cols = [col for col in train_df.columns if col not in survival_cols]\n",
    "standardize = [([col], StandardScaler()) for col in covariate_cols]\n",
    "leave = [(col, None) for col in survival_cols]\n",
    "x_mapper = DataFrameMapper(standardize)\n",
    "\n",
    "# gene expression data\n",
    "x_train = x_mapper.fit_transform(tr_df[covariate_cols]).astype('float32')\n",
    "x_val = x_mapper.fit_transform(val_df[covariate_cols]).astype('float32')\n",
    "x_test = x_mapper.transform(test_df[covariate_cols]).astype('float32')\n",
    "\n",
    "# prepare labels\n",
    "get_target = lambda df: (df['time'].values, df['status'].values)\n",
    "y_train = get_target(tr_df)\n",
    "y_val = get_target(val_df)\n",
    "t_test, e_test = get_target(test_df)\n",
    "val = x_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net\n",
    "\n",
    "We create a simple MLP with two hidden layers, ReLU activations, batch norm and dropout. \n",
    "Here, we just use the `torchtuples.practical.MLPVanilla` net to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = x_train.shape[1]\n",
    "num_nodes = [32, 16]\n",
    "out_features = 1\n",
    "batch_norm = True\n",
    "dropout = 0.2\n",
    "output_bias = True\n",
    "\n",
    "net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm,\n",
    "                            dropout, output_bias=output_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "To train the model we need to define a `torch.optim` optimizer; here we instead use one from `tt.optim` as it has some added functionality.\n",
    "We use the `Adam` optimizer and set the desired learning rate with `model.lr_finder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tt.optim.Adam(weight_decay=0.01)\n",
    "\n",
    "be_model = CoxPHStratified(net, optimizer)\n",
    "\n",
    "# we  set it manually to 0.001\n",
    "be_model.optimizer.set_lr(1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the `EarlyStopping` callback to stop training when the validation loss stops improving. After training, this callback will also load the best performing model in terms of validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = 64\n",
    "epochs = 500\n",
    "callbacks = [tt.callbacks.EarlyStopping(patience=20, min_delta=5e-2)]\n",
    "verbose = True\n",
    "\n",
    "batch_indices = np.ones(len(y_train[1]))\n",
    "log = be_model.fit(x_train, y_train,\n",
    "                batch_indices,\n",
    "                batch_size,\n",
    "                epochs,\n",
    "                callbacks, \n",
    "                verbose=verbose,\n",
    "                val_data=val, val_batch_size=batch_size\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
