{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# ! pip install sklearn-pandas\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "\n",
    "import optuna\n",
    "# from ray import tune\n",
    "# from ray.tune.schedulers import ASHAScheduler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.chdir(\"../\")\n",
    "from pycox.models.cox import CoxPH#, CoxPHStratified\n",
    "from pycox.evaluation.eval_surv import EvalSurv#, EvalurvStratified\n",
    "from scr.utils import *\n",
    "from scr.runDeepSurvModels import *\n",
    "\n",
    "np.random.seed(1234)\n",
    "_ = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hyperparameters(trial, config=None):\n",
    "    \"\"\"\n",
    "    Convert a dictionary of hyperparameter ranges into Optuna-compatible format.\n",
    "    \n",
    "    Args:\n",
    "    - trial (optuna.trial.Trial): The Optuna trial object to sample parameters from.\n",
    "    - hyperparam_dict (dict): A dictionary with hyperparameter names as keys. Values should be a dictionary specifying the type of parameter ('int', 'float', or 'categorical')\n",
    "    and the range or choices.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary of sampled hyperparameters compatible with the given ranges.\n",
    "    \"\"\"\n",
    "    # config = self.hyperparameters if config is None else config\n",
    "    params = {}\n",
    "    for param_name, param_info in config.items():\n",
    "        param_type = param_info.get(\"type\")\n",
    "        if param_type == \"int\":\n",
    "            params[param_name] = trial.suggest_int(param_name, param_info[\"low\"], param_info[\"high\"])\n",
    "        elif param_type == \"float\":\n",
    "            params[param_name] = trial.suggest_float(param_name, param_info[\"low\"], param_info[\"high\"], log=param_info.get(\"log\", False))\n",
    "        elif param_type == \"categorical\":\n",
    "            params[param_name] = trial.suggest_categorical(param_name, param_info[\"choices\"])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported hyperparameter type: {param_type}\")\n",
    "    return params\n",
    "\n",
    "\n",
    "from abc import ABC\n",
    "class DeepSurvPipeline(ABC):\n",
    "    def __init__(self, train_df, test_df, \n",
    "                time_col='time', status_col='status',\n",
    "                dataName=None, hyperparameters=None, random_state=42):\n",
    "        self.train_df=train_df\n",
    "        self.test_df=test_df\n",
    "        self.n = train_df.shape[0]\n",
    "        self.time_col = time_col\n",
    "        self.status_col = status_col\n",
    "        self.dataName = dataName\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.modelString = 'deepsurv-torch'\n",
    "    \n",
    "    def _preprocess_data(self, df):\n",
    "        \"\"\"Use StandardScaler() to transform input miRNA data.\n",
    "        Return transformed features and labels (survival time and censor status) separately.\n",
    "        \"\"\"\n",
    "        survival_cols = [self.time_col, self.status_col]\n",
    "        covariate_cols = [col for col in df.columns if col not in survival_cols]\n",
    "        standardize = [([col], StandardScaler()) for col in covariate_cols]\n",
    "        x_mapper = DataFrameMapper(standardize)\n",
    "\n",
    "        # transform features (miRNA expression)\n",
    "        x = x_mapper.fit_transform(df[covariate_cols]).astype('float32')\n",
    "        # prepare labels (survival data)\n",
    "        get_target = lambda df: (df[self.time_col].values, df[self.status_col].values)\n",
    "        y = get_target(df)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def train_mlp(self, train_df, val_df, params, patience, min_delta, verbose=False, is_tuning=False):\n",
    "        \"\"\"Training function that works for both tuning and main training.\n",
    "\n",
    "        Args:\n",
    "            x_train (pandas DataFrame): _description_\n",
    "            y_train (pandas DataFrame): _description_\n",
    "            params (dictionary): neural network training parameters\n",
    "                - 'batch_size': batch size\n",
    "                - 'epochs': number of training epochs\n",
    "                - 'num_nodes': hidden layer size\n",
    "                - 'learning_rate'\n",
    "                - 'dropout'\n",
    "                - ...\n",
    "        \"\"\"\n",
    "        # Set hyperparameters with default values\n",
    "        num_nodes = params.get(\"num_nodes\", [16,16])            # Default to [16,16] if not provided\n",
    "        dropout = params.get(\"dropout\", 0.5)                    # Default dropout rate\n",
    "        learning_rate = params.get(\"learning_rate\", 1e-3)       # Default learning rate\n",
    "        batch_size = params.get(\"batch_size\", 32)               # Default batch size\n",
    "        epochs = params.get(\"epochs\", 100)                      # Default number of epochs\n",
    "        batch_norm = params.get(\"batch_norm\", True)             # Default batch normalization\n",
    "        output_bias = params.get(\"output_bias\", True)           # Default output bias\n",
    "        \n",
    "        # Prepare data \n",
    "        x_train, y_train = self._preprocess_data(train_df)\n",
    "        val = self._preprocess_data(val_df)\n",
    "        x_val, y_val = val\n",
    "        input_size = x_train.shape[1]\n",
    "        output_size = 1\n",
    "        \n",
    "        # =================== Build Neural Net ===================\n",
    "        ## define network\n",
    "        net = tt.practical.MLPVanilla(in_features=input_size,\n",
    "                                    out_features=output_size,\n",
    "                                    num_nodes=num_nodes,\n",
    "                                    dropout=dropout, \n",
    "                                    batch_norm=batch_norm,\n",
    "                                    # activation=nn.ReLu,\n",
    "                                    output_bias=output_bias)\n",
    "        # define optimizer \n",
    "        optimizer = tt.optim.Adam(weight_decay=0.01)\n",
    "        model = CoxPH(net, optimizer)\n",
    "        model.optimizer.set_lr(learning_rate)\n",
    "\n",
    "        # =================== Train Model =====================\n",
    "        callbacks = [tt.callbacks.EarlyStopping(patience=patience, min_delta=min_delta)]\n",
    "        \n",
    "        start = time.time() # Record iteration start time\n",
    "        log = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callbacks, \n",
    "                        verbose=verbose,\n",
    "                        val_data=val,\n",
    "                        val_batch_size=batch_size\n",
    "                        )\n",
    "        stop = time.time() # Record time when training finished\n",
    "        duration = round(stop - start, 2)\n",
    "        \n",
    "        # =================== Evaluation ===================\n",
    "        _ = model.compute_baseline_hazards()\n",
    "        val_surv = model.predict_surv_df(x_val)\n",
    "        val_c_index  = EvalSurv(val_surv, y_val[0], y_val[1], censor_surv='km').concordance_td() \n",
    "        \n",
    "        tr_surv = model.predict_surv_df(x_train)\n",
    "        tr_c_index  = EvalSurv(tr_surv, y_train[0], y_train[1], censor_surv='km').concordance_td() \n",
    "        \n",
    "        if not is_tuning:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "            \n",
    "        return duration, tr_c_index, val_c_index\n",
    "    \n",
    "    \n",
    "    def _objective(self, trial, train_df, config, fixed_params=None, n_splits=5):\n",
    "        \"\"\"Perform K-Fold CV and return the average validation loss across folds.\"\"\" \n",
    "        \n",
    "        tunable_params = parse_hyperparameters(trial, config=config)\n",
    "        \n",
    "        full_params = {**fixed_params, **tunable_params} if fixed_params else tunable_params\n",
    "        \n",
    "        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=self.random_state)\n",
    "        val_c_indexes = []\n",
    "\n",
    "        for train_index, val_index in kf.split(train_df, train_df[self.status_col]):\n",
    "            tr_df, val_df = train_df.iloc[train_index,:], train_df.iloc[val_index,:]\n",
    "\n",
    "            # Train and evaluate on the current fold -- validation score\n",
    "            *_, val_c_index = self.train_mlp(tr_df, val_df,\n",
    "                                            params=full_params,\n",
    "                                            patience=10, min_delta=1e-2,\n",
    "                                            verbose=False,\n",
    "                                            is_tuning=True)\n",
    "            val_c_indexes.append(val_c_index)\n",
    "        \n",
    "        return np.mean(val_c_indexes)\n",
    "    \n",
    "    \n",
    "    def tune_hyperparameters(self, train_df, \n",
    "                            config=None,\n",
    "                            storage_url=None, study_name=None,\n",
    "                            trial_threshold=20,\n",
    "                            n_splits=5, n_trials=20, n_jobs=2):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter tuning with K-Fold CV, tuning only parameters \n",
    "        not already optimized in the existing study.\n",
    "        \n",
    "        Args:\n",
    "            train_df: Training dataset.\n",
    "            config: Search grid for hyperparameter tuning.\n",
    "            storage_url: URL for Optuna storage.\n",
    "            study_name: Name of the study.\n",
    "            n_splits: Number of splits for cross-validation.\n",
    "            n_trials: Number of trials to run for tuning.\n",
    "            n_jobs: Number of parallel jobs for tuning.\n",
    "            trial_threshold: Minimum number of trials required to skip tuning.\n",
    "        \"\"\"\n",
    "        storage_url = \"sqlite:///deepsurv-torch-hp-log.db\" if storage_url is None else storage_url\n",
    "        study_name = f\"{self.dataName}-{self.train_df.shape[0]}\" if study_name is None else study_name\n",
    "        \n",
    "        # Create an Optuna study for hyperparameter optimization\n",
    "        study = optuna.create_study(direction='maximize', \n",
    "                                    storage=storage_url, study_name=study_name, load_if_exists=True)\n",
    "        \n",
    "        # Define search space\n",
    "        config = self.hyperparameters if config is None else config\n",
    "        if config is None:\n",
    "            raise ValueError(\"No hyperparameter search space provided. Set `config` or `self.hyperparameters`.\")\n",
    "\n",
    "        # Extract already optimized hyperparameters and prepare new hyperparameter search grid\n",
    "        fixed_params = study.best_params if len(study.trials) >= trial_threshold else {}\n",
    "        new_config = {k: v for k, v in config.items() if k not in fixed_params}\n",
    "\n",
    "        if len(new_config) == 0:\n",
    "            print(\"All hyperparameters already tuned. Skipping optimization...\")\n",
    "            self.best_params = fixed_params\n",
    "            return study\n",
    "            \n",
    "        # Optimize the study using the objective function\n",
    "        print(f\"Tuning parameters: {new_config}\")\n",
    "        print(f\"Fixed parameters during tuning: {fixed_params}\")\n",
    "        \n",
    "        study.optimize(lambda trial: self._objective(trial,\n",
    "                                                    train_df=train_df,\n",
    "                                                    config=new_config,\n",
    "                                                    fixed_params=fixed_params,\n",
    "                                                    n_splits=n_splits),\n",
    "                            n_trials=n_trials, \n",
    "                            n_jobs=n_jobs)\n",
    "        \n",
    "        # Save the best hyperparameters\n",
    "        self.best_params = {**fixed_params, **study.best_params}\n",
    "        print(f\"Best trial parameters: {self.best_params}\")\n",
    "        return study\n",
    "        \n",
    "        \n",
    "    def train_with_best_params(self, val_size=0.2, \n",
    "                            params=None, \n",
    "                            patience=25, min_delta=1e-2,\n",
    "                            verbose=True, print_scores=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            subset (_type_): _description_\n",
    "            batch_sizes (_type_): _description_\n",
    "            val_size (float, optional): _description_. Defaults to 0.2.\n",
    "            kwargs (_type_, optional): _description_. Defaults to None.\n",
    "            time_col (str, optional): _description_. Defaults to 'time'.\n",
    "            status_col (str, optional): _description_. Defaults to 'status'.\n",
    "            verbose (bool, optional): _description_. Defaults to True.\n",
    "            print_scores (bool, optional): _description_. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        if params is None and self.best_params is None:\n",
    "            raise ValueError(\"No hyperparameters found. Run tune_hyperparameters() first or define them manually.\")\n",
    "        elif params is None:\n",
    "            params = self.best_params\n",
    "        \n",
    "        # ===================== Prepare Data =======================\n",
    "        tr_df, val_df = train_test_split(self.train_df, \n",
    "                                        test_size=val_size,\n",
    "                                        shuffle=True, random_state=self.random_state,\n",
    "                                        stratify=self.train_df[self.status_col])\n",
    "        \n",
    "        # ===================== Train Model =====================\n",
    "        duration, train_c, _ = self.train_mlp(tr_df, val_df, params=params, \n",
    "                                            patience=patience, min_delta=min_delta,\n",
    "                                            verbose=verbose, is_tuning=False)\n",
    "\n",
    "        # ===================== Evaluation =====================\n",
    "        x_test, y_test = self._preprocess_data(self.test_df)\n",
    "        te_surv = self.model.predict_surv_df(x_test)\n",
    "        test_c  = EvalSurv(te_surv, y_test[0], y_test[1], censor_surv='km').concordance_td()\n",
    "        \n",
    "        if print_scores:\n",
    "            print(f\"N={self.n} Training time ({duration}s): Train C-Index: {round(train_c,3)} | Test C-index: {round(test_c,3)}\")\n",
    "            \n",
    "        return duration, train_c, test_c\n",
    "    \n",
    "    \n",
    "    def write(self, model_results, out_dir=None, fileName='model.results.txt'):\n",
    "        \n",
    "        out_dir=os.path.join('models', self.dataName, self.modelString) if out_dir is None else out_dir\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        # save results as txt or csv file\n",
    "        if 'txt' in fileName:\n",
    "            model_results.to_csv(os.path.join(out_dir,fileName), sep='\\t')\n",
    "        elif 'csv' in fileName:\n",
    "            model_results.to_csv(os.path.join(out_dir,fileName), index=False)\n",
    "        else:\n",
    "            print('Please specify a file name with either a txt or csv extension.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Test: Hyperparameter Search*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dimensions: (18000, 1035)\n",
      "Testing data dimensions:  (2000, 1035)\n"
     ]
    }
   ],
   "source": [
    "## Parameter setup\n",
    "# Load data\n",
    "folder = 'linear'\n",
    "keywords = ['moderate', \"latest\", 'RW']\n",
    "DATANAME = 'linear-moderate'\n",
    "train_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, test_size=0.1)\n",
    "\n",
    "print(f\"Training data dimensions: {train_df.shape}\")\n",
    "print(f\"Testing data dimensions:  {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-20 16:09:20,284] Using an existing study with name 'linear-moderate-500' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.0008244158673895692, 'dropout': 0.5}\n",
      "Tuning parameters: {'num_nodes': {'type': 'categorical', 'choices': [[32, 32], [16, 16]]}}\n",
      "Fixed parameters during tuning: {'learning_rate': 0.0008244158673895692, 'dropout': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-20 16:11:40,730] Trial 84 finished with value: 0.7529822689632015 and parameters: {'num_nodes': [16, 16]}. Best is trial 1 with value: 0.8057856397403643.\n",
      "[I 2025-01-20 16:11:41,206] Trial 86 finished with value: 0.7721897338355549 and parameters: {'num_nodes': [16, 16]}. Best is trial 1 with value: 0.8057856397403643.\n",
      "[I 2025-01-20 16:11:42,628] Trial 88 finished with value: 0.7571963385207968 and parameters: {'num_nodes': [16, 16]}. Best is trial 1 with value: 0.8057856397403643.\n",
      "[I 2025-01-20 16:11:43,594] Trial 85 finished with value: 0.7600363231195704 and parameters: {'num_nodes': [16, 16]}. Best is trial 1 with value: 0.8057856397403643.\n",
      "[I 2025-01-20 16:11:45,180] Trial 83 finished with value: 0.759894528779274 and parameters: {'num_nodes': [16, 16]}. Best is trial 1 with value: 0.8057856397403643.\n",
      "[I 2025-01-20 16:11:52,106] Trial 87 finished with value: 0.764548444854028 and parameters: {'num_nodes': [16, 16]}. Best is trial 1 with value: 0.8057856397403643.\n",
      "[I 2025-01-20 16:11:52,364] Trial 81 finished with value: 0.7625403753881679 and parameters: {'num_nodes': [16, 16]}. Best is trial 1 with value: 0.8057856397403643.\n",
      "[I 2025-01-20 16:11:52,720] Trial 89 finished with value: 0.7642078305724087 and parameters: {'num_nodes': [16, 16]}. Best is trial 1 with value: 0.8057856397403643.\n",
      "[I 2025-01-20 16:11:52,781] Trial 90 finished with value: 0.7698900411204216 and parameters: {'num_nodes': [16, 16]}. Best is trial 1 with value: 0.8057856397403643.\n",
      "[I 2025-01-20 16:11:54,339] Trial 82 finished with value: 0.7647585034358484 and parameters: {'num_nodes': [16, 16]}. Best is trial 1 with value: 0.8057856397403643.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0008244158673895692, 'dropout': 0.5}\n",
      "N=500 Training time (1.56s): Train C-Index: 0.898 | Test C-index: 0.777\n"
     ]
    }
   ],
   "source": [
    "n = 500\n",
    "batch_size =32\n",
    "hyperparams = {\n",
    "    \"learning_rate\": {\"type\": \"float\", \"low\": 1e-4, \"high\": 1e-2, \"log\": True},\n",
    "    \"num_nodes\": {\"type\": \"categorical\", \"choices\": [[32,32], [16,16]]},\n",
    "    \"dropout\": {\"type\": \"categorical\", \"choices\": [0.1,0.5]},\n",
    "}\n",
    "status_col='status'\n",
    "time_col = \"time\"\n",
    "\n",
    "# study = optuna.load_study(study_name=f\"{DATANAME}-{n}\", storage=\"sqlite:///deepsurv-torch-hp-log.db\")\n",
    "# print(study.best_params)\n",
    "\n",
    "train_sub,_ = train_test_split(train_df,\n",
    "                            train_size=n/train_df.shape[0], \n",
    "                            shuffle=True, random_state=42,\n",
    "                            stratify=train_df[status_col])\n",
    "\n",
    "ds = DeepSurvPipeline(train_sub, test_df, hyperparameters=hyperparams, dataName=DATANAME)\n",
    "study = ds.tune_hyperparameters(train_sub, n_trials=10, n_jobs=16, trial_threshold=10)\n",
    "best_params = study.best_params\n",
    "\n",
    "duration, train_c, test_c = ds.train_with_best_params(params=best_params, verbose=False, print_scores=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-19 12:59:10,979] Using an existing study with name 'linear-moderate-2000' instead of creating a new one.\n",
      "[I 2024-12-19 13:04:21,747] Trial 15 finished with value: 0.8022942951215862 and parameters: {'learning_rate': 0.006009978743102076}. Best is trial 6 with value: 0.8123028530262433.\n",
      "[I 2024-12-19 13:05:27,827] Trial 13 finished with value: 0.8062255286746003 and parameters: {'learning_rate': 0.004559020639706925}. Best is trial 6 with value: 0.8123028530262433.\n",
      "[I 2024-12-19 13:05:49,559] Trial 9 finished with value: 0.8127500640404941 and parameters: {'learning_rate': 0.002201511520589423}. Best is trial 9 with value: 0.8127500640404941.\n",
      "[I 2024-12-19 13:06:18,305] Trial 10 finished with value: 0.8109925257893711 and parameters: {'learning_rate': 0.0009558039133670892}. Best is trial 9 with value: 0.8127500640404941.\n",
      "[I 2024-12-19 13:06:23,216] Trial 8 finished with value: 0.8140903684040733 and parameters: {'learning_rate': 0.0011600637818254633}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2024-12-19 13:06:42,768] Trial 17 finished with value: 0.8132234323332849 and parameters: {'learning_rate': 0.0008559998357304268}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2024-12-19 13:08:20,446] Trial 11 finished with value: 0.8125169219168932 and parameters: {'learning_rate': 0.00032139843142640396}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2024-12-19 13:08:49,260] Trial 12 finished with value: 0.8068800159738704 and parameters: {'learning_rate': 0.00016377310904255898}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2024-12-19 13:08:52,557] Trial 14 finished with value: 0.812145563177426 and parameters: {'learning_rate': 0.00019851380155305872}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2024-12-19 13:08:55,030] Trial 16 finished with value: 0.8060663659822318 and parameters: {'learning_rate': 0.00012027410909892253}. Best is trial 8 with value: 0.8140903684040733.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0011600637818254633}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-19 13:10:17,919] A new study created in RDB with name: linear-moderate-5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=2000 Training time (5.9670000000000005s): Train C-Index: 0.854 | Test C-index: 0.802 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-19 13:20:05,774] Trial 5 finished with value: 0.7927337377421871 and parameters: {'learning_rate': 0.006022412109524099}. Best is trial 5 with value: 0.7927337377421871.\n",
      "[I 2024-12-19 13:20:45,590] Trial 0 finished with value: 0.790737702080128 and parameters: {'learning_rate': 0.008471472207974845}. Best is trial 5 with value: 0.7927337377421871.\n",
      "[I 2024-12-19 13:22:31,622] Trial 7 finished with value: 0.8104625894619994 and parameters: {'learning_rate': 0.0014709245888834556}. Best is trial 7 with value: 0.8104625894619994.\n",
      "[I 2024-12-19 13:25:12,338] Trial 2 finished with value: 0.8141047517610334 and parameters: {'learning_rate': 0.0006463855250705586}. Best is trial 2 with value: 0.8141047517610334.\n",
      "[I 2024-12-19 13:25:33,151] Trial 6 finished with value: 0.8143067819181269 and parameters: {'learning_rate': 0.0006632800819145778}. Best is trial 6 with value: 0.8143067819181269.\n",
      "[I 2024-12-19 13:26:09,271] Trial 9 finished with value: 0.8143609530003116 and parameters: {'learning_rate': 0.0007371054431740679}. Best is trial 9 with value: 0.8143609530003116.\n",
      "[I 2024-12-19 13:26:40,369] Trial 4 finished with value: 0.8148163147073626 and parameters: {'learning_rate': 0.00036212251189812105}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2024-12-19 13:27:18,963] Trial 8 finished with value: 0.8147040587968231 and parameters: {'learning_rate': 0.00031192665225630314}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2024-12-19 13:27:36,199] Trial 1 finished with value: 0.8120633684688425 and parameters: {'learning_rate': 0.00013403255385413735}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2024-12-19 13:27:38,112] Trial 3 finished with value: 0.8139924501368803 and parameters: {'learning_rate': 0.00018371243347183435}. Best is trial 4 with value: 0.8148163147073626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.00036212251189812105}\n",
      "N=5000 Training time (16.205999999999996s): Train C-Index: 0.845 | Test C-index: 0.815 (Mean)\n"
     ]
    }
   ],
   "source": [
    "best_params_ls, n_train, model_time, train_scores, test_scores = [],[],[],[],[]\n",
    "\n",
    "for (n, batch_size, n_run) in zip(subset, batch_sizes, runs): \n",
    "    \n",
    "    run_train_scores, run_test_scores, run_time = [],[],[]\n",
    "    for run in range(n_run+1):\n",
    "        if n < train_df.shape[0]:\n",
    "            train_sub,_ = train_test_split(train_df,\n",
    "                                        train_size=n/train_df.shape[0], \n",
    "                                        shuffle=True, random_state=run,\n",
    "                                        stratify=train_df[status_col])\n",
    "        else:\n",
    "            train_sub = train_df\n",
    "        \n",
    "        ds = DeepSurvPipeline(train_sub, test_df, hyperparameters=hyperparams, dataName=DATANAME)\n",
    "        if run == 0:\n",
    "            study = ds.tune_hyperparameters(train_sub, n_trials=10, n_jobs=16)\n",
    "            best_params = study.best_params\n",
    "            continue\n",
    "        duration, train_c, test_c = ds.train_with_best_params(params=best_params, verbose=False, print_scores=False)\n",
    "        \n",
    "        n_train.append(n)\n",
    "        model_time.append(duration)\n",
    "        train_scores.append(train_c)\n",
    "        test_scores.append(test_c)\n",
    "            \n",
    "        run_train_scores.append(train_c)\n",
    "        run_test_scores.append(test_c)\n",
    "        run_time.append(duration)\n",
    "    \n",
    "    print(f\"N={n} Training time ({np.mean(run_time)}s): Train C-Index: {round(np.mean(run_train_scores),3)} | Test C-index: {round(np.mean(run_test_scores),3)} (Mean)\")                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Save results ===============\n",
    "model_results = pd.DataFrame({\n",
    "    'n train': n_train, \n",
    "    'train time':model_time,\n",
    "    'train score':train_scores, \n",
    "    'test score':test_scores}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Test: Train with Best Parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 3.5403,\tval_loss: 3.2964\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 3.4134,\tval_loss: 3.2791\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 3.3461,\tval_loss: 3.2570\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 3.3030,\tval_loss: 3.2236\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 3.2657,\tval_loss: 3.1785\n",
      "5:\t[0s / 1s],\t\ttrain_loss: 3.2341,\tval_loss: 3.1687\n",
      "6:\t[0s / 1s],\t\ttrain_loss: 3.2070,\tval_loss: 3.1396\n",
      "7:\t[0s / 1s],\t\ttrain_loss: 3.1874,\tval_loss: 3.1003\n",
      "8:\t[0s / 1s],\t\ttrain_loss: 3.1596,\tval_loss: 3.0747\n",
      "9:\t[0s / 1s],\t\ttrain_loss: 3.1537,\tval_loss: 3.0915\n",
      "10:\t[0s / 2s],\t\ttrain_loss: 3.1609,\tval_loss: 3.0784\n",
      "11:\t[0s / 2s],\t\ttrain_loss: 3.1334,\tval_loss: 3.0673\n",
      "12:\t[0s / 2s],\t\ttrain_loss: 3.1119,\tval_loss: 3.0606\n",
      "13:\t[0s / 2s],\t\ttrain_loss: 3.1137,\tval_loss: 3.0546\n",
      "14:\t[0s / 2s],\t\ttrain_loss: 3.1027,\tval_loss: 3.0859\n",
      "15:\t[0s / 3s],\t\ttrain_loss: 3.0951,\tval_loss: 3.0575\n",
      "16:\t[0s / 3s],\t\ttrain_loss: 3.0909,\tval_loss: 3.0622\n",
      "17:\t[0s / 3s],\t\ttrain_loss: 3.0874,\tval_loss: 3.0655\n",
      "18:\t[0s / 3s],\t\ttrain_loss: 3.0789,\tval_loss: 3.0144\n",
      "19:\t[0s / 3s],\t\ttrain_loss: 3.0873,\tval_loss: 3.0467\n",
      "20:\t[0s / 4s],\t\ttrain_loss: 3.0921,\tval_loss: 3.0298\n",
      "21:\t[0s / 4s],\t\ttrain_loss: 3.0758,\tval_loss: 3.0321\n",
      "22:\t[0s / 4s],\t\ttrain_loss: 3.0738,\tval_loss: 3.0872\n",
      "23:\t[0s / 4s],\t\ttrain_loss: 3.0612,\tval_loss: 3.0588\n",
      "24:\t[0s / 4s],\t\ttrain_loss: 3.0738,\tval_loss: 3.0438\n",
      "25:\t[0s / 5s],\t\ttrain_loss: 3.0564,\tval_loss: 3.1071\n",
      "26:\t[0s / 5s],\t\ttrain_loss: 3.0524,\tval_loss: 3.0789\n",
      "27:\t[0s / 5s],\t\ttrain_loss: 3.0507,\tval_loss: 3.0143\n",
      "28:\t[0s / 5s],\t\ttrain_loss: 3.0563,\tval_loss: 3.0366\n",
      "29:\t[0s / 5s],\t\ttrain_loss: 3.0432,\tval_loss: 3.0507\n",
      "30:\t[0s / 6s],\t\ttrain_loss: 3.0381,\tval_loss: 3.0431\n",
      "31:\t[0s / 6s],\t\ttrain_loss: 3.0511,\tval_loss: 3.0816\n",
      "32:\t[0s / 6s],\t\ttrain_loss: 3.0442,\tval_loss: 3.0584\n",
      "33:\t[0s / 6s],\t\ttrain_loss: 3.0476,\tval_loss: 3.0760\n",
      "34:\t[0s / 6s],\t\ttrain_loss: 3.0541,\tval_loss: 3.0252\n",
      "35:\t[0s / 7s],\t\ttrain_loss: 3.0294,\tval_loss: 3.0705\n",
      "36:\t[0s / 7s],\t\ttrain_loss: 3.0373,\tval_loss: 3.0049\n",
      "37:\t[0s / 7s],\t\ttrain_loss: 3.0292,\tval_loss: 3.0474\n",
      "38:\t[0s / 7s],\t\ttrain_loss: 3.0347,\tval_loss: 3.0449\n",
      "39:\t[0s / 7s],\t\ttrain_loss: 3.0381,\tval_loss: 3.0358\n",
      "40:\t[0s / 8s],\t\ttrain_loss: 3.0236,\tval_loss: 3.0186\n",
      "41:\t[0s / 8s],\t\ttrain_loss: 3.0300,\tval_loss: 3.0211\n",
      "42:\t[0s / 8s],\t\ttrain_loss: 3.0334,\tval_loss: 2.9886\n",
      "43:\t[0s / 8s],\t\ttrain_loss: 3.0329,\tval_loss: 3.0253\n",
      "44:\t[0s / 8s],\t\ttrain_loss: 3.0375,\tval_loss: 3.0334\n",
      "45:\t[0s / 9s],\t\ttrain_loss: 3.0381,\tval_loss: 3.0427\n",
      "46:\t[0s / 9s],\t\ttrain_loss: 3.0299,\tval_loss: 3.0837\n",
      "47:\t[0s / 9s],\t\ttrain_loss: 3.0282,\tval_loss: 3.0393\n",
      "48:\t[0s / 9s],\t\ttrain_loss: 3.0143,\tval_loss: 3.0104\n",
      "49:\t[0s / 9s],\t\ttrain_loss: 3.0296,\tval_loss: 3.0185\n",
      "50:\t[0s / 10s],\t\ttrain_loss: 3.0247,\tval_loss: 3.0065\n",
      "51:\t[0s / 10s],\t\ttrain_loss: 3.0376,\tval_loss: 2.9994\n",
      "52:\t[0s / 10s],\t\ttrain_loss: 3.0455,\tval_loss: 3.0181\n",
      "53:\t[0s / 10s],\t\ttrain_loss: 3.0269,\tval_loss: 3.0466\n",
      "54:\t[0s / 10s],\t\ttrain_loss: 3.0360,\tval_loss: 3.0260\n",
      "55:\t[0s / 11s],\t\ttrain_loss: 3.0252,\tval_loss: 3.0249\n",
      "56:\t[0s / 11s],\t\ttrain_loss: 3.0372,\tval_loss: 3.0222\n",
      "57:\t[0s / 11s],\t\ttrain_loss: 3.0258,\tval_loss: 3.0192\n",
      "58:\t[0s / 11s],\t\ttrain_loss: 3.0168,\tval_loss: 3.0363\n",
      "59:\t[0s / 11s],\t\ttrain_loss: 3.0267,\tval_loss: 3.0281\n",
      "60:\t[0s / 11s],\t\ttrain_loss: 3.0403,\tval_loss: 3.0345\n",
      "61:\t[0s / 12s],\t\ttrain_loss: 3.0370,\tval_loss: 3.0356\n",
      "62:\t[0s / 12s],\t\ttrain_loss: 3.0353,\tval_loss: 3.0488\n",
      "63:\t[0s / 12s],\t\ttrain_loss: 3.0265,\tval_loss: 3.0543\n",
      "64:\t[0s / 12s],\t\ttrain_loss: 3.0542,\tval_loss: 3.0121\n",
      "65:\t[0s / 12s],\t\ttrain_loss: 3.0169,\tval_loss: 3.0006\n",
      "66:\t[0s / 13s],\t\ttrain_loss: 3.0187,\tval_loss: 3.0271\n",
      "67:\t[0s / 13s],\t\ttrain_loss: 3.0248,\tval_loss: 3.0075\n",
      "N=8000 Training time (13.36s): Train C-Index: 0.796 | Test C-index: 0.782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13.36, 0.7959285645144366, 0.7816845248306526)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get C index on test set\n",
    "params = study.best_params\n",
    "params['num_nodes'] = [16,16]\n",
    "params['dropout'] = .5\n",
    "params['batch_size'] = 64\n",
    "ds.train_with_best_params(params=params, verbose=True, print_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': {'type': 'float', 'low': 0.0001, 'high': 0.01, 'log': True}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Tune\n",
    "### Network Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tune_config(hyperparams):\n",
    "    \"\"\"\n",
    "    Convert a normal dictionary of hyperparameter values to Ray Tune-compatible formats.\n",
    "\n",
    "    Args:\n",
    "        hyperparams (dict): Dictionary of hyperparameters with values as single values, ranges, or lists. Examples:\n",
    "\n",
    "    Returns:\n",
    "        dict: Ray Tune-compatible search space.\n",
    "    \"\"\"\n",
    "    if hyperparams is None:\n",
    "        raise ValueError(\"No hyperparameters given. Input parameter values to convert into Tune compatible format.\")\n",
    "\n",
    "    tune_config = {}\n",
    "    for key, value in hyperparams.items():\n",
    "        # Check if value is a list: Use tune.choice\n",
    "        if isinstance(value, list):\n",
    "            tune_config[key] = tune.choice(value)\n",
    "        \n",
    "        # Check if value is a tuple with two values: Use tune.uniform or tune.randint\n",
    "        elif isinstance(value, tuple) and len(value) == 2:\n",
    "            if all(isinstance(v, int) for v in value):  # Integers: use randint\n",
    "                tune_config[key] = tune.randint(value[0], value[1])\n",
    "            else:  # Floats: use uniform\n",
    "                tune_config[key] = tune.uniform(value[0], value[1])\n",
    "        \n",
    "        # Check if value is a tuple with \"log\" indicator: Use tune.loguniform\n",
    "        elif isinstance(value, tuple) and len(value) == 3 and value[2] == \"log\":\n",
    "            tune_config[key] = tune.loguniform(value[0], value[1])\n",
    "        \n",
    "        # Single value: Keep as-is (constant)\n",
    "        else:\n",
    "            tune_config[key] = value\n",
    "\n",
    "    return tune_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DeepSurvPipeline(ABC):\n",
    "    def __init__(self, train_df, test_df, \n",
    "                time_col='time', status_col='status',\n",
    "                dataName=None, hyperparameters=None, random_state=42):\n",
    "        self.train_df=train_df\n",
    "        self.test_df=test_df\n",
    "        self.n = train_df.shape[0]\n",
    "        self.time_col = time_col\n",
    "        self.status_col = status_col\n",
    "        self.dataName = dataName\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.modelString = 'deepsurv-torch'\n",
    "    \n",
    "    def _preprocess_data(self, df):\n",
    "        \"\"\"Use StandardScaler() to transform input miRNA data.\n",
    "        Return transformed features and labels (survival time and censor status) separately.\n",
    "        \"\"\"\n",
    "        survival_cols = [self.time_col, self.status_col]\n",
    "        covariate_cols = [col for col in df.columns if col not in survival_cols]\n",
    "        standardize = [([col], StandardScaler()) for col in covariate_cols]\n",
    "        x_mapper = DataFrameMapper(standardize)\n",
    "\n",
    "        # transform features (miRNA expression)\n",
    "        x = x_mapper.fit_transform(df[covariate_cols]).astype('float32')\n",
    "        # prepare labels (survival data)\n",
    "        get_target = lambda df: (df[self.time_col].values, df[self.status_col].values)\n",
    "        y = get_target(df)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def _train_mlp(self, train_df, val_df, config, patience, min_delta, verbose=False, is_tuning=False):\n",
    "        \"\"\"Training function that works for both tuning and main training.\n",
    "\n",
    "        Args:\n",
    "            x_train (pandas DataFrame): _description_\n",
    "            y_train (pandas DataFrame): _description_\n",
    "            config (dictionary): neural network training parameters\n",
    "                - 'batch_size': batch size\n",
    "                - 'epochs': number of training epochs\n",
    "                - 'num_nodes': hidden layer size\n",
    "                - 'learning_rate'\n",
    "                - 'dropout'\n",
    "                - ...\n",
    "        \"\"\"\n",
    "        output_size   = 1 \n",
    "        num_nodes     = [32, 32] if 'num_nodes' not in config.keys() else config['num_nodes']\n",
    "        batch_norm    = True if 'batch_norm' not in config.keys() else config['batch_norm'] \n",
    "        output_bias   = True if 'output_bias' not in config.keys() else config['output_bias']\n",
    "        dropout       = 0.2 if 'dropout' not in config.keys() else config['dropout']\n",
    "        learning_rate = 1e-3 if 'learning_rate' not in config.keys() else config['learning_rate']\n",
    "        batch_size    = 32 if 'batch_size' not in config.keys() else config['batch_size']\n",
    "        epochs        = 100 if 'epochs' not in config.keys() else config['epochs']\n",
    "        \n",
    "        # Prepare data \n",
    "        x_train, y_train = self._preprocess_data(train_df)\n",
    "        val = self._preprocess_data(val_df)\n",
    "        x_val, y_val = val\n",
    "        input_size = x_train.shape[1]\n",
    "\n",
    "        \n",
    "        # =================== Build Neural Net ===================\n",
    "        ## define network\n",
    "        net = tt.practical.MLPVanilla(in_features=input_size,\n",
    "                                    out_features=output_size,\n",
    "                                    num_nodes=num_nodes,\n",
    "                                    dropout=dropout, \n",
    "                                    batch_norm=batch_norm,\n",
    "                                    output_bias=output_bias)\n",
    "        # define optimizer \n",
    "        optimizer = tt.optim.Adam(weight_decay=0.01)\n",
    "        model = CoxPH(net, optimizer)\n",
    "        model.optimizer.set_lr(learning_rate)\n",
    "\n",
    "        # =================== Train Model =====================\n",
    "        callbacks = [tt.callbacks.EarlyStopping(patience=patience, min_delta=min_delta)]\n",
    "        \n",
    "        start = time.time() # Record iteration start time\n",
    "        log = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callbacks, \n",
    "                        verbose=False if is_tuning else verbose, # print training steps\n",
    "                        val_data=val, val_batch_size=batch_size\n",
    "                        )\n",
    "        stop = time.time() # Record time when training finished\n",
    "        duration = round(stop - start, 2)\n",
    "        \n",
    "        # =================== Evaluation ===================\n",
    "        _ = model.compute_baseline_hazards()\n",
    "        val_surv = model.predict_surv_df(x_val)\n",
    "        val_c_index  = EvalSurv(val_surv, y_val[0], y_val[1], censor_surv='km').concordance_td() \n",
    "        \n",
    "        tr_surv = model.predict_surv_df(x_train)\n",
    "        tr_c_index  = EvalSurv(tr_surv, y_train[0], y_train[1], censor_surv='km').concordance_td() \n",
    "        \n",
    "        if not is_tuning:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "            \n",
    "        return duration, tr_c_index, val_c_index\n",
    "    \n",
    "    \n",
    "    def _objective_with_cv(self, train_df, config, n_splits):\n",
    "        \"\"\"Perform K-Fold CV and return the average validation loss across folds.\"\"\" \n",
    "                \n",
    "        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=self.random_state)\n",
    "        val_c_indexes = []\n",
    "\n",
    "        for train_index, val_index in kf.split(train_df, train_df[self.status_col]):\n",
    "            tr_df, val_df = train_df.iloc[train_index,:], train_df.iloc[val_index,:]\n",
    "\n",
    "            # Train and evaluate on the current fold -- validation score\n",
    "            *_, val_c_index = self._train_mlp(tr_df, val_df,\n",
    "                                            config=config,\n",
    "                                            patience=10, min_delta=1e-2,\n",
    "                                            verbose=False,\n",
    "                                            is_tuning=True)\n",
    "            val_c_indexes.append(val_c_index)\n",
    "\n",
    "        # Calculate average validation c index score across folds\n",
    "        tune.report(c_index=np.mean(val_c_indexes))\n",
    "    \n",
    "    \n",
    "    def tune_hyperparameters(self, train_df,\n",
    "                            config=None, n_splits=5,\n",
    "                            num_samples=10, max_t=20, grace_period=1,\n",
    "                            n_cpu=2, n_gpu=0):\n",
    "        \"\"\"Method to perform hyperparameter tuning with K-Fold CV.\"\"\"\n",
    "        \n",
    "        # Define search space\n",
    "        self.hyperparameters = config if config is not None else self.hyperparameters\n",
    "        if self.hyperparameters is None:\n",
    "            config = {\n",
    "                \"num_nodes\": tune.choice([[32, 32], [16, 16]]),\n",
    "                \"dropout\": tune.choice([0.1, 0.5]),\n",
    "                \"learning_rate\": tune.loguniform(1e-5, 1e-3),\n",
    "                \"epochs\": tune.choice([20]),  # Can also tune epochs if desired\n",
    "                \"batch_size\": tune.choice([16, 32, 64])  # Optional: tune batch size\n",
    "            }\n",
    "        else:\n",
    "            config = to_tune_config(self.hyperparameters)\n",
    "\n",
    "        # Run tuning with the cross-validation objective\n",
    "        scheduler = ASHAScheduler(metric=\"c_index\", \n",
    "                                mode=\"max\", \n",
    "                                max_t=max_t, \n",
    "                                grace_period=grace_period)\n",
    "        tuner = tune.run(\n",
    "            lambda cfg: self._objective_with_cv(train_df, cfg, n_splits=n_splits),\n",
    "            config=config,\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "            resources_per_trial={\"cpu\": n_cpu, \"gpu\": n_gpu}\n",
    "        )\n",
    "        \n",
    "        # Save the best hyperparameters\n",
    "        self.best_params = tuner.get_best_config(metric=\"c_index\", mode='max')\n",
    "        print(f\"Best trial parameters: {self.best_params.config}\")\n",
    "        \n",
    "        \n",
    "    def train_with_best_params(self, val_size=0.2, \n",
    "                            kwargs=None, \n",
    "                            patience=25, min_delta=1e-2,\n",
    "                            verbose=True, print_scores=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            subset (_type_): _description_\n",
    "            batch_sizes (_type_): _description_\n",
    "            val_size (float, optional): _description_. Defaults to 0.2.\n",
    "            kwargs (_type_, optional): _description_. Defaults to None.\n",
    "            time_col (str, optional): _description_. Defaults to 'time'.\n",
    "            status_col (str, optional): _description_. Defaults to 'status'.\n",
    "            verbose (bool, optional): _description_. Defaults to True.\n",
    "            print_scores (bool, optional): _description_. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        if kwargs is None and self.best_params is None:\n",
    "            raise ValueError(\"No hyperparameters found. Run tune_hyperparameters() first or define them manually.\")\n",
    "        elif kwargs is None:\n",
    "            kwargs = self.best_params\n",
    "        \n",
    "        # ===================== Prepare Data =======================\n",
    "        tr_df, val_df = train_test_split(self.train_df, \n",
    "                                        test_size=val_size,\n",
    "                                        shuffle=True, random_state=self.random_state,\n",
    "                                        stratify=self.train_df[self.status_col])\n",
    "        \n",
    "        # ===================== Train Model =====================\n",
    "        duration, train_c, _ = self._train_mlp(tr_df, val_df, config=kwargs, \n",
    "                                            patience=patience, min_delta=min_delta,\n",
    "                                            verbose=verbose, is_tuning=False)\n",
    "\n",
    "        # ===================== Evaluation =====================\n",
    "        x_test, y_test = self._preprocess_data(self.test_df)\n",
    "        te_surv = self.model.predict_surv_df(x_test)\n",
    "        test_c  = EvalSurv(te_surv, y_test[0], y_test[1], censor_surv='km').concordance_td()\n",
    "        \n",
    "        if print_scores:\n",
    "            print(f\"N={self.n} Training time ({duration}s): Train C-Index: {round(train_c,3)} | Test C-index: {round(test_c,3)}\")\n",
    "            \n",
    "        return duration, train_c, test_c\n",
    "    \n",
    "    \n",
    "    def write(self, model_results, out_dir=None, fileName='model.results.txt'):\n",
    "        out_dir=os.path.join('models', self.dataName, self.modelString) if out_dir is None else out_dir\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        # save results as txt or csv file\n",
    "        if 'txt' in fileName:\n",
    "            model_results.to_csv(os.path.join(out_dir,fileName), sep='\\t')\n",
    "        elif 'csv' in fileName:\n",
    "            model_results.to_csv(os.path.join(out_dir,fileName), index=False)\n",
    "        else:\n",
    "            print('Please specify a file name with either a txt or csv extension.')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to plot simulated data -- later add to utils!!\n",
    "def plot_simulation_data(train_df, test_df):\n",
    "    # observe data\n",
    "    print(\"Event rate in train set: %f\" % (sum(train_df['status']==1) / train_df.shape[0]))\n",
    "    print(\"Event rate in test set: %f\" %  (sum(test_df['status']==1) / test_df.shape[0]))\n",
    "    print('Survival time distribution:')\n",
    "    _, ax = plt.subplots(figsize=(3,3))\n",
    "    ax.hist(train_df['time'], label='train')\n",
    "    # ax.hist(val_df['time'],   label='val', alpha=.8)\n",
    "    ax.hist(test_df['time'], label='test', alpha=0.6)\n",
    "    ax.legend(fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_nodes': <ray.tune.search.sample.Categorical at 0x7f4c15d130a0>,\n",
       " 'dropout': <ray.tune.search.sample.Categorical at 0x7f4c15d134c0>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset = [50, 500, 1000, 2000, 5000, 8000]\n",
    "# batch_sizes = [8, 16, 16, 32, 64, 128] #[8, 16, 16, 32, 32, 64]\n",
    "\n",
    "parameters = {\n",
    "    'num_nodes': [32, 32],\n",
    "    'batch_norm': True,\n",
    "    'output_bias': True,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-3\n",
    "}\n",
    "\n",
    "hyperparams =  {\n",
    "        \"num_nodes\": [[32, 32], [16, 16]],       # list for tune.choice\n",
    "        \"dropout\": [0.1, 0.5]                    # tuple for tune.uniform\n",
    "        # \"learning_rate\": (1e-5, 1e-3, \"log\"),  # tuple with \"log\" for tune.loguniform\n",
    "        # \"batch_size\": [32, 64]                 # single value, unchanged\n",
    "    }\n",
    "to_tune_config(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dimensions: (8000, 1035)\n",
      "Testing data dimensions:  (2000, 1035)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "folder = 'linear'\n",
    "keywords = ['moderate', \"latest\", 'RW']\n",
    "train_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, test_size=0.2)\n",
    "\n",
    "print(f\"Training data dimensions: {train_df.shape}\")\n",
    "print(f\"Testing data dimensions:  {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 09:47:29,805\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9288bce62fef4397b235c62f85734931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.8.10</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.10.0</b></td>\n",
       "    </tr>\n",
       "    \n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.8.10', ray_version='2.10.0', ray_commit='09abba26b5bf2707639bb637c208d062a47b46f6')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 09:52:20,493\tINFO tune.py:613 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2024-11-15 09:52:20,704\tWARNING callback.py:137 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-11-15 09:55:28</td></tr>\n",
       "<tr><td>Running for: </td><td>00:03:07.45        </td></tr>\n",
       "<tr><td>Memory:      </td><td>8.2/256.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 16.000: None | Iter 4.000: None | Iter 1.000: None<br>Logical resource usage: 0/32 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status  </th><th>loc  </th><th>num_nodes  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>lambda_3751e_00000</td><td>PENDING </td><td>     </td><td>[16, 16]   </td></tr>\n",
       "<tr><td>lambda_3751e_00001</td><td>PENDING </td><td>     </td><td>[16, 16]   </td></tr>\n",
       "<tr><td>lambda_3751e_00002</td><td>PENDING </td><td>     </td><td>[32, 32]   </td></tr>\n",
       "<tr><td>lambda_3751e_00003</td><td>PENDING </td><td>     </td><td>[16, 16]   </td></tr>\n",
       "<tr><td>lambda_3751e_00004</td><td>PENDING </td><td>     </td><td>[32, 32]   </td></tr>\n",
       "<tr><td>lambda_3751e_00005</td><td>PENDING </td><td>     </td><td>[32, 32]   </td></tr>\n",
       "<tr><td>lambda_3751e_00006</td><td>PENDING </td><td>     </td><td>[16, 16]   </td></tr>\n",
       "<tr><td>lambda_3751e_00007</td><td>PENDING </td><td>     </td><td>[16, 16]   </td></tr>\n",
       "<tr><td>lambda_3751e_00008</td><td>PENDING </td><td>     </td><td>[32, 32]   </td></tr>\n",
       "<tr><td>lambda_3751e_00009</td><td>PENDING </td><td>     </td><td>[16, 16]   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 09:55:28,149\tWARNING tune.py:229 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-11-15 09:55:28,162\tINFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to '/home/nfs/dengy/ray_results/lambda_2024-11-15_09-52-20' in 0.0114s.\n",
      "2024-11-15 09:55:28,186\tINFO tune.py:1048 -- Total run time: 187.69 seconds (187.44 seconds for the tuning loop).\n",
      "2024-11-15 09:55:28,188\tWARNING tune.py:1063 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2024-11-15 09:55:28,207\tWARNING experiment_analysis.py:190 -- Failed to fetch metrics for 10 trial(s):\n",
      "- lambda_3751e_00000: FileNotFoundError('Could not fetch metrics for lambda_3751e_00000: both result.json and progress.csv were not found at /home/nfs/dengy/ray_results/lambda_2024-11-15_09-52-20/lambda_3751e_00000_0_num_nodes=16_16_2024-11-15_09-52-25')\n",
      "- lambda_3751e_00001: FileNotFoundError('Could not fetch metrics for lambda_3751e_00001: both result.json and progress.csv were not found at /home/nfs/dengy/ray_results/lambda_2024-11-15_09-52-20/lambda_3751e_00001_1_num_nodes=16_16_2024-11-15_09-52-25')\n",
      "- lambda_3751e_00002: FileNotFoundError('Could not fetch metrics for lambda_3751e_00002: both result.json and progress.csv were not found at /home/nfs/dengy/ray_results/lambda_2024-11-15_09-52-20/lambda_3751e_00002_2_num_nodes=32_32_2024-11-15_09-52-25')\n",
      "- lambda_3751e_00003: FileNotFoundError('Could not fetch metrics for lambda_3751e_00003: both result.json and progress.csv were not found at /home/nfs/dengy/ray_results/lambda_2024-11-15_09-52-20/lambda_3751e_00003_3_num_nodes=16_16_2024-11-15_09-52-26')\n",
      "- lambda_3751e_00004: FileNotFoundError('Could not fetch metrics for lambda_3751e_00004: both result.json and progress.csv were not found at /home/nfs/dengy/ray_results/lambda_2024-11-15_09-52-20/lambda_3751e_00004_4_num_nodes=32_32_2024-11-15_09-52-26')\n",
      "- lambda_3751e_00005: FileNotFoundError('Could not fetch metrics for lambda_3751e_00005: both result.json and progress.csv were not found at /home/nfs/dengy/ray_results/lambda_2024-11-15_09-52-20/lambda_3751e_00005_5_num_nodes=32_32_2024-11-15_09-52-26')\n",
      "- lambda_3751e_00006: FileNotFoundError('Could not fetch metrics for lambda_3751e_00006: both result.json and progress.csv were not found at /home/nfs/dengy/ray_results/lambda_2024-11-15_09-52-20/lambda_3751e_00006_6_num_nodes=16_16_2024-11-15_09-52-26')\n",
      "- lambda_3751e_00007: FileNotFoundError('Could not fetch metrics for lambda_3751e_00007: both result.json and progress.csv were not found at /home/nfs/dengy/ray_results/lambda_2024-11-15_09-52-20/lambda_3751e_00007_7_num_nodes=16_16_2024-11-15_09-52-27')\n",
      "- lambda_3751e_00008: FileNotFoundError('Could not fetch metrics for lambda_3751e_00008: both result.json and progress.csv were not found at /home/nfs/dengy/ray_results/lambda_2024-11-15_09-52-20/lambda_3751e_00008_8_num_nodes=32_32_2024-11-15_09-52-27')\n",
      "- lambda_3751e_00009: FileNotFoundError('Could not fetch metrics for lambda_3751e_00009: both result.json and progress.csv were not found at /home/nfs/dengy/ray_results/lambda_2024-11-15_09-52-20/lambda_3751e_00009_9_num_nodes=16_16_2024-11-15_09-52-27')\n",
      "2024-11-15 09:55:28,208\tWARNING experiment_analysis.py:568 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m hyperparams \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: [[\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m], [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m]]}\n\u001b[1;32m      2\u001b[0m ds \u001b[38;5;241m=\u001b[39m DeepSurvPipeline(train_df, test_df, hyperparameters\u001b[38;5;241m=\u001b[39mhyperparams)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_cpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m ds\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# ds.train_with_best_params(subset, time_col='t', status_col='delta',\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#                             batch_sizes=[8],\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#                             patience=25, min_delta=1e-2, \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#                             kwargs=parameters, verbose=False, print_scores=True)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 162\u001b[0m, in \u001b[0;36mDeepSurvPipeline.tune_hyperparameters\u001b[0;34m(self, train_df, config, n_splits, num_samples, max_t, grace_period, n_cpu, n_gpu)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Save the best hyperparameters\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_params \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_config(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "hyperparams = {\"num_nodes\": [[32, 32], [16, 16]]}\n",
    "ds = DeepSurvPipeline(train_df, test_df, hyperparameters=hyperparams)\n",
    "ds.tune_hyperparameters(train_df, n_cpu=2)\n",
    "ds.best_params\n",
    "# ds.train_with_best_params(subset, time_col='t', status_col='delta',\n",
    "#                             batch_sizes=[8],\n",
    "#                             patience=25, min_delta=1e-2, \n",
    "#                             kwargs=parameters, verbose=False, print_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Data with Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA real world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca_clin  = pd.read_csv(os.path.join(\"data\", \"batch\", \"TCGA_miRNA_clinical.csv\"))\n",
    "brca_mirna = pd.read_csv(os.path.join(\"data\", \"batch\", \"TCGA-BRCA-3.csv\"), index_col=0)\n",
    "\n",
    "brca_clin = brca_clin.rename(columns={'bcr_patient_barcode':'Sample'})\n",
    "\n",
    "brca = pd.merge(brca_mirna, brca_clin, on='Sample', how = 'left')\n",
    "brca_df = brca.loc[:, ['days_to_death', \n",
    "                       'days_to_last_followup'] +\n",
    "                    [col for col in brca.columns if 'hsa' in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events: 102\n",
      "Number of censored cases: 912\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deathtt = brca_df['days_to_death'][~np.isnan(brca_df['days_to_death'])]\n",
    "censortt = brca_df['days_to_last_followup'][~np.isnan(brca_df['days_to_last_followup'])]\n",
    "print(\"Number of events: %d\" % len(deathtt))\n",
    "print(\"Number of censored cases: %d\" %len(censortt))\n",
    "plt.hist(deathtt, color='b', label='event time', alpha=0.7)\n",
    "plt.hist(censortt, color='r', label='censor time', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_777201/4163331497.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  brca_df['time'][np.isnan(brca_df['time'])] = brca_df['days_to_last_followup'][np.isnan(brca_df['time'])]\n"
     ]
    }
   ],
   "source": [
    "# get censoring status and survival time\n",
    "brca_df['status'] = [int(i) for i in ~np.isnan(brca_df['days_to_death'])]\n",
    "\n",
    "brca_df['time'] = brca_df['days_to_death']\n",
    "brca_df['time'][np.isnan(brca_df['time'])] = brca_df['days_to_last_followup'][np.isnan(brca_df['time'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca_df = brca_df.drop(columns=['days_to_death',\"days_to_last_followup\"])\n",
    "brca_df = brca_df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event rate in the TCGA BRCA data: 0.100592\n"
     ]
    }
   ],
   "source": [
    "print(\"Event rate in the TCGA BRCA data: %f\" % (sum(brca_df['status']) / brca_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "parameters = {\n",
    "    'num_nodes': [64, 32, 32],\n",
    "    'out_features': 1,\n",
    "    'batch_norm' :True,\n",
    "    'dropout' : 0.2,\n",
    "    'output_bias' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\n",
      "1:\t[0s / 0s],\t\n",
      "2:\t[0s / 0s],\t\n",
      "3:\t[0s / 0s],\t\n",
      "4:\t[0s / 0s],\t\n",
      "5:\t[0s / 0s],\t\n",
      "6:\t[0s / 0s],\t\n",
      "7:\t[0s / 0s],\t\n",
      "8:\t[0s / 0s],\t\n",
      "9:\t[0s / 0s],\t\n",
      "10:\t[0s / 0s],\t\n",
      "11:\t[0s / 0s],\t\n",
      "12:\t[0s / 0s],\t\n",
      "13:\t[0s / 0s],\t\n",
      "14:\t[0s / 0s],\t\n",
      "15:\t[0s / 1s],\t\n",
      "16:\t[0s / 1s],\t\n",
      "17:\t[0s / 1s],\t\n",
      "18:\t[0s / 1s],\t\n",
      "19:\t[0s / 1s],\t\n",
      "20:\t[0s / 1s],\t\n",
      "21:\t[0s / 1s],\t\n",
      "22:\t[0s / 1s],\t\n",
      "23:\t[0s / 1s],\t\n",
      "24:\t[0s / 1s],\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/dengy/dl-env/lib/python3.8/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "/home/nfs/dengy/dl-survival-miRNA/pycox/utils.py:57: UserWarning: start_duration 0 is larger than minimum duration -31.0. If intentional, consider changing start_duration when calling kaplan_meier.\n",
      "  warnings.warn(f\"start_duration {start_duration} is larger than minimum duration {durations.min()}. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n train</th>\n",
       "      <th>train time</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>811</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.437989</td>\n",
       "      <td>0.51035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n train  train time  train score  test score\n",
       "0      811        1.68     0.437989     0.51035"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(brca_df, test_size=0.2, random_state=42, stratify=brca_df['status'])\n",
    "\n",
    "subset = [train_df.shape[0]]\n",
    "ds = DeepSurvPipeline(train_df=train_df, test_df=test_df, dataName=\"tcga-brca\")\n",
    "ds_results = ds.run_deepsurv(subset,\n",
    "                            batch_sizes=[32],\n",
    "                            patience=25, min_delta=1e-3,\n",
    "                            kwargs=parameters, verbose=True, print_scores=False)\n",
    "ds_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated data with batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>delta</th>\n",
       "      <th>A_25_P00010019</th>\n",
       "      <th>A_25_P00010020</th>\n",
       "      <th>A_25_P00010021</th>\n",
       "      <th>A_25_P00010023</th>\n",
       "      <th>A_25_P00010041</th>\n",
       "      <th>A_25_P00010042</th>\n",
       "      <th>A_25_P00010043</th>\n",
       "      <th>A_25_P00010044</th>\n",
       "      <th>...</th>\n",
       "      <th>A_25_P00016269</th>\n",
       "      <th>A_25_P00016270</th>\n",
       "      <th>A_25_P00016271</th>\n",
       "      <th>A_38_P00018874</th>\n",
       "      <th>A_38_P00018875</th>\n",
       "      <th>A_38_P00018876</th>\n",
       "      <th>A_38_P00018877</th>\n",
       "      <th>A_38_P00018878</th>\n",
       "      <th>A_38_P00018879</th>\n",
       "      <th>batch_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54.77</td>\n",
       "      <td>0</td>\n",
       "      <td>5.414510</td>\n",
       "      <td>5.361662</td>\n",
       "      <td>5.310293</td>\n",
       "      <td>5.324175</td>\n",
       "      <td>5.303017</td>\n",
       "      <td>5.280794</td>\n",
       "      <td>5.155670</td>\n",
       "      <td>5.133808</td>\n",
       "      <td>...</td>\n",
       "      <td>5.464329</td>\n",
       "      <td>5.540626</td>\n",
       "      <td>5.496476</td>\n",
       "      <td>5.476537</td>\n",
       "      <td>5.370435</td>\n",
       "      <td>5.593264</td>\n",
       "      <td>5.664620</td>\n",
       "      <td>5.554928</td>\n",
       "      <td>5.877363</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.63</td>\n",
       "      <td>1</td>\n",
       "      <td>5.645660</td>\n",
       "      <td>5.628909</td>\n",
       "      <td>5.428349</td>\n",
       "      <td>5.476566</td>\n",
       "      <td>5.651610</td>\n",
       "      <td>5.433626</td>\n",
       "      <td>5.502592</td>\n",
       "      <td>5.512356</td>\n",
       "      <td>...</td>\n",
       "      <td>6.003386</td>\n",
       "      <td>6.206899</td>\n",
       "      <td>6.001569</td>\n",
       "      <td>5.825270</td>\n",
       "      <td>5.861964</td>\n",
       "      <td>6.057292</td>\n",
       "      <td>6.187139</td>\n",
       "      <td>6.776756</td>\n",
       "      <td>7.332764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.93</td>\n",
       "      <td>0</td>\n",
       "      <td>5.556705</td>\n",
       "      <td>5.498588</td>\n",
       "      <td>5.415355</td>\n",
       "      <td>5.568460</td>\n",
       "      <td>5.446503</td>\n",
       "      <td>5.393021</td>\n",
       "      <td>5.474867</td>\n",
       "      <td>5.342807</td>\n",
       "      <td>...</td>\n",
       "      <td>5.669618</td>\n",
       "      <td>5.745632</td>\n",
       "      <td>5.669056</td>\n",
       "      <td>5.699790</td>\n",
       "      <td>5.704279</td>\n",
       "      <td>5.976060</td>\n",
       "      <td>5.899931</td>\n",
       "      <td>6.011728</td>\n",
       "      <td>6.423470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.84</td>\n",
       "      <td>0</td>\n",
       "      <td>5.107029</td>\n",
       "      <td>4.998545</td>\n",
       "      <td>5.001954</td>\n",
       "      <td>4.976329</td>\n",
       "      <td>5.009803</td>\n",
       "      <td>4.983482</td>\n",
       "      <td>4.873145</td>\n",
       "      <td>4.878836</td>\n",
       "      <td>...</td>\n",
       "      <td>5.203232</td>\n",
       "      <td>5.211459</td>\n",
       "      <td>5.179419</td>\n",
       "      <td>4.940771</td>\n",
       "      <td>4.868911</td>\n",
       "      <td>5.083392</td>\n",
       "      <td>5.119356</td>\n",
       "      <td>5.026625</td>\n",
       "      <td>5.225131</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.52</td>\n",
       "      <td>1</td>\n",
       "      <td>5.715634</td>\n",
       "      <td>5.554129</td>\n",
       "      <td>5.560682</td>\n",
       "      <td>5.429897</td>\n",
       "      <td>5.525073</td>\n",
       "      <td>5.539028</td>\n",
       "      <td>5.455037</td>\n",
       "      <td>5.461786</td>\n",
       "      <td>...</td>\n",
       "      <td>6.043570</td>\n",
       "      <td>6.009590</td>\n",
       "      <td>6.041434</td>\n",
       "      <td>5.494935</td>\n",
       "      <td>5.562034</td>\n",
       "      <td>5.585995</td>\n",
       "      <td>5.702457</td>\n",
       "      <td>5.546448</td>\n",
       "      <td>5.863067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  3526 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       t  delta  A_25_P00010019  A_25_P00010020  A_25_P00010021  \\\n",
       "0  54.77      0        5.414510        5.361662        5.310293   \n",
       "1   9.63      1        5.645660        5.628909        5.428349   \n",
       "2  35.93      0        5.556705        5.498588        5.415355   \n",
       "3  71.84      0        5.107029        4.998545        5.001954   \n",
       "4  20.52      1        5.715634        5.554129        5.560682   \n",
       "\n",
       "   A_25_P00010023  A_25_P00010041  A_25_P00010042  A_25_P00010043  \\\n",
       "0        5.324175        5.303017        5.280794        5.155670   \n",
       "1        5.476566        5.651610        5.433626        5.502592   \n",
       "2        5.568460        5.446503        5.393021        5.474867   \n",
       "3        4.976329        5.009803        4.983482        4.873145   \n",
       "4        5.429897        5.525073        5.539028        5.455037   \n",
       "\n",
       "   A_25_P00010044  ...  A_25_P00016269  A_25_P00016270  A_25_P00016271  \\\n",
       "0        5.133808  ...        5.464329        5.540626        5.496476   \n",
       "1        5.512356  ...        6.003386        6.206899        6.001569   \n",
       "2        5.342807  ...        5.669618        5.745632        5.669056   \n",
       "3        4.878836  ...        5.203232        5.211459        5.179419   \n",
       "4        5.461786  ...        6.043570        6.009590        6.041434   \n",
       "\n",
       "   A_38_P00018874  A_38_P00018875  A_38_P00018876  A_38_P00018877  \\\n",
       "0        5.476537        5.370435        5.593264        5.664620   \n",
       "1        5.825270        5.861964        6.057292        6.187139   \n",
       "2        5.699790        5.704279        5.976060        5.899931   \n",
       "3        4.940771        4.868911        5.083392        5.119356   \n",
       "4        5.494935        5.562034        5.585995        5.702457   \n",
       "\n",
       "   A_38_P00018878  A_38_P00018879  batch_id  \n",
       "0        5.554928        5.877363         1  \n",
       "1        6.776756        7.332764         1  \n",
       "2        6.011728        6.423470         1  \n",
       "3        5.026625        5.225131         1  \n",
       "4        5.546448        5.863067         1  \n",
       "\n",
       "[5 rows x 3526 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load simulated data\n",
    "df_withBatch = pd.read_csv(os.path.join(\"data\",\"batch\", \"demo_sim_withBatch.csv\")).iloc[:, 1:]\n",
    "df_noBatch   = pd.read_csv(os.path.join(\"data\",\"batch\", \"demo_sim_noBatch.csv\")).iloc[:, 1:]\n",
    "\n",
    "df_withBatch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_withBatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mdf_withBatch\u001b[49m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mdf_withBatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# subset = [train_df.shape[0]]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ds = DeepSurvPipeline(train_df=train_df, test_df=test_df, dataName=\"with-batch\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# ds_results = ds.run_deepsurv(subset, time_col='t', status_col='delta',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#                             kwargs=parameters, verbose=False, print_scores=True)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ds_results\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_withBatch' is not defined"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df_withBatch, test_size=0.2, random_state=42, stratify=df_withBatch['delta'])\n",
    "\n",
    "# subset = [train_df.shape[0]]\n",
    "# ds = DeepSurvPipeline(train_df=train_df, test_df=test_df, dataName=\"with-batch\")\n",
    "# ds_results = ds.run_deepsurv(subset, time_col='t', status_col='delta',\n",
    "#                             batch_sizes=[8],\n",
    "#                             patience=25, min_delta=1e-2, \n",
    "#                             kwargs=parameters, verbose=False, print_scores=True)\n",
    "# ds_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 1.4430,\tval_loss: 2.2770\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 1.2151,\tval_loss: 1.8226\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 1.1575,\tval_loss: 1.6374\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 1.1089,\tval_loss: 1.5421\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 1.1016,\tval_loss: 1.4289\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 1.0703,\tval_loss: 1.3313\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 1.0573,\tval_loss: 1.3368\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 1.0322,\tval_loss: 1.3628\n",
      "8:\t[0s / 0s],\t\ttrain_loss: 1.0965,\tval_loss: 1.1529\n",
      "9:\t[0s / 0s],\t\ttrain_loss: 0.9755,\tval_loss: 1.1564\n",
      "10:\t[0s / 0s],\t\ttrain_loss: 1.0619,\tval_loss: 1.2399\n",
      "11:\t[0s / 0s],\t\ttrain_loss: 0.9085,\tval_loss: 1.2943\n",
      "12:\t[0s / 0s],\t\ttrain_loss: 0.9006,\tval_loss: 1.3940\n",
      "13:\t[0s / 0s],\t\ttrain_loss: 0.8678,\tval_loss: 1.2190\n",
      "14:\t[0s / 0s],\t\ttrain_loss: 0.8291,\tval_loss: 1.1011\n",
      "15:\t[0s / 0s],\t\ttrain_loss: 0.9967,\tval_loss: 1.1122\n",
      "16:\t[0s / 0s],\t\ttrain_loss: 0.8468,\tval_loss: 1.2879\n",
      "17:\t[0s / 0s],\t\ttrain_loss: 0.8151,\tval_loss: 1.4811\n",
      "18:\t[0s / 0s],\t\ttrain_loss: 0.9106,\tval_loss: 1.2335\n",
      "19:\t[0s / 0s],\t\ttrain_loss: 0.8495,\tval_loss: 1.1063\n",
      "20:\t[0s / 0s],\t\ttrain_loss: 0.7727,\tval_loss: 1.0822\n",
      "21:\t[0s / 0s],\t\ttrain_loss: 0.6999,\tval_loss: 0.9951\n",
      "22:\t[0s / 0s],\t\ttrain_loss: 1.0124,\tval_loss: 1.0089\n",
      "23:\t[0s / 0s],\t\ttrain_loss: 0.9572,\tval_loss: 1.1018\n",
      "24:\t[0s / 0s],\t\ttrain_loss: 0.8851,\tval_loss: 1.1402\n",
      "25:\t[0s / 0s],\t\ttrain_loss: 0.7093,\tval_loss: 1.0727\n",
      "26:\t[0s / 0s],\t\ttrain_loss: 0.7732,\tval_loss: 1.1587\n",
      "27:\t[0s / 0s],\t\ttrain_loss: 0.8076,\tval_loss: 1.0723\n",
      "28:\t[0s / 0s],\t\ttrain_loss: 0.9194,\tval_loss: 1.0913\n",
      "29:\t[0s / 0s],\t\ttrain_loss: 0.8075,\tval_loss: 1.0611\n",
      "30:\t[0s / 0s],\t\ttrain_loss: 0.8796,\tval_loss: 1.0288\n",
      "31:\t[0s / 0s],\t\ttrain_loss: 0.8252,\tval_loss: 1.0485\n",
      "32:\t[0s / 0s],\t\ttrain_loss: 0.7634,\tval_loss: 0.9350\n",
      "33:\t[0s / 0s],\t\ttrain_loss: 0.7452,\tval_loss: 0.9143\n",
      "34:\t[0s / 0s],\t\ttrain_loss: 0.8092,\tval_loss: 0.8898\n",
      "35:\t[0s / 0s],\t\ttrain_loss: 0.7038,\tval_loss: 0.9867\n",
      "36:\t[0s / 0s],\t\ttrain_loss: 0.7511,\tval_loss: 0.9117\n",
      "37:\t[0s / 0s],\t\ttrain_loss: 0.8117,\tval_loss: 0.7681\n",
      "38:\t[0s / 0s],\t\ttrain_loss: 0.7451,\tval_loss: 0.9100\n",
      "39:\t[0s / 0s],\t\ttrain_loss: 0.7988,\tval_loss: 1.0064\n",
      "40:\t[0s / 0s],\t\ttrain_loss: 0.7880,\tval_loss: 0.9892\n",
      "41:\t[0s / 0s],\t\ttrain_loss: 0.7461,\tval_loss: 1.0176\n",
      "42:\t[0s / 0s],\t\ttrain_loss: 0.7015,\tval_loss: 0.9034\n",
      "43:\t[0s / 0s],\t\ttrain_loss: 0.6163,\tval_loss: 1.1978\n",
      "44:\t[0s / 0s],\t\ttrain_loss: 0.6431,\tval_loss: 1.0288\n",
      "45:\t[0s / 0s],\t\ttrain_loss: 0.6943,\tval_loss: 1.0487\n",
      "46:\t[0s / 0s],\t\ttrain_loss: 0.8622,\tval_loss: 1.3975\n",
      "47:\t[0s / 0s],\t\ttrain_loss: 0.6306,\tval_loss: 0.9004\n",
      "48:\t[0s / 1s],\t\ttrain_loss: 0.7566,\tval_loss: 0.8520\n",
      "49:\t[0s / 1s],\t\ttrain_loss: 0.7069,\tval_loss: 0.8622\n",
      "50:\t[0s / 1s],\t\ttrain_loss: 0.6434,\tval_loss: 0.9325\n",
      "51:\t[0s / 1s],\t\ttrain_loss: 0.7668,\tval_loss: 0.8129\n",
      "52:\t[0s / 1s],\t\ttrain_loss: 0.6815,\tval_loss: 0.7509\n",
      "53:\t[0s / 1s],\t\ttrain_loss: 0.6847,\tval_loss: 0.7474\n",
      "54:\t[0s / 1s],\t\ttrain_loss: 0.7957,\tval_loss: 0.7315\n",
      "55:\t[0s / 1s],\t\ttrain_loss: 0.7483,\tval_loss: 0.9467\n",
      "56:\t[0s / 1s],\t\ttrain_loss: 0.8272,\tval_loss: 0.8966\n",
      "57:\t[0s / 1s],\t\ttrain_loss: 0.7832,\tval_loss: 1.4767\n",
      "58:\t[0s / 1s],\t\ttrain_loss: 0.6732,\tval_loss: 0.9518\n",
      "59:\t[0s / 1s],\t\ttrain_loss: 0.7193,\tval_loss: 0.7700\n",
      "60:\t[0s / 1s],\t\ttrain_loss: 0.6625,\tval_loss: 0.7380\n",
      "61:\t[0s / 1s],\t\ttrain_loss: 0.7033,\tval_loss: 0.9198\n",
      "62:\t[0s / 1s],\t\ttrain_loss: 0.6280,\tval_loss: 0.9727\n",
      "63:\t[0s / 1s],\t\ttrain_loss: 0.8332,\tval_loss: 1.0670\n",
      "64:\t[0s / 1s],\t\ttrain_loss: 0.7226,\tval_loss: 1.0884\n",
      "65:\t[0s / 1s],\t\ttrain_loss: 0.7131,\tval_loss: 0.9510\n",
      "66:\t[0s / 1s],\t\ttrain_loss: 0.6647,\tval_loss: 0.8590\n",
      "67:\t[0s / 1s],\t\ttrain_loss: 0.6508,\tval_loss: 0.9923\n",
      "68:\t[0s / 1s],\t\ttrain_loss: 0.5749,\tval_loss: 0.7066\n",
      "69:\t[0s / 1s],\t\ttrain_loss: 0.5996,\tval_loss: 0.7098\n",
      "70:\t[0s / 1s],\t\ttrain_loss: 0.6527,\tval_loss: 0.7907\n",
      "71:\t[0s / 1s],\t\ttrain_loss: 0.6307,\tval_loss: 0.9274\n",
      "72:\t[0s / 1s],\t\ttrain_loss: 0.6549,\tval_loss: 1.1576\n",
      "73:\t[0s / 1s],\t\ttrain_loss: 0.5838,\tval_loss: 0.9391\n",
      "74:\t[0s / 1s],\t\ttrain_loss: 0.5017,\tval_loss: 0.7677\n",
      "75:\t[0s / 1s],\t\ttrain_loss: 0.7415,\tval_loss: 0.7729\n",
      "76:\t[0s / 1s],\t\ttrain_loss: 0.5490,\tval_loss: 0.8286\n",
      "77:\t[0s / 1s],\t\ttrain_loss: 0.5465,\tval_loss: 0.8122\n",
      "78:\t[0s / 1s],\t\ttrain_loss: 0.5857,\tval_loss: 0.8130\n",
      "79:\t[0s / 1s],\t\ttrain_loss: 0.4298,\tval_loss: 0.9402\n",
      "80:\t[0s / 1s],\t\ttrain_loss: 0.4904,\tval_loss: 0.7797\n",
      "81:\t[0s / 1s],\t\ttrain_loss: 0.5523,\tval_loss: 0.6844\n",
      "82:\t[0s / 1s],\t\ttrain_loss: 0.3723,\tval_loss: 0.8277\n",
      "83:\t[0s / 1s],\t\ttrain_loss: 0.4643,\tval_loss: 0.9115\n",
      "84:\t[0s / 1s],\t\ttrain_loss: 0.5569,\tval_loss: 0.7318\n",
      "85:\t[0s / 1s],\t\ttrain_loss: 0.5198,\tval_loss: 0.8432\n",
      "86:\t[0s / 1s],\t\ttrain_loss: 0.4904,\tval_loss: 0.9391\n",
      "87:\t[0s / 1s],\t\ttrain_loss: 0.5020,\tval_loss: 0.7274\n",
      "88:\t[0s / 1s],\t\ttrain_loss: 0.5850,\tval_loss: 0.7228\n",
      "89:\t[0s / 1s],\t\ttrain_loss: 0.5670,\tval_loss: 0.8655\n",
      "90:\t[0s / 1s],\t\ttrain_loss: 0.5483,\tval_loss: 0.9637\n",
      "91:\t[0s / 1s],\t\ttrain_loss: 0.6063,\tval_loss: 0.8552\n",
      "92:\t[0s / 1s],\t\ttrain_loss: 0.5788,\tval_loss: 0.6592\n",
      "93:\t[0s / 1s],\t\ttrain_loss: 0.7735,\tval_loss: 0.5114\n",
      "94:\t[0s / 1s],\t\ttrain_loss: 0.6611,\tval_loss: 0.6772\n",
      "95:\t[0s / 1s],\t\ttrain_loss: 0.5911,\tval_loss: 0.7678\n",
      "96:\t[0s / 1s],\t\ttrain_loss: 0.6131,\tval_loss: 0.6981\n",
      "97:\t[0s / 1s],\t\ttrain_loss: 0.5313,\tval_loss: 0.7006\n",
      "98:\t[0s / 1s],\t\ttrain_loss: 0.5866,\tval_loss: 0.6415\n",
      "99:\t[0s / 1s],\t\ttrain_loss: 0.6052,\tval_loss: 0.6765\n",
      "100:\t[0s / 2s],\t\ttrain_loss: 0.7140,\tval_loss: 0.5328\n",
      "101:\t[0s / 2s],\t\ttrain_loss: 0.5922,\tval_loss: 0.6243\n",
      "102:\t[0s / 2s],\t\ttrain_loss: 0.6578,\tval_loss: 0.7866\n",
      "103:\t[0s / 2s],\t\ttrain_loss: 0.5963,\tval_loss: 0.6564\n",
      "104:\t[0s / 2s],\t\ttrain_loss: 0.6396,\tval_loss: 0.6641\n",
      "105:\t[0s / 2s],\t\ttrain_loss: 0.7075,\tval_loss: 1.0104\n",
      "106:\t[0s / 2s],\t\ttrain_loss: 0.6833,\tval_loss: 0.8781\n",
      "107:\t[0s / 2s],\t\ttrain_loss: 0.7241,\tval_loss: 0.8604\n",
      "108:\t[0s / 2s],\t\ttrain_loss: 0.6668,\tval_loss: 0.9405\n",
      "109:\t[0s / 2s],\t\n",
      "110:\t[0s / 2s],\t\n",
      "111:\t[0s / 2s],\t\n",
      "112:\t[0s / 2s],\t\n",
      "113:\t[0s / 2s],\t\n",
      "114:\t[0s / 2s],\t\n",
      "115:\t[0s / 2s],\t\n",
      "116:\t[0s / 2s],\t\n",
      "117:\t[0s / 2s],\t\n",
      "118:\t[0s / 2s],\t\n",
      "N=76 Training time (2.38s): Train C-Index: 0.914 | Test C-index: 0.907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/dengy/dl-env/lib/python3.8/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df_noBatch, test_size=0.2, random_state=42, stratify=df_noBatch['delta'])\n",
    "\n",
    "subset = [train_df.shape[0]]\n",
    "ds = DeepSurvPipeline(train_df=train_df, test_df=test_df, dataName=\"with-batch\")\n",
    "ds_results = ds.run_deepsurv(subset, time_col='t', status_col='delta',\n",
    "                            batch_sizes=[8],\n",
    "                            patience=25, min_delta=1e-2, \n",
    "                            kwargs=parameters, verbose=True, print_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Simulation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_col='status'\n",
    "time_col = \"time\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear: Moderate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dimensions: (18000, 1035)\n",
      "Testing data dimensions:  (2000, 1035)\n"
     ]
    }
   ],
   "source": [
    "## Parameter setup\n",
    "# Load data\n",
    "folder = 'linear'\n",
    "keywords = ['moderate', \"latest\", 'RW']\n",
    "DATANAME = 'linear-moderate'\n",
    "train_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, test_size=0.1)\n",
    "\n",
    "print(f\"Training data dimensions: {train_df.shape}\")\n",
    "print(f\"Testing data dimensions:  {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [50, 500, 1000, 2000, 5000]\n",
    "batch_sizes =[8, 16, 16, 32, 64]\n",
    "runs = [20,20,10,10,5]\n",
    "hyperparams = {\n",
    "    \"learning_rate\": {\"type\": \"float\", \"low\": 1e-4, \"high\": 1e-2, \"log\": True},\n",
    "    # \"num_nodes\": {\"type\": \"categorical\", \"choices\": [[32,32], [16,16]]},\n",
    "    \"dropout\": {\"type\": \"categorical\", \"choices\": [0.1, 0.5]},\n",
    "    # \"batch_size\": {'type': \"categorical\", \"choices\": [32,64,128]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-07 16:55:05,755] A new study created in RDB with name: linear-moderate-50\n",
      "[I 2025-01-07 16:58:21,155] Trial 12 finished with value: 0.6597684517225162 and parameters: {'learning_rate': 0.004014731207287871, 'dropout': 0.5}. Best is trial 12 with value: 0.6597684517225162.\n",
      "[I 2025-01-07 16:58:30,881] Trial 6 finished with value: 0.705691086938837 and parameters: {'learning_rate': 0.0026546611830762187, 'dropout': 0.5}. Best is trial 6 with value: 0.705691086938837.\n",
      "[I 2025-01-07 16:58:31,475] Trial 13 finished with value: 0.5797497723914312 and parameters: {'learning_rate': 0.000194718270195271, 'dropout': 0.1}. Best is trial 6 with value: 0.705691086938837.\n",
      "[I 2025-01-07 16:58:31,654] Trial 0 finished with value: 0.5393251503338623 and parameters: {'learning_rate': 0.0001301035200430944, 'dropout': 0.5}. Best is trial 6 with value: 0.705691086938837.\n",
      "[I 2025-01-07 16:58:32,511] Trial 4 finished with value: 0.49468918534366013 and parameters: {'learning_rate': 0.0001433123360471385, 'dropout': 0.1}. Best is trial 6 with value: 0.705691086938837.\n",
      "[I 2025-01-07 16:58:32,562] Trial 11 finished with value: 0.5597022964076027 and parameters: {'learning_rate': 0.0013424147323391083, 'dropout': 0.1}. Best is trial 6 with value: 0.705691086938837.\n",
      "[I 2025-01-07 16:58:34,260] Trial 5 finished with value: 0.5706586762890312 and parameters: {'learning_rate': 0.00036366518326552563, 'dropout': 0.5}. Best is trial 6 with value: 0.705691086938837.\n",
      "[I 2025-01-07 16:58:38,190] Trial 14 finished with value: 0.6745537360376441 and parameters: {'learning_rate': 0.0028366946766874234, 'dropout': 0.5}. Best is trial 6 with value: 0.705691086938837.\n",
      "[I 2025-01-07 16:58:43,443] Trial 2 finished with value: 0.5960624263057843 and parameters: {'learning_rate': 0.00025927794666245196, 'dropout': 0.1}. Best is trial 6 with value: 0.705691086938837.\n",
      "[I 2025-01-07 16:58:50,935] Trial 9 finished with value: 0.5508346946211443 and parameters: {'learning_rate': 0.00028468255905584315, 'dropout': 0.1}. Best is trial 6 with value: 0.705691086938837.\n",
      "[I 2025-01-07 16:58:51,740] Trial 1 finished with value: 0.5343954027588558 and parameters: {'learning_rate': 0.00045046028493700576, 'dropout': 0.5}. Best is trial 6 with value: 0.705691086938837.\n",
      "[I 2025-01-07 16:58:52,226] Trial 7 finished with value: 0.6065262335044177 and parameters: {'learning_rate': 0.0003420183674172095, 'dropout': 0.5}. Best is trial 6 with value: 0.705691086938837.\n",
      "[I 2025-01-07 16:58:52,338] Trial 3 finished with value: 0.6848502302890667 and parameters: {'learning_rate': 0.00368143822062143, 'dropout': 0.1}. Best is trial 6 with value: 0.705691086938837.\n",
      "[I 2025-01-07 16:58:54,163] Trial 8 finished with value: 0.6603323692890977 and parameters: {'learning_rate': 0.00033715015699866324, 'dropout': 0.5}. Best is trial 6 with value: 0.705691086938837.\n",
      "[I 2025-01-07 16:58:54,219] Trial 10 finished with value: 0.5874052747837758 and parameters: {'learning_rate': 0.0010162910450433598, 'dropout': 0.5}. Best is trial 6 with value: 0.705691086938837.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0026546611830762187, 'dropout': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-07 16:59:43,701] A new study created in RDB with name: linear-moderate-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=50 Training time (0.2375s): Train C-Index: 0.789 | Test C-index: 0.61 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-07 17:02:45,740] Trial 8 finished with value: 0.7807337521066222 and parameters: {'learning_rate': 0.0028418050739308673, 'dropout': 0.1}. Best is trial 8 with value: 0.7807337521066222.\n",
      "[I 2025-01-07 17:03:02,719] Trial 5 finished with value: 0.7909293135773565 and parameters: {'learning_rate': 0.0018850874343691712, 'dropout': 0.1}. Best is trial 5 with value: 0.7909293135773565.\n",
      "[I 2025-01-07 17:03:21,979] Trial 9 finished with value: 0.7926057798315662 and parameters: {'learning_rate': 0.0010743659843153533, 'dropout': 0.1}. Best is trial 9 with value: 0.7926057798315662.\n",
      "[I 2025-01-07 17:03:25,427] Trial 10 finished with value: 0.7994840508189205 and parameters: {'learning_rate': 0.0017075104422977479, 'dropout': 0.1}. Best is trial 10 with value: 0.7994840508189205.\n",
      "[I 2025-01-07 17:03:29,081] Trial 4 finished with value: 0.7794749521745045 and parameters: {'learning_rate': 0.0006818761506251239, 'dropout': 0.1}. Best is trial 10 with value: 0.7994840508189205.\n",
      "[I 2025-01-07 17:03:38,326] Trial 13 finished with value: 0.7875049559164484 and parameters: {'learning_rate': 0.0006154632643364859, 'dropout': 0.1}. Best is trial 10 with value: 0.7994840508189205.\n",
      "[I 2025-01-07 17:03:45,174] Trial 2 finished with value: 0.801113675036612 and parameters: {'learning_rate': 0.008286175049263756, 'dropout': 0.5}. Best is trial 2 with value: 0.801113675036612.\n",
      "[I 2025-01-07 17:04:00,232] Trial 6 finished with value: 0.8004577468348438 and parameters: {'learning_rate': 0.003537160071739708, 'dropout': 0.5}. Best is trial 2 with value: 0.801113675036612.\n",
      "[I 2025-01-07 17:04:04,453] Trial 14 finished with value: 0.7980232540787858 and parameters: {'learning_rate': 0.00203716911402851, 'dropout': 0.5}. Best is trial 2 with value: 0.801113675036612.\n",
      "[I 2025-01-07 17:04:13,562] Trial 7 finished with value: 0.7413185133442937 and parameters: {'learning_rate': 0.0001786492490305536, 'dropout': 0.1}. Best is trial 2 with value: 0.801113675036612.\n",
      "[I 2025-01-07 17:04:17,043] Trial 12 finished with value: 0.7525934684867404 and parameters: {'learning_rate': 0.00016335268903193932, 'dropout': 0.1}. Best is trial 2 with value: 0.801113675036612.\n",
      "[I 2025-01-07 17:04:19,534] Trial 0 finished with value: 0.7983820905736174 and parameters: {'learning_rate': 0.0013244020373195627, 'dropout': 0.5}. Best is trial 2 with value: 0.801113675036612.\n",
      "[I 2025-01-07 17:04:30,682] Trial 3 finished with value: 0.7563015025250799 and parameters: {'learning_rate': 0.00023756728761512, 'dropout': 0.5}. Best is trial 2 with value: 0.801113675036612.\n",
      "[I 2025-01-07 17:04:31,534] Trial 11 finished with value: 0.7887595581925236 and parameters: {'learning_rate': 0.0005549076314852569, 'dropout': 0.5}. Best is trial 2 with value: 0.801113675036612.\n",
      "[I 2025-01-07 17:04:31,922] Trial 1 finished with value: 0.8057856397403643 and parameters: {'learning_rate': 0.0008244158673895692, 'dropout': 0.5}. Best is trial 1 with value: 0.8057856397403643.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0008244158673895692, 'dropout': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-07 17:05:54,673] Using an existing study with name 'linear-moderate-1000' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=500 Training time (1.8809999999999996s): Train C-Index: 0.882 | Test C-index: 0.774 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-07 17:10:24,350] Trial 40 finished with value: 0.8062546454077794 and parameters: {'learning_rate': 0.004228427982293931, 'dropout': 0.1}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:10:36,481] Trial 37 finished with value: 0.7997613924912255 and parameters: {'learning_rate': 0.004791503868503723, 'dropout': 0.1}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:11:01,191] Trial 41 finished with value: 0.8018266029459069 and parameters: {'learning_rate': 0.004510775580388289, 'dropout': 0.1}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:11:18,986] Trial 38 finished with value: 0.8032147687172598 and parameters: {'learning_rate': 0.003898202531046584, 'dropout': 0.1}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:11:24,095] Trial 32 finished with value: 0.8004345516212273 and parameters: {'learning_rate': 0.004531611231460206, 'dropout': 0.1}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:11:35,098] Trial 33 finished with value: 0.7984618896393559 and parameters: {'learning_rate': 0.0008253912916790287, 'dropout': 0.1}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:11:52,902] Trial 31 finished with value: 0.8057383343110847 and parameters: {'learning_rate': 0.004476149413308648, 'dropout': 0.5}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:12:07,046] Trial 34 finished with value: 0.8057866415857251 and parameters: {'learning_rate': 0.004998061780519396, 'dropout': 0.5}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:12:12,974] Trial 44 finished with value: 0.8068448078120015 and parameters: {'learning_rate': 0.0046506593994442855, 'dropout': 0.5}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:12:13,576] Trial 35 finished with value: 0.7990642860722563 and parameters: {'learning_rate': 0.004026743193516972, 'dropout': 0.5}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:12:15,284] Trial 30 finished with value: 0.8071353411347388 and parameters: {'learning_rate': 0.003753932187435146, 'dropout': 0.5}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:12:20,374] Trial 43 finished with value: 0.8081285188288628 and parameters: {'learning_rate': 0.005129606963153469, 'dropout': 0.5}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:12:21,130] Trial 36 finished with value: 0.8066254173847979 and parameters: {'learning_rate': 0.0037550610573570085, 'dropout': 0.5}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:12:22,186] Trial 39 finished with value: 0.807855057217824 and parameters: {'learning_rate': 0.003854606128929076, 'dropout': 0.5}. Best is trial 5 with value: 0.8131435667926394.\n",
      "[I 2025-01-07 17:12:26,335] Trial 42 finished with value: 0.810508695495584 and parameters: {'learning_rate': 0.0008263852809276259, 'dropout': 0.5}. Best is trial 5 with value: 0.8131435667926394.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0007641732585905166}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-07 17:13:23,689] Using an existing study with name 'linear-moderate-2000' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=1000 Training time (3.4379999999999997s): Train C-Index: 0.872 | Test C-index: 0.79 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-07 17:21:04,038] Trial 32 finished with value: 0.8078967252200584 and parameters: {'learning_rate': 0.0015315098916342965, 'dropout': 0.1}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:21:24,793] Trial 19 finished with value: 0.8111586613568449 and parameters: {'learning_rate': 0.0014353357710198095, 'dropout': 0.1}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:21:25,984] Trial 30 finished with value: 0.8045751378878535 and parameters: {'learning_rate': 0.0006455938478845993, 'dropout': 0.1}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:21:26,983] Trial 22 finished with value: 0.8043397016678435 and parameters: {'learning_rate': 0.0006140472895904695, 'dropout': 0.1}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:21:30,012] Trial 25 finished with value: 0.8019157011576267 and parameters: {'learning_rate': 0.0006906707248437058, 'dropout': 0.1}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:21:33,432] Trial 18 finished with value: 0.8050200596111784 and parameters: {'learning_rate': 0.000596201493790932, 'dropout': 0.1}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:21:40,631] Trial 21 finished with value: 0.8055218687245675 and parameters: {'learning_rate': 0.000618877751809391, 'dropout': 0.1}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:21:50,325] Trial 24 finished with value: 0.8094856948152431 and parameters: {'learning_rate': 0.0006472405931668014, 'dropout': 0.1}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:21:58,681] Trial 29 finished with value: 0.8077221910952674 and parameters: {'learning_rate': 0.0006826933373138872, 'dropout': 0.1}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:23:20,728] Trial 23 finished with value: 0.809755208892766 and parameters: {'learning_rate': 0.0007039223514190968, 'dropout': 0.5}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:23:23,859] Trial 31 finished with value: 0.8126008917456217 and parameters: {'learning_rate': 0.0015771836104340312, 'dropout': 0.5}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:23:32,468] Trial 28 finished with value: 0.8126128293603443 and parameters: {'learning_rate': 0.0006601647565521603, 'dropout': 0.5}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:23:38,670] Trial 27 finished with value: 0.8108498397080934 and parameters: {'learning_rate': 0.0007635365115061535, 'dropout': 0.5}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:23:44,811] Trial 26 finished with value: 0.8133961108646031 and parameters: {'learning_rate': 0.000583595246583933, 'dropout': 0.5}. Best is trial 8 with value: 0.8140903684040733.\n",
      "[I 2025-01-07 17:23:44,914] Trial 20 finished with value: 0.8127755128128138 and parameters: {'learning_rate': 0.0007002574855746063, 'dropout': 0.5}. Best is trial 8 with value: 0.8140903684040733.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0011600637818254633}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-07 17:25:05,385] Using an existing study with name 'linear-moderate-5000' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=2000 Training time (5.671s): Train C-Index: 0.852 | Test C-index: 0.803 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-07 17:38:43,889] Trial 20 finished with value: 0.8088291606509816 and parameters: {'learning_rate': 0.0025065611224212208, 'dropout': 0.1}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:39:13,365] Trial 13 finished with value: 0.8095005309384973 and parameters: {'learning_rate': 0.002762043830169803, 'dropout': 0.1}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:40:14,333] Trial 21 finished with value: 0.8120118260779581 and parameters: {'learning_rate': 0.0020434670021805177, 'dropout': 0.1}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:40:53,252] Trial 16 finished with value: 0.8082720156102988 and parameters: {'learning_rate': 0.0023752433589418827, 'dropout': 0.5}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:40:55,689] Trial 10 finished with value: 0.8117096872092645 and parameters: {'learning_rate': 0.0016083435458083227, 'dropout': 0.1}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:41:22,687] Trial 17 finished with value: 0.8076459264828246 and parameters: {'learning_rate': 0.0028561823288349004, 'dropout': 0.5}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:41:42,318] Trial 24 finished with value: 0.8071456659491606 and parameters: {'learning_rate': 0.0023239855939146538, 'dropout': 0.5}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:41:49,711] Trial 14 finished with value: 0.8082651622366361 and parameters: {'learning_rate': 0.0019076023226084387, 'dropout': 0.5}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:41:51,126] Trial 15 finished with value: 0.8121701360827644 and parameters: {'learning_rate': 0.002143191641281378, 'dropout': 0.1}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:42:00,352] Trial 11 finished with value: 0.806461659579268 and parameters: {'learning_rate': 0.002326195136783299, 'dropout': 0.5}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:42:28,558] Trial 22 finished with value: 0.8098407464280148 and parameters: {'learning_rate': 0.001946955989118413, 'dropout': 0.5}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:42:30,804] Trial 19 finished with value: 0.8103640085648539 and parameters: {'learning_rate': 0.0020362311670155705, 'dropout': 0.5}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:42:41,986] Trial 23 finished with value: 0.8083078669652674 and parameters: {'learning_rate': 0.0022342038368732644, 'dropout': 0.5}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:42:42,296] Trial 12 finished with value: 0.8098398507580553 and parameters: {'learning_rate': 0.0019008835440668453, 'dropout': 0.5}. Best is trial 4 with value: 0.8148163147073626.\n",
      "[I 2025-01-07 17:42:45,711] Trial 18 finished with value: 0.8115146475609801 and parameters: {'learning_rate': 0.0019821120395205187, 'dropout': 0.5}. Best is trial 4 with value: 0.8148163147073626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.00036212251189812105}\n",
      "N=5000 Training time (15.190000000000001s): Train C-Index: 0.842 | Test C-index: 0.814 (Mean)\n"
     ]
    }
   ],
   "source": [
    "best_params_ls, n_train, model_time, train_scores, test_scores = [],[],[],[],[]\n",
    "\n",
    "for (n, batch_size, n_run) in zip(subset, batch_sizes, runs): \n",
    "    \n",
    "    run_train_scores, run_test_scores, run_time = [],[],[]\n",
    "    for run in range(n_run+1):\n",
    "        if n < train_df.shape[0]:\n",
    "            train_sub,_ = train_test_split(train_df,\n",
    "                                        train_size=n/train_df.shape[0], \n",
    "                                        shuffle=True, random_state=run,\n",
    "                                        stratify=train_df[status_col])\n",
    "        else:\n",
    "            train_sub = train_df\n",
    "        \n",
    "        ds = DeepSurvPipeline(train_sub, test_df, hyperparameters=hyperparams, dataName=DATANAME)\n",
    "        if run == 0:\n",
    "            study = ds.tune_hyperparameters(train_sub, n_trials=15, n_jobs=16)\n",
    "            best_params = study.best_params\n",
    "            continue\n",
    "        duration, train_c, test_c = ds.train_with_best_params(params=best_params, verbose=False, print_scores=False)\n",
    "        \n",
    "        n_train.append(n)\n",
    "        model_time.append(duration)\n",
    "        train_scores.append(train_c)\n",
    "        test_scores.append(test_c)\n",
    "        best_params_ls.append(best_params)\n",
    "        \n",
    "        run_train_scores.append(train_c)\n",
    "        run_test_scores.append(test_c)\n",
    "        run_time.append(duration)\n",
    "    \n",
    "    print(f\"N={n} Training time ({np.mean(run_time)}s): Train C-Index: {round(np.mean(run_train_scores),3)} | Test C-index: {round(np.mean(run_test_scores),3)} (Mean)\")                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear: Weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event rate in train set: 0.750333\n",
      "Event rate in test set: 0.750500\n",
      "Survival time distribution:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data\n",
    "folder = 'linear'\n",
    "keywords = ['weak', \"latest\", '20000']\n",
    "DATANAME = 'linear-weak'\n",
    "# train_df, val_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, val_split=True)\n",
    "train_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, test_size=0.1)\n",
    "\n",
    "# observe data\n",
    "print(\"Event rate in train set: %f\" % (sum(train_df['status']==1) / train_df.shape[0]))\n",
    "print(\"Event rate in test set: %f\" %  (sum(test_df['status']==1) / test_df.shape[0]))\n",
    "print('Survival time distribution:')\n",
    "_, ax = plt.subplots(figsize=(3,3))\n",
    "ax.hist(train_df['time'], label='train')\n",
    "# ax.hist(val_df['time'],   label='val', alpha=.8)\n",
    "ax.hist(test_df['time'], label='test', alpha=0.6)\n",
    "ax.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [50,500,1000, 2000,5000]\n",
    "batch_sizes =[8, 16, 16, 32, 64]\n",
    "runs = [10,10,10,10,10]\n",
    "hyperparams = {\n",
    "    \"learning_rate\": {\"type\": \"float\", \"low\": 1e-4, \"high\": 1e-2, \"log\": True},\n",
    "    # \"num_nodes\": {\"type\": \"categorical\", \"choices\": [[32,32], [16,16]]},\n",
    "    \"dropout\": {\"type\": \"categorical\", \"choices\": [0.1,0.5]},\n",
    "    # \"batch_size\": {'type': \"categorical\", \"choices\": [32,64,128]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-19 13:39:06,780] A new study created in RDB with name: linear-weak-50\n",
      "[I 2024-12-19 13:42:19,865] Trial 2 finished with value: 0.5709425631839424 and parameters: {'learning_rate': 0.0001736726273715398, 'dropout': 0.5}. Best is trial 2 with value: 0.5709425631839424.\n",
      "[I 2024-12-19 13:42:25,851] Trial 8 finished with value: 0.5075163342404722 and parameters: {'learning_rate': 0.000394379427207373, 'dropout': 0.1}. Best is trial 2 with value: 0.5709425631839424.\n",
      "[I 2024-12-19 13:42:33,208] Trial 6 finished with value: 0.5848225912019015 and parameters: {'learning_rate': 0.0033361936894263314, 'dropout': 0.1}. Best is trial 6 with value: 0.5848225912019015.\n",
      "[I 2024-12-19 13:42:33,327] Trial 4 finished with value: 0.6891083054876159 and parameters: {'learning_rate': 0.0011164339757238563, 'dropout': 0.1}. Best is trial 4 with value: 0.6891083054876159.\n",
      "[I 2024-12-19 13:42:34,975] Trial 1 finished with value: 0.5643491565905359 and parameters: {'learning_rate': 0.00031268287342646586, 'dropout': 0.1}. Best is trial 4 with value: 0.6891083054876159.\n",
      "[I 2024-12-19 13:42:38,050] Trial 7 finished with value: 0.7390286150630978 and parameters: {'learning_rate': 0.003000504349779096, 'dropout': 0.1}. Best is trial 7 with value: 0.7390286150630978.\n",
      "[I 2024-12-19 13:42:38,529] Trial 0 finished with value: 0.5538865157830675 and parameters: {'learning_rate': 0.0008395692300773494, 'dropout': 0.5}. Best is trial 7 with value: 0.7390286150630978.\n",
      "[I 2024-12-19 13:42:39,701] Trial 12 finished with value: 0.6164312124656952 and parameters: {'learning_rate': 0.001976371525221072, 'dropout': 0.5}. Best is trial 7 with value: 0.7390286150630978.\n",
      "[I 2024-12-19 13:42:46,201] Trial 11 finished with value: 0.4227234259992881 and parameters: {'learning_rate': 0.00014401292665440214, 'dropout': 0.5}. Best is trial 7 with value: 0.7390286150630978.\n",
      "[I 2024-12-19 13:42:46,663] Trial 5 finished with value: 0.5799141662934766 and parameters: {'learning_rate': 0.002851159881951608, 'dropout': 0.5}. Best is trial 7 with value: 0.7390286150630978.\n",
      "[I 2024-12-19 13:42:46,953] Trial 3 finished with value: 0.555491462560428 and parameters: {'learning_rate': 0.007661217745623652, 'dropout': 0.5}. Best is trial 7 with value: 0.7390286150630978.\n",
      "[I 2024-12-19 13:42:47,073] Trial 9 finished with value: 0.5137168004409384 and parameters: {'learning_rate': 0.005108373683031822, 'dropout': 0.5}. Best is trial 7 with value: 0.7390286150630978.\n",
      "[I 2024-12-19 13:42:48,837] Trial 14 finished with value: 0.5172923053957537 and parameters: {'learning_rate': 0.0004786782682061343, 'dropout': 0.5}. Best is trial 7 with value: 0.7390286150630978.\n",
      "[I 2024-12-19 13:42:48,869] Trial 13 finished with value: 0.4911749744508366 and parameters: {'learning_rate': 0.0001789000078683193, 'dropout': 0.1}. Best is trial 7 with value: 0.7390286150630978.\n",
      "[I 2024-12-19 13:42:48,933] Trial 10 finished with value: 0.577300228507125 and parameters: {'learning_rate': 0.00012410447068823415, 'dropout': 0.1}. Best is trial 7 with value: 0.7390286150630978.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.003000504349779096, 'dropout': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-19 13:43:12,476] A new study created in RDB with name: linear-weak-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=50 Training time (0.164s): Train C-Index: 0.757 | Test C-index: 0.528 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-19 13:46:16,315] Trial 6 finished with value: 0.6202960287310872 and parameters: {'learning_rate': 0.0027708892195662652, 'dropout': 0.1}. Best is trial 6 with value: 0.6202960287310872.\n",
      "[I 2024-12-19 13:46:20,695] Trial 7 finished with value: 0.609006332548231 and parameters: {'learning_rate': 0.001155760320497242, 'dropout': 0.1}. Best is trial 6 with value: 0.6202960287310872.\n",
      "[I 2024-12-19 13:46:28,681] Trial 12 finished with value: 0.6329414058423761 and parameters: {'learning_rate': 0.003900276255359308, 'dropout': 0.1}. Best is trial 12 with value: 0.6329414058423761.\n",
      "[I 2024-12-19 13:46:33,544] Trial 1 finished with value: 0.5859278039345754 and parameters: {'learning_rate': 0.0004046185899388891, 'dropout': 0.1}. Best is trial 12 with value: 0.6329414058423761.\n",
      "[I 2024-12-19 13:46:40,954] Trial 0 finished with value: 0.5754724633145752 and parameters: {'learning_rate': 0.0002736996742796856, 'dropout': 0.1}. Best is trial 12 with value: 0.6329414058423761.\n",
      "[I 2024-12-19 13:46:59,668] Trial 9 finished with value: 0.6093764811909801 and parameters: {'learning_rate': 0.0009648171590544319, 'dropout': 0.5}. Best is trial 12 with value: 0.6329414058423761.\n",
      "[I 2024-12-19 13:47:09,558] Trial 4 finished with value: 0.6115577264011038 and parameters: {'learning_rate': 0.00033651697791499375, 'dropout': 0.1}. Best is trial 12 with value: 0.6329414058423761.\n",
      "[I 2024-12-19 13:47:14,483] Trial 8 finished with value: 0.6171132828859418 and parameters: {'learning_rate': 0.0003348373990612675, 'dropout': 0.5}. Best is trial 12 with value: 0.6329414058423761.\n",
      "[I 2024-12-19 13:47:18,273] Trial 14 finished with value: 0.5632498682475273 and parameters: {'learning_rate': 0.00017331985926755637, 'dropout': 0.1}. Best is trial 12 with value: 0.6329414058423761.\n",
      "[I 2024-12-19 13:47:18,371] Trial 2 finished with value: 0.6015722400441532 and parameters: {'learning_rate': 0.0004364568171594753, 'dropout': 0.5}. Best is trial 12 with value: 0.6329414058423761.\n",
      "[I 2024-12-19 13:47:19,509] Trial 11 finished with value: 0.6361341346389568 and parameters: {'learning_rate': 0.0016962878776665866, 'dropout': 0.5}. Best is trial 11 with value: 0.6361341346389568.\n",
      "[I 2024-12-19 13:47:22,541] Trial 5 finished with value: 0.6427428632721242 and parameters: {'learning_rate': 0.004853164341959896, 'dropout': 0.5}. Best is trial 5 with value: 0.6427428632721242.\n",
      "[I 2024-12-19 13:47:22,625] Trial 3 finished with value: 0.6422568773228776 and parameters: {'learning_rate': 0.0036229065178781574, 'dropout': 0.5}. Best is trial 5 with value: 0.6427428632721242.\n",
      "[I 2024-12-19 13:47:22,805] Trial 10 finished with value: 0.5963154278433158 and parameters: {'learning_rate': 0.00038626005041487847, 'dropout': 0.5}. Best is trial 5 with value: 0.6427428632721242.\n",
      "[I 2024-12-19 13:47:23,034] Trial 13 finished with value: 0.6416245672408231 and parameters: {'learning_rate': 0.002597333284967014, 'dropout': 0.5}. Best is trial 5 with value: 0.6427428632721242.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.004853164341959896, 'dropout': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-19 13:47:57,394] A new study created in RDB with name: linear-weak-1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=500 Training time (1.206s): Train C-Index: 0.773 | Test C-index: 0.648 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-19 13:52:28,055] Trial 9 finished with value: 0.664194623272976 and parameters: {'learning_rate': 0.00144567954633041, 'dropout': 0.1}. Best is trial 9 with value: 0.664194623272976.\n",
      "[I 2024-12-19 13:52:35,642] Trial 5 finished with value: 0.6790018673176454 and parameters: {'learning_rate': 0.0018830862423128002, 'dropout': 0.1}. Best is trial 5 with value: 0.6790018673176454.\n",
      "[I 2024-12-19 13:52:45,845] Trial 12 finished with value: 0.6658042556680086 and parameters: {'learning_rate': 0.0009809859186989827, 'dropout': 0.1}. Best is trial 5 with value: 0.6790018673176454.\n",
      "[I 2024-12-19 13:52:57,005] Trial 10 finished with value: 0.6692508510321595 and parameters: {'learning_rate': 0.0006848671313275324, 'dropout': 0.1}. Best is trial 5 with value: 0.6790018673176454.\n",
      "[I 2024-12-19 13:53:07,023] Trial 14 finished with value: 0.6743702453089101 and parameters: {'learning_rate': 0.008689497584497559, 'dropout': 0.1}. Best is trial 5 with value: 0.6790018673176454.\n",
      "[I 2024-12-19 13:53:23,817] Trial 11 finished with value: 0.6520099115127777 and parameters: {'learning_rate': 0.00022837603909456203, 'dropout': 0.1}. Best is trial 5 with value: 0.6790018673176454.\n",
      "[I 2024-12-19 13:54:13,145] Trial 1 finished with value: 0.6794107212105422 and parameters: {'learning_rate': 0.001466332894775375, 'dropout': 0.5}. Best is trial 1 with value: 0.6794107212105422.\n",
      "[I 2024-12-19 13:54:17,187] Trial 0 finished with value: 0.6787689166319726 and parameters: {'learning_rate': 0.0015850046974908239, 'dropout': 0.5}. Best is trial 1 with value: 0.6794107212105422.\n",
      "[I 2024-12-19 13:54:21,916] Trial 2 finished with value: 0.6531479284146385 and parameters: {'learning_rate': 0.00015208629746729996, 'dropout': 0.1}. Best is trial 1 with value: 0.6794107212105422.\n",
      "[I 2024-12-19 13:54:22,390] Trial 13 finished with value: 0.6729480791505388 and parameters: {'learning_rate': 0.0024596802965400322, 'dropout': 0.5}. Best is trial 1 with value: 0.6794107212105422.\n",
      "[I 2024-12-19 13:54:24,199] Trial 4 finished with value: 0.6453385781958103 and parameters: {'learning_rate': 0.00014724944533870337, 'dropout': 0.1}. Best is trial 1 with value: 0.6794107212105422.\n",
      "[I 2024-12-19 13:54:43,349] Trial 7 finished with value: 0.6863467497252598 and parameters: {'learning_rate': 0.0009174678091085844, 'dropout': 0.5}. Best is trial 7 with value: 0.6863467497252598.\n",
      "[I 2024-12-19 13:54:45,068] Trial 8 finished with value: 0.6664819000165068 and parameters: {'learning_rate': 0.00037885362411838584, 'dropout': 0.5}. Best is trial 7 with value: 0.6863467497252598.\n",
      "[I 2024-12-19 13:54:47,675] Trial 6 finished with value: 0.6730714489159955 and parameters: {'learning_rate': 0.00040321607447806724, 'dropout': 0.5}. Best is trial 7 with value: 0.6863467497252598.\n",
      "[I 2024-12-19 13:54:47,888] Trial 3 finished with value: 0.6675402103258506 and parameters: {'learning_rate': 0.00019525357003519043, 'dropout': 0.5}. Best is trial 7 with value: 0.6863467497252598.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0009174678091085844, 'dropout': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-19 13:55:37,465] A new study created in RDB with name: linear-weak-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=1000 Training time (2.7159999999999997s): Train C-Index: 0.784 | Test C-index: 0.667 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-19 14:02:48,327] Trial 9 finished with value: 0.6877153508484285 and parameters: {'learning_rate': 0.00499257746929855, 'dropout': 0.1}. Best is trial 9 with value: 0.6877153508484285.\n",
      "[I 2024-12-19 14:02:50,264] Trial 12 finished with value: 0.6903910019419387 and parameters: {'learning_rate': 0.0006174942374684412, 'dropout': 0.1}. Best is trial 12 with value: 0.6903910019419387.\n",
      "[I 2024-12-19 14:03:07,976] Trial 3 finished with value: 0.6807116926735821 and parameters: {'learning_rate': 0.005368397644866624, 'dropout': 0.5}. Best is trial 12 with value: 0.6903910019419387.\n",
      "[I 2024-12-19 14:03:24,104] Trial 0 finished with value: 0.6850815094093996 and parameters: {'learning_rate': 0.006144138244668673, 'dropout': 0.1}. Best is trial 12 with value: 0.6903910019419387.\n",
      "[I 2024-12-19 14:04:00,099] Trial 13 finished with value: 0.6873728534343222 and parameters: {'learning_rate': 0.0042128339532117726, 'dropout': 0.5}. Best is trial 12 with value: 0.6903910019419387.\n",
      "[I 2024-12-19 14:04:06,274] Trial 7 finished with value: 0.6854710816563098 and parameters: {'learning_rate': 0.0035484348246187457, 'dropout': 0.5}. Best is trial 12 with value: 0.6903910019419387.\n",
      "[I 2024-12-19 14:04:15,370] Trial 4 finished with value: 0.6767480597158964 and parameters: {'learning_rate': 0.00024827371173529985, 'dropout': 0.1}. Best is trial 12 with value: 0.6903910019419387.\n",
      "[I 2024-12-19 14:04:30,585] Trial 2 finished with value: 0.6896753475920983 and parameters: {'learning_rate': 0.005021009158820669, 'dropout': 0.5}. Best is trial 12 with value: 0.6903910019419387.\n",
      "[I 2024-12-19 14:04:36,993] Trial 8 finished with value: 0.6908678041670544 and parameters: {'learning_rate': 0.0025490730003396465, 'dropout': 0.5}. Best is trial 8 with value: 0.6908678041670544.\n",
      "[I 2024-12-19 14:04:40,910] Trial 5 finished with value: 0.7020948240927448 and parameters: {'learning_rate': 0.0018096840486263746, 'dropout': 0.5}. Best is trial 5 with value: 0.7020948240927448.\n",
      "[I 2024-12-19 14:04:58,614] Trial 6 finished with value: 0.7008013073173534 and parameters: {'learning_rate': 0.001768398322675059, 'dropout': 0.5}. Best is trial 5 with value: 0.7020948240927448.\n",
      "[I 2024-12-19 14:05:27,318] Trial 1 finished with value: 0.695716317642468 and parameters: {'learning_rate': 0.0005911063067665655, 'dropout': 0.5}. Best is trial 5 with value: 0.7020948240927448.\n",
      "[I 2024-12-19 14:06:21,812] Trial 10 finished with value: 0.6912309974649851 and parameters: {'learning_rate': 0.00031679275898918155, 'dropout': 0.5}. Best is trial 5 with value: 0.7020948240927448.\n",
      "[I 2024-12-19 14:06:22,027] Trial 11 finished with value: 0.695078724122762 and parameters: {'learning_rate': 0.00025344108450958886, 'dropout': 0.5}. Best is trial 5 with value: 0.7020948240927448.\n",
      "[I 2024-12-19 14:06:25,758] Trial 14 finished with value: 0.6787135120043578 and parameters: {'learning_rate': 0.00011742456682373994, 'dropout': 0.5}. Best is trial 5 with value: 0.7020948240927448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0018096840486263746, 'dropout': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-19 14:07:46,646] A new study created in RDB with name: linear-weak-5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=2000 Training time (5.773000000000001s): Train C-Index: 0.764 | Test C-index: 0.685 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-19 14:20:08,721] Trial 9 finished with value: 0.6779651478128734 and parameters: {'learning_rate': 0.006905318282056183, 'dropout': 0.5}. Best is trial 9 with value: 0.6779651478128734.\n",
      "[I 2024-12-19 14:21:09,451] Trial 3 finished with value: 0.6720365036810054 and parameters: {'learning_rate': 0.009272237946647034, 'dropout': 0.5}. Best is trial 9 with value: 0.6779651478128734.\n",
      "[I 2024-12-19 14:22:48,447] Trial 14 finished with value: 0.6955547235673682 and parameters: {'learning_rate': 0.006265641829045784, 'dropout': 0.1}. Best is trial 14 with value: 0.6955547235673682.\n",
      "[I 2024-12-19 14:23:26,410] Trial 12 finished with value: 0.6847508698682673 and parameters: {'learning_rate': 0.006308917852564534, 'dropout': 0.5}. Best is trial 14 with value: 0.6955547235673682.\n",
      "[I 2024-12-19 14:23:40,988] Trial 13 finished with value: 0.7089572566059041 and parameters: {'learning_rate': 0.0007334095533844638, 'dropout': 0.1}. Best is trial 13 with value: 0.7089572566059041.\n",
      "[I 2024-12-19 14:23:48,243] Trial 0 finished with value: 0.7043124635555976 and parameters: {'learning_rate': 0.004207596948784063, 'dropout': 0.1}. Best is trial 13 with value: 0.7089572566059041.\n",
      "[I 2024-12-19 14:24:07,526] Trial 6 finished with value: 0.7055288721674533 and parameters: {'learning_rate': 0.0006066695232212387, 'dropout': 0.1}. Best is trial 13 with value: 0.7089572566059041.\n",
      "[I 2024-12-19 14:24:08,373] Trial 2 finished with value: 0.7118643187827235 and parameters: {'learning_rate': 0.0005086376543049847, 'dropout': 0.1}. Best is trial 2 with value: 0.7118643187827235.\n",
      "[I 2024-12-19 14:24:38,422] Trial 8 finished with value: 0.6999368893629525 and parameters: {'learning_rate': 0.005247258541199346, 'dropout': 0.1}. Best is trial 2 with value: 0.7118643187827235.\n",
      "[I 2024-12-19 14:24:49,496] Trial 1 finished with value: 0.7065351263450821 and parameters: {'learning_rate': 0.00031698040565182915, 'dropout': 0.1}. Best is trial 2 with value: 0.7118643187827235.\n",
      "[I 2024-12-19 14:24:51,279] Trial 11 finished with value: 0.6985877495166726 and parameters: {'learning_rate': 0.0036370040494841382, 'dropout': 0.5}. Best is trial 2 with value: 0.7118643187827235.\n",
      "[I 2024-12-19 14:24:51,762] Trial 5 finished with value: 0.7060532028082289 and parameters: {'learning_rate': 0.0024064189070485274, 'dropout': 0.5}. Best is trial 2 with value: 0.7118643187827235.\n",
      "[I 2024-12-19 14:25:05,847] Trial 4 finished with value: 0.7038639179049812 and parameters: {'learning_rate': 0.00021947617557521487, 'dropout': 0.1}. Best is trial 2 with value: 0.7118643187827235.\n",
      "[I 2024-12-19 14:25:59,166] Trial 10 finished with value: 0.714036445143503 and parameters: {'learning_rate': 0.0002803110737743245, 'dropout': 0.5}. Best is trial 10 with value: 0.714036445143503.\n",
      "[I 2024-12-19 14:26:13,404] Trial 7 finished with value: 0.7107408648483794 and parameters: {'learning_rate': 0.00013992932228906575, 'dropout': 0.5}. Best is trial 10 with value: 0.714036445143503.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0002803110737743245, 'dropout': 0.5}\n",
      "N=5000 Training time (15.019s): Train C-Index: 0.758 | Test C-index: 0.701 (Mean)\n"
     ]
    }
   ],
   "source": [
    "best_params_ls, n_train, model_time, train_scores, test_scores = [],[],[],[],[]\n",
    "\n",
    "for (n, batch_size, n_run) in zip(subset, batch_sizes, runs): \n",
    "    \n",
    "    run_train_scores, run_test_scores, run_time = [],[],[]\n",
    "    for run in range(n_run+1):\n",
    "        if n < train_df.shape[0]:\n",
    "            train_sub,_ = train_test_split(train_df,\n",
    "                                        train_size=n/train_df.shape[0], \n",
    "                                        shuffle=True, random_state=run,\n",
    "                                        stratify=train_df[status_col])\n",
    "        else:\n",
    "            train_sub = train_df\n",
    "        \n",
    "        ds = DeepSurvPipeline(train_sub, test_df, hyperparameters=hyperparams, dataName=DATANAME)\n",
    "        if run == 0:\n",
    "            study = ds.tune_hyperparameters(train_sub, n_trials=15, n_jobs=16)\n",
    "            best_params = study.best_params\n",
    "            best_params_ls.append(best_params)\n",
    "            continue\n",
    "        duration, train_c, test_c = ds.train_with_best_params(params=best_params, verbose=False, print_scores=False)\n",
    "        \n",
    "        n_train.append(n)\n",
    "        model_time.append(duration)\n",
    "        train_scores.append(train_c)\n",
    "        test_scores.append(test_c)\n",
    "            \n",
    "        run_train_scores.append(train_c)\n",
    "        run_test_scores.append(test_c)\n",
    "        run_time.append(duration)\n",
    "    \n",
    "    print(f\"N={n} Training time ({np.mean(run_time)}s): Train C-Index: {round(np.mean(run_train_scores),3)} | Test C-index: {round(np.mean(run_test_scores),3)} (Mean)\")                \n",
    "\n",
    "# ============== Save results ===============\n",
    "model_results = pd.DataFrame({\n",
    "    'n train': n_train, \n",
    "    'train time':model_time,\n",
    "    'train score':train_scores, \n",
    "    'test score':test_scores}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.write(model_results=model_results, fileName=\"model.results.10runs.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear: Quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event rate in train set: 0.746222\n",
      "Event rate in test set: 0.746000\n",
      "Survival time distribution:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data\n",
    "folder = 'nonlinear'\n",
    "keywords = ['quadratic', \"latest\", '20000']\n",
    "DATANAME = 'nl-quadratic'\n",
    "# train_df, val_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, val_split=True)\n",
    "train_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, test_size=0.1)\n",
    "plot_simulation_data(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [50, 500, 1000, 2000, 5000]\n",
    "batch_sizes =[8, 16, 16, 32, 64]\n",
    "runs = [20,20,10,10,10]\n",
    "hyperparams = {\n",
    "    \"learning_rate\": {\"type\": \"float\", \"low\": 1e-4, \"high\": 1e-2, \"log\": True},\n",
    "    \"num_nodes\": {\"type\": \"categorical\", \"choices\": [[32,32], [16,16]]},\n",
    "    \"dropout\": {\"type\": \"categorical\", \"choices\": [0.1,0.5]},\n",
    "    # \"batch_size\": {'type': \"categorical\", \"choices\": [32,64,128]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 11:17:06,902] Using an existing study with name 'nl-quadratic-50' instead of creating a new one.\n",
      "[I 2025-01-06 11:21:30,648] Trial 49 finished with value: 0.6869830659536542 and parameters: {'learning_rate': 0.00510971837679028, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:21:40,503] Trial 36 finished with value: 0.6496836007130125 and parameters: {'learning_rate': 0.005247316158886313, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:21:40,530] Trial 41 finished with value: 0.6635847975553857 and parameters: {'learning_rate': 0.00500918802141851, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:21:46,122] Trial 50 finished with value: 0.6195588235294117 and parameters: {'learning_rate': 0.005792686174354546, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:21:51,428] Trial 42 finished with value: 0.6272962821492234 and parameters: {'learning_rate': 0.004929109086826519, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:21:54,720] Trial 37 finished with value: 0.6705494015788134 and parameters: {'learning_rate': 0.004942028419234184, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:21:56,179] Trial 43 finished with value: 0.6580920550038197 and parameters: {'learning_rate': 0.005684251833912963, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:21:56,457] Trial 47 finished with value: 0.7091590272472625 and parameters: {'learning_rate': 0.00506362824485706, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:21:57,013] Trial 40 finished with value: 0.6789425770308124 and parameters: {'learning_rate': 0.004264057237037251, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:21:58,746] Trial 45 finished with value: 0.6619397759103641 and parameters: {'learning_rate': 0.005116032517655661, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:22:00,453] Trial 48 finished with value: 0.6878705118411002 and parameters: {'learning_rate': 0.0052709355509590805, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:22:02,873] Trial 39 finished with value: 0.6612585943468295 and parameters: {'learning_rate': 0.005061194412687424, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:22:03,688] Trial 35 finished with value: 0.696744970715559 and parameters: {'learning_rate': 0.0050949238178329736, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:22:03,931] Trial 44 finished with value: 0.6889909600203719 and parameters: {'learning_rate': 0.005188146615284437, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:22:12,506] Trial 38 finished with value: 0.6253596893302775 and parameters: {'learning_rate': 0.005231555153492605, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:22:12,663] Trial 46 finished with value: 0.6801648841354724 and parameters: {'learning_rate': 0.005155664373362866, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:22:30,854] Trial 53 finished with value: 0.6173548510313216 and parameters: {'learning_rate': 0.003106891516414615, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:22:31,027] Trial 54 finished with value: 0.5012433155080214 and parameters: {'learning_rate': 0.00015129313819666913, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:22:31,349] Trial 51 finished with value: 0.6871778711484595 and parameters: {'learning_rate': 0.005485384162997585, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n",
      "[I 2025-01-06 11:22:32,982] Trial 52 finished with value: 0.6751330532212885 and parameters: {'learning_rate': 0.002506959922150669, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 25 with value: 0.7420874713521772.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0015890814833935034, 'dropout': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 11:23:22,742] Using an existing study with name 'nl-quadratic-500' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=50 Training time (0.21399999999999997s): Train C-Index: 0.834 | Test C-index: 0.546 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 11:27:17,625] Trial 47 finished with value: 0.6657045936779126 and parameters: {'learning_rate': 0.004101402436319301, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 32 with value: 0.6949629383750308.\n",
      "[I 2025-01-06 11:27:34,757] Trial 41 finished with value: 0.662133031119079 and parameters: {'learning_rate': 0.004431160815456842, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 32 with value: 0.6949629383750308.\n",
      "[I 2025-01-06 11:27:47,663] Trial 37 finished with value: 0.6670842057211886 and parameters: {'learning_rate': 0.003790095810549351, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 32 with value: 0.6949629383750308.\n",
      "[I 2025-01-06 11:27:50,120] Trial 45 finished with value: 0.7035422519361789 and parameters: {'learning_rate': 0.004437688519422004, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:27:52,155] Trial 44 finished with value: 0.6691903309325513 and parameters: {'learning_rate': 0.004313401748333162, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:27:52,592] Trial 36 finished with value: 0.677620517580049 and parameters: {'learning_rate': 0.0040627874646236095, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:27:54,262] Trial 49 finished with value: 0.6878863272331488 and parameters: {'learning_rate': 0.0045057378511690355, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:27:54,394] Trial 50 finished with value: 0.6600751462364202 and parameters: {'learning_rate': 0.004510864404029403, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:27:55,371] Trial 39 finished with value: 0.6845154290598512 and parameters: {'learning_rate': 0.004180218819310049, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:27:56,928] Trial 38 finished with value: 0.6880301984107818 and parameters: {'learning_rate': 0.004209863200478561, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:27:58,159] Trial 46 finished with value: 0.6710283703495137 and parameters: {'learning_rate': 0.004937193375767752, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:27:58,413] Trial 40 finished with value: 0.6859644309748767 and parameters: {'learning_rate': 0.0033284140230074983, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:27:59,087] Trial 35 finished with value: 0.6941938970917041 and parameters: {'learning_rate': 0.004349870555753369, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:27:59,356] Trial 48 finished with value: 0.683118128059963 and parameters: {'learning_rate': 0.003612459736290451, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:27:59,437] Trial 43 finished with value: 0.668656827285755 and parameters: {'learning_rate': 0.004441841901970785, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:27:59,451] Trial 42 finished with value: 0.6775828965922046 and parameters: {'learning_rate': 0.004180781828075492, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:28:34,961] Trial 51 finished with value: 0.6669209212114063 and parameters: {'learning_rate': 0.0012613991012325753, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:28:35,759] Trial 52 finished with value: 0.6710206235898655 and parameters: {'learning_rate': 0.0028440660185204856, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:28:36,002] Trial 53 finished with value: 0.6806123747063193 and parameters: {'learning_rate': 0.005298940848788909, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 45 with value: 0.7035422519361789.\n",
      "[I 2025-01-06 11:28:37,827] Trial 54 finished with value: 0.6693172587383707 and parameters: {'learning_rate': 0.005562381078252014, 'num_nodes': [32, 32], 'dropout': 0.5}. Best is trial 45 with value: 0.7035422519361789.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.004437688519422004, 'num_nodes': [32, 32], 'dropout': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 11:29:40,682] Using an existing study with name 'nl-quadratic-1000' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=500 Training time (0.8700000000000001s): Train C-Index: 0.818 | Test C-index: 0.66 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 11:36:41,087] Trial 45 finished with value: 0.6891911346657277 and parameters: {'learning_rate': 0.0029449732501657727, 'num_nodes': [16, 16], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:37:02,776] Trial 50 finished with value: 0.6922109962536334 and parameters: {'learning_rate': 0.0028615207471592813, 'num_nodes': [16, 16], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:37:03,223] Trial 39 finished with value: 0.6929203856198626 and parameters: {'learning_rate': 0.002777538060315972, 'num_nodes': [32, 32], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:37:21,834] Trial 40 finished with value: 0.7059605493833041 and parameters: {'learning_rate': 0.0027825489260210764, 'num_nodes': [32, 32], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:37:23,032] Trial 42 finished with value: 0.7086813955140994 and parameters: {'learning_rate': 0.0029129697282007125, 'num_nodes': [32, 32], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:37:30,062] Trial 44 finished with value: 0.6945779261409897 and parameters: {'learning_rate': 0.0026668278054114048, 'num_nodes': [16, 16], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:37:38,712] Trial 48 finished with value: 0.7087167861716951 and parameters: {'learning_rate': 0.0025858176508988784, 'num_nodes': [32, 32], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:37:44,592] Trial 38 finished with value: 0.6955638540836295 and parameters: {'learning_rate': 0.0028752119242620735, 'num_nodes': [16, 16], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:37:46,453] Trial 49 finished with value: 0.6984647964436703 and parameters: {'learning_rate': 0.00279086503350101, 'num_nodes': [16, 16], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:37:48,107] Trial 35 finished with value: 0.7104588479738628 and parameters: {'learning_rate': 0.0027898106319469464, 'num_nodes': [32, 32], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:37:49,252] Trial 46 finished with value: 0.7122719521537728 and parameters: {'learning_rate': 0.002757159352860461, 'num_nodes': [32, 32], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:37:52,123] Trial 36 finished with value: 0.7172810773917726 and parameters: {'learning_rate': 0.002598268523156049, 'num_nodes': [32, 32], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:37:54,003] Trial 43 finished with value: 0.6998407318535577 and parameters: {'learning_rate': 0.0028721412845713894, 'num_nodes': [16, 16], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:38:00,277] Trial 41 finished with value: 0.7033039737588191 and parameters: {'learning_rate': 0.0027947777927850056, 'num_nodes': [16, 16], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:38:03,077] Trial 47 finished with value: 0.7082672086296059 and parameters: {'learning_rate': 0.002974427492285553, 'num_nodes': [16, 16], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:38:06,709] Trial 37 finished with value: 0.6952975553114908 and parameters: {'learning_rate': 0.0032141364282864513, 'num_nodes': [16, 16], 'dropout': 0.5}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:38:42,527] Trial 51 finished with value: 0.7269242565194973 and parameters: {'learning_rate': 0.006101480914129155, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:38:47,752] Trial 53 finished with value: 0.7209910813705502 and parameters: {'learning_rate': 0.006409760759527405, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:38:49,490] Trial 52 finished with value: 0.7360937485969516 and parameters: {'learning_rate': 0.00628826196141572, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 7 with value: 0.7443072880179173.\n",
      "[I 2025-01-06 11:38:49,578] Trial 54 finished with value: 0.7274279613089012 and parameters: {'learning_rate': 0.006076897691363865, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 7 with value: 0.7443072880179173.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.005240711451605818, 'dropout': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 11:39:39,014] Using an existing study with name 'nl-quadratic-2000' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=1000 Training time (2.6710000000000003s): Train C-Index: 0.864 | Test C-index: 0.729 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 11:49:12,240] Trial 45 finished with value: 0.7721719451727246 and parameters: {'learning_rate': 0.001401485955502852, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:49:16,704] Trial 37 finished with value: 0.7660454756170022 and parameters: {'learning_rate': 0.0014510601349400486, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:49:25,588] Trial 50 finished with value: 0.7659627475934551 and parameters: {'learning_rate': 0.001451908203009275, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:49:35,246] Trial 41 finished with value: 0.7737658729648087 and parameters: {'learning_rate': 0.0015916664431855345, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:49:54,211] Trial 42 finished with value: 0.7659228777881715 and parameters: {'learning_rate': 0.00150969851264965, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:49:55,011] Trial 36 finished with value: 0.7652222981602779 and parameters: {'learning_rate': 0.0014295874818977007, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:50:01,981] Trial 47 finished with value: 0.7759764559748168 and parameters: {'learning_rate': 0.0015330252025139767, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:50:16,378] Trial 38 finished with value: 0.7709722989487247 and parameters: {'learning_rate': 0.0012559103032682255, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:50:26,515] Trial 39 finished with value: 0.7745665233669977 and parameters: {'learning_rate': 0.001381557840832931, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:50:35,752] Trial 49 finished with value: 0.7679897401936724 and parameters: {'learning_rate': 0.001331433370658741, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:50:43,504] Trial 40 finished with value: 0.7670067956140263 and parameters: {'learning_rate': 0.0013950618752166031, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:50:49,464] Trial 44 finished with value: 0.7719358334197659 and parameters: {'learning_rate': 0.0013968481187299174, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:50:53,398] Trial 48 finished with value: 0.7651580293499761 and parameters: {'learning_rate': 0.001425359431140446, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:50:58,645] Trial 43 finished with value: 0.7614402250618826 and parameters: {'learning_rate': 0.0014742605866797814, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:51:01,605] Trial 46 finished with value: 0.7744619189686316 and parameters: {'learning_rate': 0.0014266278577192404, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:51:10,134] Trial 35 finished with value: 0.7754174350074441 and parameters: {'learning_rate': 0.0015244521611999476, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:52:14,907] Trial 52 finished with value: 0.7751464236114126 and parameters: {'learning_rate': 0.0016765859855929542, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:52:23,303] Trial 54 finished with value: 0.7508729735963333 and parameters: {'learning_rate': 0.0007070343963875105, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:52:25,276] Trial 53 finished with value: 0.765970747332983 and parameters: {'learning_rate': 0.00072652329273819, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n",
      "[I 2025-01-06 11:52:25,775] Trial 51 finished with value: 0.7737571709964655 and parameters: {'learning_rate': 0.0015867070819060832, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 13 with value: 0.7775780265860612.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0014286264369105714, 'dropout': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 11:53:32,963] Using an existing study with name 'nl-quadratic-5000' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=2000 Training time (4.372s): Train C-Index: 0.875 | Test C-index: 0.769 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 12:12:09,988] Trial 36 finished with value: 0.7381948479755905 and parameters: {'learning_rate': 0.008019098669824198, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:12:18,331] Trial 46 finished with value: 0.7371333498093577 and parameters: {'learning_rate': 0.008426891540312799, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:13:54,060] Trial 38 finished with value: 0.7470617595982563 and parameters: {'learning_rate': 0.008195324747797278, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:14:03,173] Trial 49 finished with value: 0.7646283349882015 and parameters: {'learning_rate': 0.007384805805329934, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:14:30,478] Trial 40 finished with value: 0.75720721287098 and parameters: {'learning_rate': 0.007048093700290785, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:15:06,102] Trial 45 finished with value: 0.7543096027807465 and parameters: {'learning_rate': 0.0074752373479186635, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:15:07,165] Trial 50 finished with value: 0.7399291723771213 and parameters: {'learning_rate': 0.008478519002625497, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:16:11,933] Trial 42 finished with value: 0.7515602150427887 and parameters: {'learning_rate': 0.007564355627148014, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:16:12,132] Trial 35 finished with value: 0.7578400126225535 and parameters: {'learning_rate': 0.007602940636337282, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:16:56,200] Trial 43 finished with value: 0.7573406560279313 and parameters: {'learning_rate': 0.0073219848911363645, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:16:57,273] Trial 39 finished with value: 0.7515223507798444 and parameters: {'learning_rate': 0.008692946613947315, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:18:53,703] Trial 48 finished with value: 0.7732710684687749 and parameters: {'learning_rate': 0.005592324468165686, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:18:59,585] Trial 37 finished with value: 0.7699643539549743 and parameters: {'learning_rate': 0.006204101232022448, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:19:48,694] Trial 41 finished with value: 0.7661138176332848 and parameters: {'learning_rate': 0.007202070776983791, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 23 with value: 0.812919646825663.\n",
      "[I 2025-01-06 12:19:50,876] Trial 47 finished with value: 0.8146314771878613 and parameters: {'learning_rate': 0.0006976857427331728, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 47 with value: 0.8146314771878613.\n",
      "[I 2025-01-06 12:20:05,010] Trial 44 finished with value: 0.8137716648761948 and parameters: {'learning_rate': 0.000667553398898902, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 47 with value: 0.8146314771878613.\n",
      "[I 2025-01-06 12:21:34,097] Trial 54 finished with value: 0.8050317398572782 and parameters: {'learning_rate': 0.002150449732832182, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 47 with value: 0.8146314771878613.\n",
      "[I 2025-01-06 12:22:10,145] Trial 51 finished with value: 0.8094670581673201 and parameters: {'learning_rate': 0.0021118930059839357, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 47 with value: 0.8146314771878613.\n",
      "[I 2025-01-06 12:22:20,580] Trial 52 finished with value: 0.8132424595523826 and parameters: {'learning_rate': 0.0022449643808480526, 'num_nodes': [32, 32], 'dropout': 0.1}. Best is trial 47 with value: 0.8146314771878613.\n",
      "[I 2025-01-06 12:22:21,095] Trial 53 finished with value: 0.8135729368982201 and parameters: {'learning_rate': 0.0022741082624543447, 'num_nodes': [16, 16], 'dropout': 0.1}. Best is trial 47 with value: 0.8146314771878613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0006976857427331728, 'num_nodes': [16, 16], 'dropout': 0.1}\n",
      "N=5000 Training time (14.021s): Train C-Index: 0.883 | Test C-index: 0.818 (Mean)\n"
     ]
    }
   ],
   "source": [
    "best_params_ls, n_train, model_time, train_scores, test_scores = [],[],[],[],[]\n",
    "\n",
    "for (n, batch_size, n_run) in zip(subset, batch_sizes, runs): \n",
    "    \n",
    "    run_train_scores, run_test_scores, run_time = [],[],[]\n",
    "    for run in range(n_run+1):\n",
    "        if n < train_df.shape[0]:\n",
    "            train_sub,_ = train_test_split(train_df,\n",
    "                                        train_size=n/train_df.shape[0], \n",
    "                                        shuffle=True, random_state=run,\n",
    "                                        stratify=train_df[status_col])\n",
    "        else:\n",
    "            train_sub = train_df\n",
    "        \n",
    "        ds = DeepSurvPipeline(train_sub, test_df, hyperparameters=hyperparams, dataName=DATANAME)\n",
    "        if run == 0:\n",
    "            study = ds.tune_hyperparameters(train_sub, n_trials=20, n_jobs=16)\n",
    "            best_params = study.best_params\n",
    "            best_params_ls.append(best_params)\n",
    "            continue\n",
    "        duration, train_c, test_c = ds.train_with_best_params(params=best_params, verbose=False, print_scores=False)\n",
    "        \n",
    "        n_train.append(n)\n",
    "        model_time.append(duration)\n",
    "        train_scores.append(train_c)\n",
    "        test_scores.append(test_c)\n",
    "            \n",
    "        run_train_scores.append(train_c)\n",
    "        run_test_scores.append(test_c)\n",
    "        run_time.append(duration)\n",
    "    \n",
    "    print(f\"N={n} Training time ({np.mean(run_time)}s): Train C-Index: {round(np.mean(run_train_scores),3)} | Test C-index: {round(np.mean(run_test_scores),3)} (Mean)\")                \n",
    "\n",
    "# ============== Save results ===============\n",
    "model_results = pd.DataFrame({\n",
    "    'n train': n_train, \n",
    "    'train time':model_time,\n",
    "    'train score':train_scores, \n",
    "    'test score':test_scores}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n train</th>\n",
       "      <th>train time</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.879185</td>\n",
       "      <td>0.531668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.637119</td>\n",
       "      <td>0.487477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.970464</td>\n",
       "      <td>0.553731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.692086</td>\n",
       "      <td>0.505219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.877941</td>\n",
       "      <td>0.566384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5000</td>\n",
       "      <td>14.51</td>\n",
       "      <td>0.882573</td>\n",
       "      <td>0.817137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5000</td>\n",
       "      <td>16.79</td>\n",
       "      <td>0.895345</td>\n",
       "      <td>0.818173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5000</td>\n",
       "      <td>15.18</td>\n",
       "      <td>0.890899</td>\n",
       "      <td>0.819656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>5000</td>\n",
       "      <td>12.21</td>\n",
       "      <td>0.879755</td>\n",
       "      <td>0.813790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5000</td>\n",
       "      <td>12.48</td>\n",
       "      <td>0.880726</td>\n",
       "      <td>0.813845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n train  train time  train score  test score\n",
       "0        50        0.22     0.879185    0.531668\n",
       "1        50        0.13     0.637119    0.487477\n",
       "2        50        0.37     0.970464    0.553731\n",
       "3        50        0.13     0.692086    0.505219\n",
       "4        50        0.21     0.877941    0.566384\n",
       "..      ...         ...          ...         ...\n",
       "65     5000       14.51     0.882573    0.817137\n",
       "66     5000       16.79     0.895345    0.818173\n",
       "67     5000       15.18     0.890899    0.819656\n",
       "68     5000       12.21     0.879755    0.813790\n",
       "69     5000       12.48     0.880726    0.813845\n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.write(model_results=model_results, fileName=\"model.results.10runs.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear: Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event rate in train set: 0.750000\n",
      "Event rate in test set: 0.750000\n",
      "Survival time distribution:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data\n",
    "folder = 'nonlinear'\n",
    "keywords = ['interaction', \"latest\", '20000']\n",
    "DATANAME = 'nl-interaction'\n",
    "# train_df, val_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, val_split=True)\n",
    "train_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, test_size=0.1)\n",
    "plot_simulation_data(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [50, 500, 1000, 2000, 5000]\n",
    "batch_sizes =[8, 16, 16, 32, 64]\n",
    "runs = [20,20,10,10,10]\n",
    "hyperparams = {\n",
    "    \"learning_rate\": {\"type\": \"float\", \"low\": 1e-4, \"high\": 1e-2, \"log\": True},\n",
    "    # \"num_nodes\": {\"type\": \"categorical\", \"choices\": [[32,32], [16,16]]},\n",
    "    \"dropout\": {\"type\": \"categorical\", \"choices\": [0.1,0.5]},\n",
    "    # \"batch_size\": {'type': \"categorical\", \"choices\": [32,64,128]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 12:41:33,645] A new study created in RDB with name: nl-interaction-50\n",
      "[I 2025-01-06 12:44:30,721] Trial 11 finished with value: 0.6035442035442035 and parameters: {'learning_rate': 0.0010053860433327363, 'dropout': 0.5}. Best is trial 11 with value: 0.6035442035442035.\n",
      "[I 2025-01-06 12:44:43,877] Trial 5 finished with value: 0.5816430716430716 and parameters: {'learning_rate': 0.0018706852875630911, 'dropout': 0.5}. Best is trial 11 with value: 0.6035442035442035.\n",
      "[I 2025-01-06 12:44:44,090] Trial 4 finished with value: 0.6774048774048775 and parameters: {'learning_rate': 0.004595808312506594, 'dropout': 0.1}. Best is trial 4 with value: 0.6774048774048775.\n",
      "[I 2025-01-06 12:44:50,274] Trial 10 finished with value: 0.505913935913936 and parameters: {'learning_rate': 0.00012650313139414877, 'dropout': 0.5}. Best is trial 4 with value: 0.6774048774048775.\n",
      "[I 2025-01-06 12:44:50,351] Trial 13 finished with value: 0.5654634854634855 and parameters: {'learning_rate': 0.0012234345860081051, 'dropout': 0.1}. Best is trial 4 with value: 0.6774048774048775.\n",
      "[I 2025-01-06 12:44:50,908] Trial 6 finished with value: 0.5204280104280103 and parameters: {'learning_rate': 0.00011485155096549002, 'dropout': 0.5}. Best is trial 4 with value: 0.6774048774048775.\n",
      "[I 2025-01-06 12:45:02,750] Trial 8 finished with value: 0.6746236346236347 and parameters: {'learning_rate': 0.0031410876818946734, 'dropout': 0.1}. Best is trial 4 with value: 0.6774048774048775.\n",
      "[I 2025-01-06 12:45:06,164] Trial 2 finished with value: 0.5832901032901032 and parameters: {'learning_rate': 0.00010725635050356089, 'dropout': 0.1}. Best is trial 4 with value: 0.6774048774048775.\n",
      "[I 2025-01-06 12:45:10,284] Trial 12 finished with value: 0.601987261987262 and parameters: {'learning_rate': 0.003974179723794534, 'dropout': 0.1}. Best is trial 4 with value: 0.6774048774048775.\n",
      "[I 2025-01-06 12:45:10,447] Trial 14 finished with value: 0.6107368907368907 and parameters: {'learning_rate': 0.0012883973778171852, 'dropout': 0.1}. Best is trial 4 with value: 0.6774048774048775.\n",
      "[I 2025-01-06 12:45:12,429] Trial 9 finished with value: 0.6156185856185856 and parameters: {'learning_rate': 0.0006037719010181006, 'dropout': 0.5}. Best is trial 4 with value: 0.6774048774048775.\n",
      "[I 2025-01-06 12:45:12,559] Trial 1 finished with value: 0.5807834207834208 and parameters: {'learning_rate': 0.00019976711275817855, 'dropout': 0.5}. Best is trial 4 with value: 0.6774048774048775.\n",
      "[I 2025-01-06 12:45:12,660] Trial 3 finished with value: 0.7240593340593341 and parameters: {'learning_rate': 0.002899663561690444, 'dropout': 0.5}. Best is trial 3 with value: 0.7240593340593341.\n",
      "[I 2025-01-06 12:45:12,736] Trial 0 finished with value: 0.6174745074745075 and parameters: {'learning_rate': 0.004564374803672771, 'dropout': 0.5}. Best is trial 3 with value: 0.7240593340593341.\n",
      "[I 2025-01-06 12:45:12,796] Trial 7 finished with value: 0.5456806256806257 and parameters: {'learning_rate': 0.00015254589300636379, 'dropout': 0.1}. Best is trial 3 with value: 0.7240593340593341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.002899663561690444, 'dropout': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 12:46:00,492] A new study created in RDB with name: nl-interaction-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=50 Training time (0.1905s): Train C-Index: 0.723 | Test C-index: 0.535 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 12:49:27,427] Trial 13 finished with value: 0.5887919974440237 and parameters: {'learning_rate': 0.0037822367614973734, 'dropout': 0.1}. Best is trial 13 with value: 0.5887919974440237.\n",
      "[I 2025-01-06 12:49:35,214] Trial 0 finished with value: 0.611939831293077 and parameters: {'learning_rate': 0.0017009794008026196, 'dropout': 0.1}. Best is trial 0 with value: 0.611939831293077.\n",
      "[I 2025-01-06 12:49:38,599] Trial 3 finished with value: 0.5925558743178149 and parameters: {'learning_rate': 0.0016029462321855963, 'dropout': 0.1}. Best is trial 0 with value: 0.611939831293077.\n",
      "[I 2025-01-06 12:49:39,572] Trial 5 finished with value: 0.5966821456371871 and parameters: {'learning_rate': 0.0035979586486551427, 'dropout': 0.5}. Best is trial 0 with value: 0.611939831293077.\n",
      "[I 2025-01-06 12:49:40,025] Trial 14 finished with value: 0.5912939648123667 and parameters: {'learning_rate': 0.0006235708704987642, 'dropout': 0.1}. Best is trial 0 with value: 0.611939831293077.\n",
      "[I 2025-01-06 12:49:40,233] Trial 7 finished with value: 0.5995199125203325 and parameters: {'learning_rate': 0.005666806048778004, 'dropout': 0.1}. Best is trial 0 with value: 0.611939831293077.\n",
      "[I 2025-01-06 12:49:41,693] Trial 12 finished with value: 0.5531998347623885 and parameters: {'learning_rate': 0.0009502587557843829, 'dropout': 0.1}. Best is trial 0 with value: 0.611939831293077.\n",
      "[I 2025-01-06 12:49:47,767] Trial 6 finished with value: 0.5781273830235957 and parameters: {'learning_rate': 0.0002724559425129252, 'dropout': 0.1}. Best is trial 0 with value: 0.611939831293077.\n",
      "[I 2025-01-06 12:49:54,490] Trial 1 finished with value: 0.5550519158232674 and parameters: {'learning_rate': 0.00015761833602458538, 'dropout': 0.1}. Best is trial 0 with value: 0.611939831293077.\n",
      "[I 2025-01-06 12:49:56,577] Trial 8 finished with value: 0.5709794449457531 and parameters: {'learning_rate': 0.00023474242925716838, 'dropout': 0.1}. Best is trial 0 with value: 0.611939831293077.\n",
      "[I 2025-01-06 12:49:59,078] Trial 10 finished with value: 0.5728085286269267 and parameters: {'learning_rate': 0.0004472552491920054, 'dropout': 0.5}. Best is trial 0 with value: 0.611939831293077.\n",
      "[I 2025-01-06 12:50:03,893] Trial 9 finished with value: 0.5612657660793883 and parameters: {'learning_rate': 0.00024583503538101986, 'dropout': 0.1}. Best is trial 0 with value: 0.611939831293077.\n",
      "[I 2025-01-06 12:50:05,029] Trial 2 finished with value: 0.5777472360192983 and parameters: {'learning_rate': 0.00017742331797834657, 'dropout': 0.1}. Best is trial 0 with value: 0.611939831293077.\n",
      "[I 2025-01-06 12:50:05,149] Trial 11 finished with value: 0.5984728036100944 and parameters: {'learning_rate': 0.0012859626061628034, 'dropout': 0.5}. Best is trial 0 with value: 0.611939831293077.\n",
      "[I 2025-01-06 12:50:05,241] Trial 4 finished with value: 0.59994531683805 and parameters: {'learning_rate': 0.0009378887511047682, 'dropout': 0.5}. Best is trial 0 with value: 0.611939831293077.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0017009794008026196, 'dropout': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 12:51:04,858] A new study created in RDB with name: nl-interaction-1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=500 Training time (0.759s): Train C-Index: 0.746 | Test C-index: 0.59 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 12:55:34,831] Trial 4 finished with value: 0.6377974260788233 and parameters: {'learning_rate': 0.0019221075725515385, 'dropout': 0.1}. Best is trial 4 with value: 0.6377974260788233.\n",
      "[I 2025-01-06 12:55:41,329] Trial 6 finished with value: 0.6381161179964566 and parameters: {'learning_rate': 0.004911919835967987, 'dropout': 0.1}. Best is trial 6 with value: 0.6381161179964566.\n",
      "[I 2025-01-06 12:55:48,196] Trial 0 finished with value: 0.6171512805685903 and parameters: {'learning_rate': 0.0008146652479372544, 'dropout': 0.1}. Best is trial 6 with value: 0.6381161179964566.\n",
      "[I 2025-01-06 12:55:56,324] Trial 7 finished with value: 0.6393828988423459 and parameters: {'learning_rate': 0.002339735391562102, 'dropout': 0.1}. Best is trial 7 with value: 0.6393828988423459.\n",
      "[I 2025-01-06 12:56:01,964] Trial 11 finished with value: 0.6329516338851267 and parameters: {'learning_rate': 0.008918103617609894, 'dropout': 0.1}. Best is trial 7 with value: 0.6393828988423459.\n",
      "[I 2025-01-06 12:56:25,490] Trial 1 finished with value: 0.6418202937981703 and parameters: {'learning_rate': 0.008298609250203875, 'dropout': 0.5}. Best is trial 1 with value: 0.6418202937981703.\n",
      "[I 2025-01-06 12:56:30,061] Trial 14 finished with value: 0.649961500069755 and parameters: {'learning_rate': 0.006990438503131439, 'dropout': 0.5}. Best is trial 14 with value: 0.649961500069755.\n",
      "[I 2025-01-06 12:56:37,786] Trial 12 finished with value: 0.6264125679554451 and parameters: {'learning_rate': 0.00027448661035666056, 'dropout': 0.1}. Best is trial 14 with value: 0.649961500069755.\n",
      "[I 2025-01-06 12:56:44,032] Trial 5 finished with value: 0.6488167653034711 and parameters: {'learning_rate': 0.0034197102438186045, 'dropout': 0.5}. Best is trial 14 with value: 0.649961500069755.\n",
      "[I 2025-01-06 12:56:47,992] Trial 2 finished with value: 0.643025925448735 and parameters: {'learning_rate': 0.0023574617406339313, 'dropout': 0.5}. Best is trial 14 with value: 0.649961500069755.\n",
      "[I 2025-01-06 12:56:49,753] Trial 9 finished with value: 0.6471459858707475 and parameters: {'learning_rate': 0.003521915794135038, 'dropout': 0.5}. Best is trial 14 with value: 0.649961500069755.\n",
      "[I 2025-01-06 12:56:50,127] Trial 8 finished with value: 0.6503687165917604 and parameters: {'learning_rate': 0.004912733538278034, 'dropout': 0.5}. Best is trial 8 with value: 0.6503687165917604.\n",
      "[I 2025-01-06 12:56:50,924] Trial 10 finished with value: 0.646984154901417 and parameters: {'learning_rate': 0.0038056294521218834, 'dropout': 0.5}. Best is trial 8 with value: 0.6503687165917604.\n",
      "[I 2025-01-06 12:56:51,498] Trial 3 finished with value: 0.6390877888141746 and parameters: {'learning_rate': 0.0007455450173599053, 'dropout': 0.5}. Best is trial 8 with value: 0.6503687165917604.\n",
      "[I 2025-01-06 12:56:52,427] Trial 13 finished with value: 0.6114547557826536 and parameters: {'learning_rate': 0.00012434329488627565, 'dropout': 0.5}. Best is trial 8 with value: 0.6503687165917604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.004912733538278034, 'dropout': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 12:57:35,392] A new study created in RDB with name: nl-interaction-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=1000 Training time (2.0300000000000002s): Train C-Index: 0.714 | Test C-index: 0.623 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 13:04:44,907] Trial 8 finished with value: 0.6357678192788451 and parameters: {'learning_rate': 0.0003901551145199842, 'dropout': 0.1}. Best is trial 8 with value: 0.6357678192788451.\n",
      "[I 2025-01-06 13:04:53,779] Trial 7 finished with value: 0.6404815366963577 and parameters: {'learning_rate': 0.001122203218366551, 'dropout': 0.1}. Best is trial 7 with value: 0.6404815366963577.\n",
      "[I 2025-01-06 13:05:26,296] Trial 4 finished with value: 0.6487540434140107 and parameters: {'learning_rate': 0.0014578086664934031, 'dropout': 0.1}. Best is trial 4 with value: 0.6487540434140107.\n",
      "[I 2025-01-06 13:05:36,704] Trial 10 finished with value: 0.6446159192223566 and parameters: {'learning_rate': 0.00578355012134912, 'dropout': 0.1}. Best is trial 4 with value: 0.6487540434140107.\n",
      "[I 2025-01-06 13:05:58,864] Trial 5 finished with value: 0.6358981743616541 and parameters: {'learning_rate': 0.0003012568464374959, 'dropout': 0.1}. Best is trial 4 with value: 0.6487540434140107.\n",
      "[I 2025-01-06 13:05:59,332] Trial 13 finished with value: 0.6475390681278042 and parameters: {'learning_rate': 0.0043735971238252555, 'dropout': 0.5}. Best is trial 4 with value: 0.6487540434140107.\n",
      "[I 2025-01-06 13:06:13,477] Trial 3 finished with value: 0.652998388222365 and parameters: {'learning_rate': 0.0045357159638708, 'dropout': 0.1}. Best is trial 3 with value: 0.652998388222365.\n",
      "[I 2025-01-06 13:06:25,409] Trial 12 finished with value: 0.6245611742029082 and parameters: {'learning_rate': 0.00012712305144663323, 'dropout': 0.1}. Best is trial 3 with value: 0.652998388222365.\n",
      "[I 2025-01-06 13:06:30,194] Trial 2 finished with value: 0.6404315324915408 and parameters: {'learning_rate': 0.0001547373620956504, 'dropout': 0.1}. Best is trial 3 with value: 0.652998388222365.\n",
      "[I 2025-01-06 13:06:46,777] Trial 11 finished with value: 0.6504984566586094 and parameters: {'learning_rate': 0.0029614047493103884, 'dropout': 0.5}. Best is trial 3 with value: 0.652998388222365.\n",
      "[I 2025-01-06 13:06:49,608] Trial 14 finished with value: 0.63867931114419 and parameters: {'learning_rate': 0.00014658157407030476, 'dropout': 0.1}. Best is trial 3 with value: 0.652998388222365.\n",
      "[I 2025-01-06 13:07:15,483] Trial 1 finished with value: 0.6243667777698649 and parameters: {'learning_rate': 0.00014879721480923783, 'dropout': 0.5}. Best is trial 3 with value: 0.652998388222365.\n",
      "[I 2025-01-06 13:07:17,229] Trial 9 finished with value: 0.6392678382574258 and parameters: {'learning_rate': 0.00027995169559855143, 'dropout': 0.5}. Best is trial 3 with value: 0.652998388222365.\n",
      "[I 2025-01-06 13:07:17,333] Trial 6 finished with value: 0.6493380914897529 and parameters: {'learning_rate': 0.0010235788145794825, 'dropout': 0.5}. Best is trial 3 with value: 0.652998388222365.\n",
      "[I 2025-01-06 13:07:19,360] Trial 0 finished with value: 0.6541029821932729 and parameters: {'learning_rate': 0.0007431981167338991, 'dropout': 0.5}. Best is trial 0 with value: 0.6541029821932729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0007431981167338991, 'dropout': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 13:08:33,212] A new study created in RDB with name: nl-interaction-5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=2000 Training time (5.044s): Train C-Index: 0.732 | Test C-index: 0.643 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 13:21:44,320] Trial 9 finished with value: 0.632277567520009 and parameters: {'learning_rate': 0.006542550857150502, 'dropout': 0.5}. Best is trial 9 with value: 0.632277567520009.\n",
      "[I 2025-01-06 13:22:40,250] Trial 4 finished with value: 0.6421717978318865 and parameters: {'learning_rate': 0.004288601074455535, 'dropout': 0.5}. Best is trial 4 with value: 0.6421717978318865.\n",
      "[I 2025-01-06 13:23:07,197] Trial 1 finished with value: 0.6384667263064798 and parameters: {'learning_rate': 0.005005500258698507, 'dropout': 0.5}. Best is trial 4 with value: 0.6421717978318865.\n",
      "[I 2025-01-06 13:24:03,518] Trial 8 finished with value: 0.647241151313448 and parameters: {'learning_rate': 0.0031707223098063958, 'dropout': 0.5}. Best is trial 8 with value: 0.647241151313448.\n",
      "[I 2025-01-06 13:24:10,171] Trial 13 finished with value: 0.6585494963719476 and parameters: {'learning_rate': 0.0020234629566174022, 'dropout': 0.1}. Best is trial 13 with value: 0.6585494963719476.\n",
      "[I 2025-01-06 13:24:22,886] Trial 14 finished with value: 0.6589669183372732 and parameters: {'learning_rate': 0.0024188620385024897, 'dropout': 0.1}. Best is trial 14 with value: 0.6589669183372732.\n",
      "[I 2025-01-06 13:24:23,748] Trial 6 finished with value: 0.6602418259355027 and parameters: {'learning_rate': 0.0007193632592692405, 'dropout': 0.1}. Best is trial 6 with value: 0.6602418259355027.\n",
      "[I 2025-01-06 13:24:26,105] Trial 2 finished with value: 0.6637117567234199 and parameters: {'learning_rate': 0.0010178200156848115, 'dropout': 0.1}. Best is trial 2 with value: 0.6637117567234199.\n",
      "[I 2025-01-06 13:24:32,670] Trial 10 finished with value: 0.6478353168562199 and parameters: {'learning_rate': 0.0027061380561222985, 'dropout': 0.5}. Best is trial 2 with value: 0.6637117567234199.\n",
      "[I 2025-01-06 13:24:45,655] Trial 5 finished with value: 0.6497574235960476 and parameters: {'learning_rate': 0.000284362570892428, 'dropout': 0.1}. Best is trial 2 with value: 0.6637117567234199.\n",
      "[I 2025-01-06 13:24:59,289] Trial 7 finished with value: 0.660779551617878 and parameters: {'learning_rate': 0.0013834930710226124, 'dropout': 0.1}. Best is trial 2 with value: 0.6637117567234199.\n",
      "[I 2025-01-06 13:25:00,039] Trial 3 finished with value: 0.6585506293429022 and parameters: {'learning_rate': 0.0005662444398114354, 'dropout': 0.1}. Best is trial 2 with value: 0.6637117567234199.\n",
      "[I 2025-01-06 13:25:01,055] Trial 11 finished with value: 0.6608933188699431 and parameters: {'learning_rate': 0.0012346980909151498, 'dropout': 0.1}. Best is trial 2 with value: 0.6637117567234199.\n",
      "[I 2025-01-06 13:25:02,770] Trial 0 finished with value: 0.6617385344778255 and parameters: {'learning_rate': 0.002186409337949845, 'dropout': 0.1}. Best is trial 2 with value: 0.6637117567234199.\n",
      "[I 2025-01-06 13:25:26,450] Trial 12 finished with value: 0.6628177516157112 and parameters: {'learning_rate': 0.0003304892136730321, 'dropout': 0.5}. Best is trial 2 with value: 0.6637117567234199.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0010178200156848115, 'dropout': 0.1}\n",
      "N=5000 Training time (8.037s): Train C-Index: 0.715 | Test C-index: 0.655 (Mean)\n"
     ]
    }
   ],
   "source": [
    "best_params_ls, n_train, model_time, train_scores, test_scores = [],[],[],[],[]\n",
    "\n",
    "for (n, batch_size, n_run) in zip(subset, batch_sizes, runs): \n",
    "    \n",
    "    run_train_scores, run_test_scores, run_time = [],[],[]\n",
    "    for run in range(n_run+1):\n",
    "        if n < train_df.shape[0]:\n",
    "            train_sub,_ = train_test_split(train_df,\n",
    "                                        train_size=n/train_df.shape[0], \n",
    "                                        shuffle=True, random_state=run,\n",
    "                                        stratify=train_df[status_col])\n",
    "        else:\n",
    "            train_sub = train_df\n",
    "        \n",
    "        ds = DeepSurvPipeline(train_sub, test_df, hyperparameters=hyperparams, dataName=DATANAME)\n",
    "        if run == 0:\n",
    "            study = ds.tune_hyperparameters(train_sub, n_trials=15, n_jobs=16)\n",
    "            best_params = study.best_params\n",
    "            best_params_ls.append(best_params)\n",
    "            continue\n",
    "        duration, train_c, test_c = ds.train_with_best_params(params=best_params, verbose=False, print_scores=False)\n",
    "        \n",
    "        n_train.append(n)\n",
    "        model_time.append(duration)\n",
    "        train_scores.append(train_c)\n",
    "        test_scores.append(test_c)\n",
    "            \n",
    "        run_train_scores.append(train_c)\n",
    "        run_test_scores.append(test_c)\n",
    "        run_time.append(duration)\n",
    "    \n",
    "    print(f\"N={n} Training time ({np.mean(run_time)}s): Train C-Index: {round(np.mean(run_train_scores),3)} | Test C-index: {round(np.mean(run_test_scores),3)} (Mean)\")                \n",
    "\n",
    "# ============== Save results ===============\n",
    "model_results = pd.DataFrame({\n",
    "    'n train': n_train, \n",
    "    'train time':model_time,\n",
    "    'train score':train_scores, \n",
    "    'test score':test_scores}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n train</th>\n",
       "      <th>train time</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.814465</td>\n",
       "      <td>0.516088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.622120</td>\n",
       "      <td>0.534780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.541523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.818874</td>\n",
       "      <td>0.543649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.745928</td>\n",
       "      <td>0.539837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5000</td>\n",
       "      <td>7.84</td>\n",
       "      <td>0.724335</td>\n",
       "      <td>0.660972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5000</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.723639</td>\n",
       "      <td>0.658422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5000</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.703557</td>\n",
       "      <td>0.653885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>5000</td>\n",
       "      <td>10.60</td>\n",
       "      <td>0.750990</td>\n",
       "      <td>0.664343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5000</td>\n",
       "      <td>8.06</td>\n",
       "      <td>0.718452</td>\n",
       "      <td>0.647226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n train  train time  train score  test score\n",
       "0        50        0.20     0.814465    0.516088\n",
       "1        50        0.13     0.622120    0.534780\n",
       "2        50        0.20     0.764706    0.541523\n",
       "3        50        0.20     0.818874    0.543649\n",
       "4        50        0.17     0.745928    0.539837\n",
       "..      ...         ...          ...         ...\n",
       "65     5000        7.84     0.724335    0.660972\n",
       "66     5000        7.83     0.723639    0.658422\n",
       "67     5000        7.83     0.703557    0.653885\n",
       "68     5000       10.60     0.750990    0.664343\n",
       "69     5000        8.06     0.718452    0.647226\n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.write(model_results=model_results, fileName=\"model.results.10runs.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear: Sine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event rate in train set: 0.748333\n",
      "Event rate in test set: 0.748500\n",
      "Survival time distribution:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data\n",
    "folder = 'nonlinear'\n",
    "keywords = ['sine', \"latest\", '20000']\n",
    "DATANAME = 'nl-sine'\n",
    "# train_df, val_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, val_split=True)\n",
    "train_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, test_size=0.1)\n",
    "plot_simulation_data(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [50, 500, 1000, 2000, 5000]\n",
    "batch_sizes =[8, 16, 16, 32, 64]\n",
    "runs = [20,20,10,10,10]\n",
    "hyperparams = {\n",
    "    \"learning_rate\": {\"type\": \"float\", \"low\": 1e-4, \"high\": 1e-2, \"log\": True},\n",
    "    # \"num_nodes\": {\"type\": \"categorical\", \"choices\": [[32,32], [16,16]]},\n",
    "    \"dropout\": {\"type\": \"categorical\", \"choices\": [0.1,0.5]},\n",
    "    # \"batch_size\": {'type': \"categorical\", \"choices\": [32,64,128]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 14:53:44,227] Using an existing study with name 'nl-sine-50' instead of creating a new one.\n",
      "[I 2025-01-06 14:57:28,994] Trial 19 finished with value: 0.6337471467715943 and parameters: {'learning_rate': 0.009876429178379394, 'dropout': 0.1}. Best is trial 0 with value: 0.6656712775854443.\n",
      "[I 2025-01-06 14:57:29,019] Trial 23 finished with value: 0.6425429700010676 and parameters: {'learning_rate': 0.009532062030884286, 'dropout': 0.1}. Best is trial 0 with value: 0.6656712775854443.\n",
      "[I 2025-01-06 14:57:29,372] Trial 24 finished with value: 0.6229473277514272 and parameters: {'learning_rate': 0.008976126498747267, 'dropout': 0.1}. Best is trial 0 with value: 0.6656712775854443.\n",
      "[I 2025-01-06 14:57:29,828] Trial 26 finished with value: 0.605916413752434 and parameters: {'learning_rate': 0.009722308780796544, 'dropout': 0.1}. Best is trial 0 with value: 0.6656712775854443.\n",
      "[I 2025-01-06 14:57:29,998] Trial 20 finished with value: 0.5714417890568206 and parameters: {'learning_rate': 0.00610423642983851, 'dropout': 0.1}. Best is trial 0 with value: 0.6656712775854443.\n",
      "[I 2025-01-06 14:57:33,003] Trial 17 finished with value: 0.6072610532416233 and parameters: {'learning_rate': 0.008862506456156757, 'dropout': 0.1}. Best is trial 0 with value: 0.6656712775854443.\n",
      "[I 2025-01-06 14:57:36,408] Trial 15 finished with value: 0.5637908157818481 and parameters: {'learning_rate': 0.009232826243797647, 'dropout': 0.1}. Best is trial 0 with value: 0.6656712775854443.\n",
      "[I 2025-01-06 14:57:38,758] Trial 18 finished with value: 0.5727477924018973 and parameters: {'learning_rate': 0.009806732580664817, 'dropout': 0.1}. Best is trial 0 with value: 0.6656712775854443.\n",
      "[I 2025-01-06 14:57:39,957] Trial 22 finished with value: 0.7244597294453171 and parameters: {'learning_rate': 0.00886724216079615, 'dropout': 0.1}. Best is trial 22 with value: 0.7244597294453171.\n",
      "[I 2025-01-06 14:57:40,549] Trial 25 finished with value: 0.5918899683285292 and parameters: {'learning_rate': 0.009379174632673714, 'dropout': 0.1}. Best is trial 22 with value: 0.7244597294453171.\n",
      "[I 2025-01-06 14:57:40,748] Trial 21 finished with value: 0.5874879897512544 and parameters: {'learning_rate': 0.009906175876710248, 'dropout': 0.1}. Best is trial 22 with value: 0.7244597294453171.\n",
      "[I 2025-01-06 14:57:41,002] Trial 29 finished with value: 0.6924140981256386 and parameters: {'learning_rate': 0.008795611428628815, 'dropout': 0.1}. Best is trial 22 with value: 0.7244597294453171.\n",
      "[I 2025-01-06 14:57:41,120] Trial 16 finished with value: 0.5447635315469201 and parameters: {'learning_rate': 0.008614353500059178, 'dropout': 0.1}. Best is trial 22 with value: 0.7244597294453171.\n",
      "[I 2025-01-06 14:57:41,273] Trial 27 finished with value: 0.549537128826122 and parameters: {'learning_rate': 0.009253394409391864, 'dropout': 0.1}. Best is trial 22 with value: 0.7244597294453171.\n",
      "[I 2025-01-06 14:57:41,283] Trial 28 finished with value: 0.55444951120194 and parameters: {'learning_rate': 0.009331941225346717, 'dropout': 0.1}. Best is trial 22 with value: 0.7244597294453171.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.00886724216079615, 'dropout': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 14:58:29,131] Using an existing study with name 'nl-sine-500' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=50 Training time (0.17099999999999999s): Train C-Index: 0.786 | Test C-index: 0.527 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 15:01:58,718] Trial 15 finished with value: 0.6523410640237289 and parameters: {'learning_rate': 0.0077837792012293615, 'dropout': 0.5}. Best is trial 7 with value: 0.668171101680351.\n",
      "[I 2025-01-06 15:02:26,661] Trial 22 finished with value: 0.649542363533502 and parameters: {'learning_rate': 0.009521170852416732, 'dropout': 0.5}. Best is trial 7 with value: 0.668171101680351.\n",
      "[I 2025-01-06 15:02:35,870] Trial 18 finished with value: 0.6724453783943023 and parameters: {'learning_rate': 0.009822696674976527, 'dropout': 0.5}. Best is trial 18 with value: 0.6724453783943023.\n",
      "[I 2025-01-06 15:02:36,057] Trial 21 finished with value: 0.6482179749327848 and parameters: {'learning_rate': 0.008031006448107784, 'dropout': 0.5}. Best is trial 18 with value: 0.6724453783943023.\n",
      "[I 2025-01-06 15:02:36,431] Trial 23 finished with value: 0.6621530798102293 and parameters: {'learning_rate': 0.009787515430900343, 'dropout': 0.5}. Best is trial 18 with value: 0.6724453783943023.\n",
      "[I 2025-01-06 15:02:39,564] Trial 25 finished with value: 0.6608777060722938 and parameters: {'learning_rate': 0.009174466842073296, 'dropout': 0.5}. Best is trial 18 with value: 0.6724453783943023.\n",
      "[I 2025-01-06 15:02:39,596] Trial 17 finished with value: 0.6566420709205788 and parameters: {'learning_rate': 0.004416278063647655, 'dropout': 0.5}. Best is trial 18 with value: 0.6724453783943023.\n",
      "[I 2025-01-06 15:02:40,258] Trial 29 finished with value: 0.6712080270596944 and parameters: {'learning_rate': 0.009786692315748873, 'dropout': 0.5}. Best is trial 18 with value: 0.6724453783943023.\n",
      "[I 2025-01-06 15:02:40,396] Trial 26 finished with value: 0.6501124134804556 and parameters: {'learning_rate': 0.009658605480583345, 'dropout': 0.5}. Best is trial 18 with value: 0.6724453783943023.\n",
      "[I 2025-01-06 15:02:40,546] Trial 27 finished with value: 0.6667589852930556 and parameters: {'learning_rate': 0.00958927592804615, 'dropout': 0.5}. Best is trial 18 with value: 0.6724453783943023.\n",
      "[I 2025-01-06 15:02:44,323] Trial 24 finished with value: 0.6703655687047654 and parameters: {'learning_rate': 0.008877128807143761, 'dropout': 0.5}. Best is trial 18 with value: 0.6724453783943023.\n",
      "[I 2025-01-06 15:02:44,401] Trial 28 finished with value: 0.66324205751972 and parameters: {'learning_rate': 0.009371282699126718, 'dropout': 0.5}. Best is trial 18 with value: 0.6724453783943023.\n",
      "[I 2025-01-06 15:02:45,320] Trial 19 finished with value: 0.6773976713233327 and parameters: {'learning_rate': 0.006645870539326249, 'dropout': 0.5}. Best is trial 19 with value: 0.6773976713233327.\n",
      "[I 2025-01-06 15:02:45,494] Trial 16 finished with value: 0.6524017885659885 and parameters: {'learning_rate': 0.004109176479538371, 'dropout': 0.5}. Best is trial 19 with value: 0.6773976713233327.\n",
      "[I 2025-01-06 15:02:45,871] Trial 20 finished with value: 0.6794839687768294 and parameters: {'learning_rate': 0.004052497934739238, 'dropout': 0.5}. Best is trial 20 with value: 0.6794839687768294.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.004052497934739238, 'dropout': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 15:03:54,304] Using an existing study with name 'nl-sine-1000' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=500 Training time (1.1609999999999998s): Train C-Index: 0.746 | Test C-index: 0.608 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 15:10:24,509] Trial 26 finished with value: 0.6648244143842823 and parameters: {'learning_rate': 0.003519143766607783, 'dropout': 0.5}. Best is trial 9 with value: 0.6708340401084949.\n",
      "[I 2025-01-06 15:10:52,002] Trial 25 finished with value: 0.6645977973805316 and parameters: {'learning_rate': 0.002772738870260153, 'dropout': 0.5}. Best is trial 9 with value: 0.6708340401084949.\n",
      "[I 2025-01-06 15:10:53,212] Trial 19 finished with value: 0.664679412386522 and parameters: {'learning_rate': 0.0025587839621954806, 'dropout': 0.5}. Best is trial 9 with value: 0.6708340401084949.\n",
      "[I 2025-01-06 15:10:53,316] Trial 20 finished with value: 0.6641953725096286 and parameters: {'learning_rate': 0.002843557499319933, 'dropout': 0.5}. Best is trial 9 with value: 0.6708340401084949.\n",
      "[I 2025-01-06 15:10:54,110] Trial 16 finished with value: 0.6647473047343276 and parameters: {'learning_rate': 0.0024956910293708425, 'dropout': 0.5}. Best is trial 9 with value: 0.6708340401084949.\n",
      "[I 2025-01-06 15:10:58,118] Trial 22 finished with value: 0.667368030815547 and parameters: {'learning_rate': 0.0029464215796471127, 'dropout': 0.5}. Best is trial 9 with value: 0.6708340401084949.\n",
      "[I 2025-01-06 15:10:58,795] Trial 15 finished with value: 0.6669231668441101 and parameters: {'learning_rate': 0.002958024049925501, 'dropout': 0.5}. Best is trial 9 with value: 0.6708340401084949.\n",
      "[I 2025-01-06 15:11:01,308] Trial 21 finished with value: 0.6665543480764762 and parameters: {'learning_rate': 0.0025676827742993763, 'dropout': 0.5}. Best is trial 9 with value: 0.6708340401084949.\n",
      "[I 2025-01-06 15:11:01,732] Trial 24 finished with value: 0.6663277269420808 and parameters: {'learning_rate': 0.0028945220872401414, 'dropout': 0.5}. Best is trial 9 with value: 0.6708340401084949.\n",
      "[I 2025-01-06 15:11:04,321] Trial 23 finished with value: 0.6650295265270164 and parameters: {'learning_rate': 0.0026795704213249797, 'dropout': 0.5}. Best is trial 9 with value: 0.6708340401084949.\n",
      "[I 2025-01-06 15:11:07,342] Trial 28 finished with value: 0.6659471454688906 and parameters: {'learning_rate': 0.0029045339664796306, 'dropout': 0.5}. Best is trial 9 with value: 0.6708340401084949.\n",
      "[I 2025-01-06 15:11:08,169] Trial 17 finished with value: 0.6713336698736343 and parameters: {'learning_rate': 0.0027288713850125833, 'dropout': 0.5}. Best is trial 17 with value: 0.6713336698736343.\n",
      "[I 2025-01-06 15:11:08,320] Trial 27 finished with value: 0.6692613233821302 and parameters: {'learning_rate': 0.002849976455070248, 'dropout': 0.5}. Best is trial 17 with value: 0.6713336698736343.\n",
      "[I 2025-01-06 15:11:08,652] Trial 29 finished with value: 0.6662840147743869 and parameters: {'learning_rate': 0.0028144107364112413, 'dropout': 0.5}. Best is trial 17 with value: 0.6713336698736343.\n",
      "[I 2025-01-06 15:11:08,732] Trial 18 finished with value: 0.6639144401869373 and parameters: {'learning_rate': 0.0029978975424434, 'dropout': 0.5}. Best is trial 17 with value: 0.6713336698736343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0027288713850125833, 'dropout': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 15:11:53,074] Using an existing study with name 'nl-sine-2000' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=1000 Training time (2.143s): Train C-Index: 0.741 | Test C-index: 0.636 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 15:20:46,889] Trial 18 finished with value: 0.6674268680857102 and parameters: {'learning_rate': 0.002078143148163079, 'dropout': 0.5}. Best is trial 5 with value: 0.6677392564902191.\n",
      "[I 2025-01-06 15:21:07,864] Trial 25 finished with value: 0.6657773425493108 and parameters: {'learning_rate': 0.0019078343574118685, 'dropout': 0.5}. Best is trial 5 with value: 0.6677392564902191.\n",
      "[I 2025-01-06 15:21:10,666] Trial 20 finished with value: 0.6650071008709947 and parameters: {'learning_rate': 0.0018863399086816062, 'dropout': 0.5}. Best is trial 5 with value: 0.6677392564902191.\n",
      "[I 2025-01-06 15:21:15,080] Trial 24 finished with value: 0.6675156167966122 and parameters: {'learning_rate': 0.0021128431860265155, 'dropout': 0.5}. Best is trial 5 with value: 0.6677392564902191.\n",
      "[I 2025-01-06 15:21:23,036] Trial 29 finished with value: 0.6685530572463612 and parameters: {'learning_rate': 0.0021163475800993394, 'dropout': 0.5}. Best is trial 29 with value: 0.6685530572463612.\n",
      "[I 2025-01-06 15:21:28,031] Trial 26 finished with value: 0.6671736723926034 and parameters: {'learning_rate': 0.0021262052186385998, 'dropout': 0.5}. Best is trial 29 with value: 0.6685530572463612.\n",
      "[I 2025-01-06 15:21:28,422] Trial 22 finished with value: 0.668243006084625 and parameters: {'learning_rate': 0.0022461534142550585, 'dropout': 0.5}. Best is trial 29 with value: 0.6685530572463612.\n",
      "[I 2025-01-06 15:21:29,766] Trial 17 finished with value: 0.6636592890839526 and parameters: {'learning_rate': 0.0020003518664132923, 'dropout': 0.5}. Best is trial 29 with value: 0.6685530572463612.\n",
      "[I 2025-01-06 15:21:31,137] Trial 19 finished with value: 0.6687657087000873 and parameters: {'learning_rate': 0.0020563471272001086, 'dropout': 0.5}. Best is trial 19 with value: 0.6687657087000873.\n",
      "[I 2025-01-06 15:21:32,314] Trial 27 finished with value: 0.6648650146045331 and parameters: {'learning_rate': 0.0019592836018996586, 'dropout': 0.5}. Best is trial 19 with value: 0.6687657087000873.\n",
      "[I 2025-01-06 15:21:35,215] Trial 28 finished with value: 0.6646934039270251 and parameters: {'learning_rate': 0.0019527995132341772, 'dropout': 0.5}. Best is trial 19 with value: 0.6687657087000873.\n",
      "[I 2025-01-06 15:21:37,178] Trial 15 finished with value: 0.6633798054575765 and parameters: {'learning_rate': 0.0021637030311670055, 'dropout': 0.5}. Best is trial 19 with value: 0.6687657087000873.\n",
      "[I 2025-01-06 15:21:37,672] Trial 16 finished with value: 0.6662097204217904 and parameters: {'learning_rate': 0.0024621274933006973, 'dropout': 0.5}. Best is trial 19 with value: 0.6687657087000873.\n",
      "[I 2025-01-06 15:21:38,737] Trial 23 finished with value: 0.6700173172423488 and parameters: {'learning_rate': 0.0023014017155613144, 'dropout': 0.5}. Best is trial 23 with value: 0.6700173172423488.\n",
      "[I 2025-01-06 15:21:40,024] Trial 21 finished with value: 0.6664653961316167 and parameters: {'learning_rate': 0.0021652793425016967, 'dropout': 0.5}. Best is trial 23 with value: 0.6700173172423488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0023014017155613144, 'dropout': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 15:22:57,183] Using an existing study with name 'nl-sine-5000' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=2000 Training time (5.345000000000001s): Train C-Index: 0.723 | Test C-index: 0.655 (Mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 15:34:46,226] Trial 29 finished with value: 0.6371754677906013 and parameters: {'learning_rate': 0.006861186579051638, 'dropout': 0.5}. Best is trial 10 with value: 0.6768435515025389.\n",
      "[I 2025-01-06 15:36:37,134] Trial 26 finished with value: 0.6802124875593525 and parameters: {'learning_rate': 0.0011589206071232186, 'dropout': 0.1}. Best is trial 26 with value: 0.6802124875593525.\n",
      "[I 2025-01-06 15:37:21,734] Trial 19 finished with value: 0.6755426111746722 and parameters: {'learning_rate': 0.0008205625979236326, 'dropout': 0.1}. Best is trial 26 with value: 0.6802124875593525.\n",
      "[I 2025-01-06 15:37:22,430] Trial 28 finished with value: 0.674576251549191 and parameters: {'learning_rate': 0.0011162232884864077, 'dropout': 0.1}. Best is trial 26 with value: 0.6802124875593525.\n",
      "[I 2025-01-06 15:37:23,529] Trial 23 finished with value: 0.6677608913280699 and parameters: {'learning_rate': 0.003731021908434111, 'dropout': 0.1}. Best is trial 26 with value: 0.6802124875593525.\n",
      "[I 2025-01-06 15:37:28,214] Trial 22 finished with value: 0.6753312898992101 and parameters: {'learning_rate': 0.002049775461226381, 'dropout': 0.1}. Best is trial 26 with value: 0.6802124875593525.\n",
      "[I 2025-01-06 15:37:47,394] Trial 25 finished with value: 0.6780102012235074 and parameters: {'learning_rate': 0.000776082239894935, 'dropout': 0.1}. Best is trial 26 with value: 0.6802124875593525.\n",
      "[I 2025-01-06 15:37:56,795] Trial 15 finished with value: 0.6741683205944209 and parameters: {'learning_rate': 0.003111056193784887, 'dropout': 0.1}. Best is trial 26 with value: 0.6802124875593525.\n",
      "[I 2025-01-06 15:38:04,349] Trial 16 finished with value: 0.6795354355695421 and parameters: {'learning_rate': 0.0010977829426818894, 'dropout': 0.1}. Best is trial 26 with value: 0.6802124875593525.\n",
      "[I 2025-01-06 15:38:10,744] Trial 21 finished with value: 0.6771454000745292 and parameters: {'learning_rate': 0.0017513406170668194, 'dropout': 0.1}. Best is trial 26 with value: 0.6802124875593525.\n",
      "[I 2025-01-06 15:38:13,167] Trial 17 finished with value: 0.6649826781138983 and parameters: {'learning_rate': 0.003136260353720955, 'dropout': 0.5}. Best is trial 26 with value: 0.6802124875593525.\n",
      "[I 2025-01-06 15:38:16,843] Trial 18 finished with value: 0.67504712583402 and parameters: {'learning_rate': 0.0031866597796426444, 'dropout': 0.1}. Best is trial 26 with value: 0.6802124875593525.\n",
      "[I 2025-01-06 15:39:04,620] Trial 24 finished with value: 0.6774944892229005 and parameters: {'learning_rate': 0.0010908519149200812, 'dropout': 0.5}. Best is trial 26 with value: 0.6802124875593525.\n",
      "[I 2025-01-06 15:39:11,592] Trial 20 finished with value: 0.6799381006079004 and parameters: {'learning_rate': 0.0008217993374071225, 'dropout': 0.5}. Best is trial 26 with value: 0.6802124875593525.\n",
      "[I 2025-01-06 15:39:27,166] Trial 27 finished with value: 0.6764793556781203 and parameters: {'learning_rate': 0.00022333524938868184, 'dropout': 0.5}. Best is trial 26 with value: 0.6802124875593525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.0011589206071232186, 'dropout': 0.1}\n",
      "N=5000 Training time (8.145s): Train C-Index: 0.727 | Test C-index: 0.673 (Mean)\n"
     ]
    }
   ],
   "source": [
    "best_params_ls, n_train, model_time, train_scores, test_scores = [],[],[],[],[]\n",
    "\n",
    "for (n, batch_size, n_run) in zip(subset, batch_sizes, runs): \n",
    "    \n",
    "    run_train_scores, run_test_scores, run_time = [],[],[]\n",
    "    for run in range(n_run+1):\n",
    "        if n < train_df.shape[0]:\n",
    "            train_sub,_ = train_test_split(train_df,\n",
    "                                        train_size=n/train_df.shape[0], \n",
    "                                        shuffle=True, random_state=run,\n",
    "                                        stratify=train_df[status_col])\n",
    "        else:\n",
    "            train_sub = train_df\n",
    "        \n",
    "        ds = DeepSurvPipeline(train_sub, test_df, hyperparameters=hyperparams, dataName=DATANAME)\n",
    "        if run == 0:\n",
    "            study = ds.tune_hyperparameters(train_sub, n_trials=15, n_jobs=16)\n",
    "            best_params = study.best_params\n",
    "            best_params_ls.append(best_params)\n",
    "            continue\n",
    "        duration, train_c, test_c = ds.train_with_best_params(params=best_params, verbose=False, print_scores=False)\n",
    "        \n",
    "        n_train.append(n)\n",
    "        model_time.append(duration)\n",
    "        train_scores.append(train_c)\n",
    "        test_scores.append(test_c)\n",
    "            \n",
    "        run_train_scores.append(train_c)\n",
    "        run_test_scores.append(test_c)\n",
    "        run_time.append(duration)\n",
    "    \n",
    "    print(f\"N={n} Training time ({np.mean(run_time)}s): Train C-Index: {round(np.mean(run_train_scores),3)} | Test C-index: {round(np.mean(run_test_scores),3)} (Mean)\")                \n",
    "\n",
    "# ============== Save results ===============\n",
    "model_results = pd.DataFrame({\n",
    "    'n train': n_train, \n",
    "    'train time':model_time,\n",
    "    'train score':train_scores, \n",
    "    'test score':test_scores}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n train</th>\n",
       "      <th>train time</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.729927</td>\n",
       "      <td>0.494439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.934426</td>\n",
       "      <td>0.556693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.766617</td>\n",
       "      <td>0.496857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.968701</td>\n",
       "      <td>0.561426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.919162</td>\n",
       "      <td>0.494433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5000</td>\n",
       "      <td>9.61</td>\n",
       "      <td>0.740225</td>\n",
       "      <td>0.676605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5000</td>\n",
       "      <td>9.10</td>\n",
       "      <td>0.743808</td>\n",
       "      <td>0.689109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5000</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.731404</td>\n",
       "      <td>0.680263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>5000</td>\n",
       "      <td>10.43</td>\n",
       "      <td>0.756007</td>\n",
       "      <td>0.677074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5000</td>\n",
       "      <td>6.96</td>\n",
       "      <td>0.713574</td>\n",
       "      <td>0.672102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n train  train time  train score  test score\n",
       "0        50        0.14     0.729927    0.494439\n",
       "1        50        0.23     0.934426    0.556693\n",
       "2        50        0.15     0.766617    0.496857\n",
       "3        50        0.31     0.968701    0.561426\n",
       "4        50        0.21     0.919162    0.494433\n",
       "..      ...         ...          ...         ...\n",
       "65     5000        9.61     0.740225    0.676605\n",
       "66     5000        9.10     0.743808    0.689109\n",
       "67     5000        7.70     0.731404    0.680263\n",
       "68     5000       10.43     0.756007    0.677074\n",
       "69     5000        6.96     0.713574    0.672102\n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.write(model_results=model_results, fileName=\"model.results.10runs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n train</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.00886724216079615, 'dropou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.004052497934739238, 'dropo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.0027288713850125833, 'drop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.0023014017155613144, 'drop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5000</td>\n",
       "      <td>{'learning_rate': 0.0011589206071232186, 'drop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n train                                    hyperparameters\n",
       "0       50  {'learning_rate': 0.00886724216079615, 'dropou...\n",
       "1      500  {'learning_rate': 0.004052497934739238, 'dropo...\n",
       "2     1000  {'learning_rate': 0.0027288713850125833, 'drop...\n",
       "3     2000  {'learning_rate': 0.0023014017155613144, 'drop...\n",
       "4     5000  {'learning_rate': 0.0011589206071232186, 'drop..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params = pd.DataFrame(\n",
    "    {'n train': subset,\n",
    "    'hyperparameters': best_params_ls}\n",
    "    )\n",
    "model_params.to_csv(os.path.join('models', DATANAME, 'deepsurv-torch', 'model.hyperparameters.csv'))\n",
    "model_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
