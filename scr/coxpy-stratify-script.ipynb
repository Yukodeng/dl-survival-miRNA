{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pycox: DeepSurv Stratified by Batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "\n",
    "\n",
    "os.chdir(\"../\")\n",
    "from pycox.models.cox import CoxPH, CoxPHStratified\n",
    "from pycox.evaluation.eval_surv import EvalSurv, EvalSurvStratified\n",
    "from scr.utils import *\n",
    "from scr.runDeepSurvModels import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hsa.let.7a.2..1</th>\n",
       "      <th>hsa.let.7a.3..1</th>\n",
       "      <th>hsa.let.7a..2..1</th>\n",
       "      <th>hsa.let.7b.1</th>\n",
       "      <th>hsa.let.7b..1</th>\n",
       "      <th>hsa.let.7c.1</th>\n",
       "      <th>hsa.let.7c..1</th>\n",
       "      <th>hsa.let.7d.1</th>\n",
       "      <th>hsa.let.7d..1</th>\n",
       "      <th>hsa.let.7e.1</th>\n",
       "      <th>...</th>\n",
       "      <th>hsa.miR.96.1</th>\n",
       "      <th>hsa.miR.96..1</th>\n",
       "      <th>hsa.miR.98.1</th>\n",
       "      <th>hsa.miR.98..1</th>\n",
       "      <th>hsa.miR.99a.1</th>\n",
       "      <th>hsa.miR.99a..1</th>\n",
       "      <th>hsa.miR.99b.1</th>\n",
       "      <th>hsa.miR.99b..1</th>\n",
       "      <th>time</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.161142</td>\n",
       "      <td>16.139914</td>\n",
       "      <td>9.440727</td>\n",
       "      <td>12.377491</td>\n",
       "      <td>5.524234</td>\n",
       "      <td>11.493406</td>\n",
       "      <td>3.315916</td>\n",
       "      <td>13.211118</td>\n",
       "      <td>4.830156</td>\n",
       "      <td>10.275969</td>\n",
       "      <td>...</td>\n",
       "      <td>4.311983</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>11.930738</td>\n",
       "      <td>6.893816</td>\n",
       "      <td>9.471225</td>\n",
       "      <td>1.244932</td>\n",
       "      <td>1.820734</td>\n",
       "      <td>0.499726</td>\n",
       "      <td>17.195448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.737665</td>\n",
       "      <td>15.622746</td>\n",
       "      <td>7.350143</td>\n",
       "      <td>12.387646</td>\n",
       "      <td>4.147150</td>\n",
       "      <td>12.739532</td>\n",
       "      <td>3.693167</td>\n",
       "      <td>11.653065</td>\n",
       "      <td>6.348644</td>\n",
       "      <td>10.511325</td>\n",
       "      <td>...</td>\n",
       "      <td>5.211017</td>\n",
       "      <td>0.010937</td>\n",
       "      <td>10.018970</td>\n",
       "      <td>4.504616</td>\n",
       "      <td>12.851019</td>\n",
       "      <td>4.458570</td>\n",
       "      <td>9.536961</td>\n",
       "      <td>4.829052</td>\n",
       "      <td>1.190342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.137077</td>\n",
       "      <td>15.838051</td>\n",
       "      <td>7.474055</td>\n",
       "      <td>14.062855</td>\n",
       "      <td>5.124704</td>\n",
       "      <td>13.175972</td>\n",
       "      <td>3.549938</td>\n",
       "      <td>12.224586</td>\n",
       "      <td>5.851290</td>\n",
       "      <td>10.720255</td>\n",
       "      <td>...</td>\n",
       "      <td>6.218995</td>\n",
       "      <td>0.023063</td>\n",
       "      <td>11.032040</td>\n",
       "      <td>5.361192</td>\n",
       "      <td>12.062099</td>\n",
       "      <td>3.112622</td>\n",
       "      <td>9.023745</td>\n",
       "      <td>4.720672</td>\n",
       "      <td>6.049298</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.152887</td>\n",
       "      <td>18.113934</td>\n",
       "      <td>7.656181</td>\n",
       "      <td>12.879390</td>\n",
       "      <td>4.768000</td>\n",
       "      <td>13.147153</td>\n",
       "      <td>3.518407</td>\n",
       "      <td>12.940474</td>\n",
       "      <td>6.602133</td>\n",
       "      <td>11.160047</td>\n",
       "      <td>...</td>\n",
       "      <td>3.165273</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>11.539787</td>\n",
       "      <td>5.979588</td>\n",
       "      <td>13.096312</td>\n",
       "      <td>4.459175</td>\n",
       "      <td>9.323495</td>\n",
       "      <td>5.122238</td>\n",
       "      <td>8.483713</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.068358</td>\n",
       "      <td>14.510637</td>\n",
       "      <td>7.261556</td>\n",
       "      <td>11.505635</td>\n",
       "      <td>5.917256</td>\n",
       "      <td>12.665515</td>\n",
       "      <td>4.810049</td>\n",
       "      <td>11.739157</td>\n",
       "      <td>6.542291</td>\n",
       "      <td>11.056379</td>\n",
       "      <td>...</td>\n",
       "      <td>17.231062</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>10.274817</td>\n",
       "      <td>5.154520</td>\n",
       "      <td>13.246133</td>\n",
       "      <td>5.570715</td>\n",
       "      <td>11.376455</td>\n",
       "      <td>6.263673</td>\n",
       "      <td>23.514074</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1035 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hsa.let.7a.2..1  hsa.let.7a.3..1  hsa.let.7a..2..1  hsa.let.7b.1  \\\n",
       "0         0.161142        16.139914          9.440727     12.377491   \n",
       "1         0.737665        15.622746          7.350143     12.387646   \n",
       "2         1.137077        15.838051          7.474055     14.062855   \n",
       "3         2.152887        18.113934          7.656181     12.879390   \n",
       "4         3.068358        14.510637          7.261556     11.505635   \n",
       "\n",
       "   hsa.let.7b..1  hsa.let.7c.1  hsa.let.7c..1  hsa.let.7d.1  hsa.let.7d..1  \\\n",
       "0       5.524234     11.493406       3.315916     13.211118       4.830156   \n",
       "1       4.147150     12.739532       3.693167     11.653065       6.348644   \n",
       "2       5.124704     13.175972       3.549938     12.224586       5.851290   \n",
       "3       4.768000     13.147153       3.518407     12.940474       6.602133   \n",
       "4       5.917256     12.665515       4.810049     11.739157       6.542291   \n",
       "\n",
       "   hsa.let.7e.1  ...  hsa.miR.96.1  hsa.miR.96..1  hsa.miR.98.1  \\\n",
       "0     10.275969  ...      4.311983       0.000551     11.930738   \n",
       "1     10.511325  ...      5.211017       0.010937     10.018970   \n",
       "2     10.720255  ...      6.218995       0.023063     11.032040   \n",
       "3     11.160047  ...      3.165273       0.015713     11.539787   \n",
       "4     11.056379  ...     17.231062       0.005042     10.274817   \n",
       "\n",
       "   hsa.miR.98..1  hsa.miR.99a.1  hsa.miR.99a..1  hsa.miR.99b.1  \\\n",
       "0       6.893816       9.471225        1.244932       1.820734   \n",
       "1       4.504616      12.851019        4.458570       9.536961   \n",
       "2       5.361192      12.062099        3.112622       9.023745   \n",
       "3       5.979588      13.096312        4.459175       9.323495   \n",
       "4       5.154520      13.246133        5.570715      11.376455   \n",
       "\n",
       "   hsa.miR.99b..1       time  status  \n",
       "0        0.499726  17.195448       0  \n",
       "1        4.829052   1.190342       1  \n",
       "2        4.720672   6.049298       1  \n",
       "3        5.122238   8.483713       1  \n",
       "4        6.263673  23.514074       1  \n",
       "\n",
       "[5 rows x 1035 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare data\n",
    "folder = 'linear'\n",
    "keywords = ['moderate', \"latest\", 'RW']\n",
    "\n",
    "train_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, test_size=0.2)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_cols = ['time', 'status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df, val_df = train_test_split(train_df, \n",
    "                                test_size=0.2,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                stratify=train_df['status'])\n",
    "\n",
    "# Transform data\n",
    "covariate_cols = [col for col in train_df.columns if col not in survival_cols]\n",
    "standardize = [([col], StandardScaler()) for col in covariate_cols]\n",
    "leave = [(col, None) for col in survival_cols]\n",
    "x_mapper = DataFrameMapper(standardize)\n",
    "\n",
    "# gene expression data\n",
    "x_train = x_mapper.fit_transform(tr_df[covariate_cols]).astype('float32')\n",
    "x_val = x_mapper.fit_transform(val_df[covariate_cols]).astype('float32')\n",
    "x_test = x_mapper.transform(test_df[covariate_cols]).astype('float32')\n",
    "\n",
    "# prepare labels\n",
    "get_target = lambda df: (df['time'].values, df['status'].values)\n",
    "y_train = get_target(tr_df)\n",
    "y_val = get_target(val_df)\n",
    "t_test, e_test = get_target(test_df)\n",
    "val = x_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net\n",
    "\n",
    "We create a simple MLP with two hidden layers, ReLU activations, batch norm and dropout. \n",
    "Here, we just use the `torchtuples.practical.MLPVanilla` net to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = x_train.shape[1]\n",
    "num_nodes = [32, 16]\n",
    "out_features = 1\n",
    "batch_norm = True\n",
    "dropout = 0.2\n",
    "output_bias = True\n",
    "\n",
    "net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm,\n",
    "                            dropout, output_bias=output_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "To train the model we need to define a `torch.optim` optimizer; here we instead use one from `tt.optim` as it has some added functionality.\n",
    "We use the `Adam` optimizer and set the desired learning rate with `model.lr_finder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tt.optim.Adam(weight_decay=0.01)\n",
    "\n",
    "be_model = CoxPHStratified(net, optimizer)\n",
    "\n",
    "# we  set it manually to 0.001\n",
    "be_model.optimizer.set_lr(1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the `EarlyStopping` callback to stop training when the validation loss stops improving. After training, this callback will also load the best performing model in terms of validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'batch_indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "\n",
      "File \u001b[0;32m~/dl-survival-miRNA/pycox/models/cox.py:509\u001b[0m, in \u001b[0;36mCoxPHStratified.fit\u001b[0;34m(self, input, target, batch_indices, batch_size, epochs, callbacks, verbose, num_workers, shuffle, metrics, val_data, val_batch_size, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`batch_indices` must be provided either at initialization or during fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    510\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    511\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/dl-survival-miRNA/pycox/models/cox.py:54\u001b[0m, in \u001b[0;36m_CoxBase.fit\u001b[0;34m(self, input, target, batch_size, epochs, callbacks, verbose, num_workers, shuffle, metrics, val_data, val_batch_size, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit  model with inputs and targets. Where 'input' is the covariates, and\u001b[39;00m\n",
      "\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m'target' is a tuple with (durations, events).\u001b[39;00m\n",
      "\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    TrainingLogger -- Training log\u001b[39;00m\n",
      "\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_data \u001b[38;5;241m=\u001b[39m tt\u001b[38;5;241m.\u001b[39mtuplefy(\u001b[38;5;28minput\u001b[39m, target)\n",
      "\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     55\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     56\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/base.py:294\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, input, target, batch_size, epochs, callbacks, verbose, num_workers, shuffle, metrics, val_data, val_batch_size, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (is_dl(val_data) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (val_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;32m    291\u001b[0m     val_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataloader(\n",
      "\u001b[1;32m    292\u001b[0m         val_data, val_batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mnum_workers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n",
      "\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;32m--> 294\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log\n",
      "\n",
      "File \u001b[0;32m~/dl-survival-miRNA/pycox/models/cox.py:480\u001b[0m, in \u001b[0;36mCoxPHStratified.fit_dataloader\u001b[0;34m(self, dataloader, epochs, callbacks, verbose, metrics, val_dataloader)\u001b[0m\n",
      "\u001b[1;32m    478\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 480\u001b[0m         stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    481\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_fit_end()\n",
      "\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/callbacks.py:128\u001b[0m, in \u001b[0;36mCallbackHandler.on_epoch_end\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;32m--> 128\u001b[0m     stop_signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stop_signal\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/callbacks.py:97\u001b[0m, in \u001b[0;36mCallbackHandler.apply_callbacks\u001b[0;34m(self, func)\u001b[0m\n",
      "\u001b[1;32m     95\u001b[0m stop_signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues():\n",
      "\u001b[0;32m---> 97\u001b[0m     stop \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     98\u001b[0m     stop \u001b[38;5;241m=\u001b[39m stop \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;32m     99\u001b[0m     stop_signal \u001b[38;5;241m=\u001b[39m stop_signal \u001b[38;5;129;01mor\u001b[39;00m stop\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/callbacks.py:128\u001b[0m, in \u001b[0;36mCallbackHandler.on_epoch_end.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n",
      "\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;32m--> 128\u001b[0m     stop_signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_callbacks(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stop_signal\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/callbacks.py:289\u001b[0m, in \u001b[0;36mMonitorFitMetrics.on_epoch_end\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    287\u001b[0m     scores \u001b[38;5;241m=\u001b[39m {name: np\u001b[38;5;241m.\u001b[39mnan \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mkeys()}\n",
      "\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 289\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_in_batches_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend_score(name, val)\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/base.py:436\u001b[0m, in \u001b[0;36mModel.score_in_batches_dataloader\u001b[0;34m(self, dataloader, score_func, eval_, mean, numpy)\u001b[0m\n",
      "\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n",
      "\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 436\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    438\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n",
      "\u001b[1;32m    439\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_func \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore_func\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m probably doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt work... Not implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    440\u001b[0m         )\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/base.py:182\u001b[0m, in \u001b[0;36mModel.compute_metrics\u001b[0;34m(self, data, metrics)\u001b[0m\n",
      "\u001b[1;32m    180\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;32m    181\u001b[0m out \u001b[38;5;241m=\u001b[39m tuplefy(out)\n",
      "\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {name: metric(\u001b[38;5;241m*\u001b[39mout, \u001b[38;5;241m*\u001b[39mtarget) \u001b[38;5;28;01mfor\u001b[39;00m name, metric \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/base.py:182\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m    180\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;32m    181\u001b[0m out \u001b[38;5;241m=\u001b[39m tuplefy(out)\n",
      "\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {name: \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, metric \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'batch_indices'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 64\n",
    "epochs = 500\n",
    "callbacks = [tt.callbacks.EarlyStopping(patience=20, min_delta=5e-2)]\n",
    "verbose = True\n",
    "\n",
    "batch_indices = np.ones(len(y_train[1]))\n",
    "log = be_model.fit(x_train, y_train,\n",
    "                batch_indices,\n",
    "                batch_size,\n",
    "                epochs,\n",
    "                callbacks, \n",
    "                verbose=verbose,\n",
    "                val_data=val, val_batch_size=batch_size\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
