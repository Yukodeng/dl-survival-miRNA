{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pycox: DeepSurv Stratified by Batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "os.chdir(\"../\")\n",
    "from pycox.models.cox import CoxPH, CoxPHStratified, StratifiedDataset\n",
    "from pycox.evaluation.eval_surv import EvalSurv, EvalSurvStratified\n",
    "from utils import *\n",
    "from runDeepSurvModels import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Test: Network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "def cox_ph_loss_sorted(log_h: Tensor, events: Tensor, eps: float = 1e-7) -> Tensor:\n",
    "    \"\"\"Requires the input to be sorted by descending duration time.\n",
    "    See DatasetDurationSorted.\n",
    "\n",
    "    We calculate the negative log of $(\\frac{h_i}{\\sum_{j \\in R_i} h_j})^d$,\n",
    "    where h = exp(log_h) are the hazards and R is the risk set, and d is event.\n",
    "\n",
    "    We just compute a cumulative sum, and not the true Risk sets. This is a\n",
    "    limitation, but simple and fast.\n",
    "    \"\"\"\n",
    "    if events.dtype is torch.bool:\n",
    "        events = events.float()\n",
    "    events = events.view(-1)\n",
    "    log_h = log_h.view(-1)\n",
    "    gamma = log_h.max()\n",
    "    log_cumsum_h = log_h.sub(gamma).exp().cumsum(0).add(eps).log().add(gamma)\n",
    "    return - log_h.sub(log_cumsum_h).mul(events).sum().div(events.sum())\n",
    "\n",
    "####### [UPDATE] 1105\n",
    "def stratified_cox_ph_loss(log_h: Tensor, durations: Tensor, events: Tensor, batch_indices: Tensor, eps: float = 1e-7) -> Tensor:\n",
    "    \"\"\"\n",
    "    Stratified CoxPH loss that computes partial likelihood across batches.\n",
    "\n",
    "    Arguments:\n",
    "        log_h {torch.Tensor} -- Log hazard predictions for each instance.\n",
    "        durations {torch.Tensor} -- Duration times for each instance.\n",
    "        events {torch.Tensor} -- Event indicators (1 if event, 0 if censored).\n",
    "        batch_indices {numpy array} -- Batch labels for each instance.\n",
    "        eps {float} -- Small epsilon for numerical stability.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor -- The total stratified negative log partial likelihood.\n",
    "    \"\"\"\n",
    "    device = batch_indices.device\n",
    "    unique_batches = torch.unique(batch_indices)\n",
    "    losses = torch.zeros(len(unique_batches), device=device)\n",
    "        \n",
    "    for i, batch in enumerate(unique_batches):\n",
    "        # Select data for the current batch\n",
    "        mask = (batch_indices == batch)\n",
    "        if mask.sum() == 0:\n",
    "            continue  # skip empty batch\n",
    "        \n",
    "        # Sort by descending durations\n",
    "        idx = torch.argsort(durations[mask], descending=True)\n",
    "        # idx = durations[mask].sort(descending=True)[1]\n",
    "        \n",
    "        events_batch = events[mask][idx]\n",
    "        log_h_batch = log_h[mask][idx]\n",
    "        if events_batch.sum() == 0:\n",
    "            continue \n",
    "        \n",
    "        losses[i] = cox_ph_loss_sorted(log_h_batch, events_batch, eps)\n",
    "    \n",
    "    return losses.sum()\n",
    "\n",
    "\n",
    "import torchtuples.callbacks as cb\n",
    "\n",
    "### UPDATE 11/05\n",
    "class CoxPHLossStratified(torch.nn.Module):\n",
    "    \"\"\"Loss for CoxPH model with batch variable.\n",
    "\n",
    "    We calculate the batch-stratified negative log of $(\\frac{h_i}{\\sum_{j \\in R_i} h_j})^d$,\n",
    "    where h = exp(log_h) are the hazards and R is the risk set, and d is event.\n",
    "\n",
    "    We just compute a cumulative sum, and not the true Risk sets. This is a\n",
    "    limitation, but simple and fast.\n",
    "    \"\"\"\n",
    "    # def forward(self, log_h: Tensor, durations: Tensor, events: Tensor, batch_indices: Tensor) -> Tensor:\n",
    "        # return stratified_cox_ph_loss(log_h, durations, events, batch_indices)\n",
    "    def forward(self, log_h: Tensor, durations: Tensor, events: Tensor, batch_indices: Tensor) -> Tensor:\n",
    "        if torch.isnan(log_h).any():\n",
    "            print(\"NaNs detected in log hazards\")\n",
    "        if torch.isnan(durations).any():\n",
    "            print(\"NaNs detected in input survival time\")\n",
    "        if torch.isnan(events).any():\n",
    "            print(\"NaNs detected in input events\")\n",
    "        if (events.sum() == 0).item():\n",
    "            print(\"No observed events in batch (val)\")\n",
    "        return stratified_cox_ph_loss(log_h, durations, events, batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Import the entire base module\n",
    "from pycox.models import base\n",
    "from pycox.models import loss as Loss\n",
    "from pycox.models import utils\n",
    "\n",
    "def search_sorted_idx(array, values):\n",
    "    '''For sorted array, get index of values.\n",
    "    If value not in array, give left index of value.\n",
    "    '''\n",
    "    n = len(array)\n",
    "    idx = np.searchsorted(array, values)\n",
    "    idx[idx == n] = n-1 # We can't have indexes higher than the length-1\n",
    "    not_exact = values != array[idx]\n",
    "    idx -= not_exact\n",
    "    if any(idx < 0):\n",
    "        warnings.warn('Given value smaller than first value')\n",
    "        idx[idx < 0] = 0\n",
    "    return idx\n",
    "\n",
    "\n",
    "class _CoxBase(base.SurvBase):\n",
    "    duration_col = 'duration'\n",
    "    event_col = 'event'\n",
    "\n",
    "    def fit(self, input, target, batch_size=256, epochs=1, callbacks=None, verbose=True,\n",
    "            num_workers=0, shuffle=True, metrics=None, val_data=None, val_batch_size=8224,\n",
    "            **kwargs):\n",
    "        \"\"\"Fit  model with inputs and targets. Where 'input' is the covariates, and\n",
    "        'target' is a tuple with (durations, events).\n",
    "        \n",
    "        Arguments:\n",
    "            input {np.array, tensor or tuple} -- Input x passed to net.\n",
    "            target {np.array, tensor or tuple} -- Target [durations, events]. \n",
    "        \n",
    "        Keyword Arguments:\n",
    "            batch_size {int} -- Elements in each batch (default: {256})\n",
    "            epochs {int} -- Number of epochs (default: {1})\n",
    "            callbacks {list} -- list of callbacks (default: {None})\n",
    "            verbose {bool} -- Print progress (default: {True})\n",
    "            num_workers {int} -- Number of workers used in the dataloader (default: {0})\n",
    "            shuffle {bool} -- If we should shuffle the order of the dataset (default: {True})\n",
    "            **kwargs are passed to 'make_dataloader' method.\n",
    "    \n",
    "        Returns:\n",
    "            TrainingLogger -- Training log\n",
    "        \"\"\"\n",
    "        self.training_data = tt.tuplefy(input, target)\n",
    "        return super().fit(input, target, batch_size, epochs, callbacks, verbose,\n",
    "                           num_workers, shuffle, metrics, val_data, val_batch_size,\n",
    "                           **kwargs)\n",
    "\n",
    "    def _compute_baseline_hazards(self, input, df, max_duration, batch_size, eval_=True, num_workers=0):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def target_to_df(self, target):\n",
    "        durations, events = tt.tuplefy(target).to_numpy()\n",
    "        df = pd.DataFrame({self.duration_col: durations, self.event_col: events}) \n",
    "        return df\n",
    "\n",
    "    def compute_baseline_hazards(self, input=None, target=None, max_duration=None, sample=None, batch_size=8224,\n",
    "                                set_hazards=True, eval_=True, num_workers=0):\n",
    "        \"\"\"Computes the Breslow estimates form the data defined by `input` and `target`\n",
    "        (if `None` use training data).\n",
    "\n",
    "        Typically call\n",
    "        model.compute_baseline_hazards() after fitting.\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            input  -- Input data (train input) (default: {None})\n",
    "            target  -- Target data (train target) (default: {None})\n",
    "            max_duration {float} -- Don't compute estimates for duration higher (default: {None})\n",
    "            sample {float or int} -- Compute estimates of subsample of data (default: {None})\n",
    "            batch_size {int} -- Batch size (default: {8224})\n",
    "            set_hazards {bool} -- Set hazards in model object, or just return hazards. (default: {True})\n",
    "        \n",
    "        Returns:\n",
    "            pd.Series -- Pandas series with baseline hazards. Index is duration_col.\n",
    "        \"\"\"\n",
    "        if (input is None) and (target is None):\n",
    "            if not hasattr(self, 'training_data'):\n",
    "                raise ValueError(\"Need to give a 'input' and 'target' to this function.\")\n",
    "            input, target = self.training_data\n",
    "        df = self.target_to_df(target)#.sort_values(self.duration_col)\n",
    "        if sample is not None:\n",
    "            if sample >= 1:\n",
    "                df = df.sample(n=sample)\n",
    "            else:\n",
    "                df = df.sample(frac=sample)\n",
    "        input = tt.tuplefy(input).to_numpy().iloc[df.index.values]\n",
    "        base_haz = self._compute_baseline_hazards(input, df, max_duration, batch_size,\n",
    "                                                  eval_=eval_, num_workers=num_workers)\n",
    "        if set_hazards:\n",
    "            self.compute_baseline_cumulative_hazards(set_hazards=True, baseline_hazards_=base_haz)\n",
    "        return base_haz\n",
    "\n",
    "    def compute_baseline_cumulative_hazards(self, input=None, target=None, max_duration=None, sample=None,\n",
    "                                            batch_size=8224, set_hazards=True, baseline_hazards_=None,\n",
    "                                            eval_=True, num_workers=0):\n",
    "        \"\"\"See `compute_baseline_hazards. This is the cumulative version.\"\"\"\n",
    "        if ((input is not None) or (target is not None)) and (baseline_hazards_ is not None):\n",
    "            raise ValueError(\"'input', 'target' and 'baseline_hazards_' can not both be different from 'None'.\")\n",
    "        if baseline_hazards_ is None:\n",
    "            baseline_hazards_ = self.compute_baseline_hazards(input, target, max_duration, sample, batch_size,\n",
    "                                                             set_hazards=False, eval_=eval_, num_workers=num_workers)\n",
    "        assert baseline_hazards_.index.is_monotonic_increasing,\\\n",
    "            'Need index of baseline_hazards_ to be monotonic increasing, as it represents time.'\n",
    "        bch = (baseline_hazards_\n",
    "                .cumsum()\n",
    "                .rename('baseline_cumulative_hazards'))\n",
    "        if set_hazards:\n",
    "            self.baseline_hazards_ = baseline_hazards_\n",
    "            self.baseline_cumulative_hazards_ = bch\n",
    "        return bch\n",
    "\n",
    "    def predict_cumulative_hazards(self, input, max_duration=None, batch_size=8224, verbose=False,\n",
    "                                   baseline_hazards_=None, eval_=True, num_workers=0):\n",
    "        \"\"\"See `predict_survival_function`.\"\"\"\n",
    "        if type(input) is pd.DataFrame:\n",
    "            input = self.df_to_input(input)\n",
    "        if baseline_hazards_ is None:\n",
    "            if not hasattr(self, 'baseline_hazards_'):\n",
    "                raise ValueError('Need to compute baseline_hazards_. E.g run `model.compute_baseline_hazards()`')\n",
    "            baseline_hazards_ = self.baseline_hazards_\n",
    "        assert baseline_hazards_.index.is_monotonic_increasing,\\\n",
    "            'Need index of baseline_hazards_ to be monotonic increasing, as it represents time.'\n",
    "        return self._predict_cumulative_hazards(input, max_duration, batch_size, verbose, baseline_hazards_,\n",
    "                                                eval_, num_workers=num_workers)\n",
    "\n",
    "    def _predict_cumulative_hazards(self, input, max_duration, batch_size, verbose, baseline_hazards_,\n",
    "                                    eval_=True, num_workers=0):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict_surv_df(self, input, max_duration=None, batch_size=8224, verbose=False, baseline_hazards_=None,\n",
    "                        eval_=True, num_workers=0):\n",
    "        \"\"\"Predict survival function for `input`. S(x, t) = exp(-H(x, t))\n",
    "        Require computed baseline hazards.\n",
    "\n",
    "        Arguments:\n",
    "            input {np.array, tensor or tuple} -- Input x passed to net.\n",
    "\n",
    "        Keyword Arguments:\n",
    "            max_duration {float} -- Don't compute estimates for duration higher (default: {None})\n",
    "            batch_size {int} -- Batch size (default: {8224})\n",
    "            baseline_hazards_ {pd.Series} -- Baseline hazards. If `None` used `model.baseline_hazards_` (default: {None})\n",
    "            eval_ {bool} -- If 'True', use 'eval' mode on net. (default: {True})\n",
    "            num_workers {int} -- Number of workers in created dataloader (default: {0})\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame -- Survival estimates. One columns for each individual.\n",
    "        \"\"\"\n",
    "        return np.exp(-self.predict_cumulative_hazards(input, max_duration, batch_size, verbose, baseline_hazards_,\n",
    "                                                       eval_, num_workers))\n",
    "\n",
    "    def predict_surv(self, input, max_duration=None, batch_size=8224, numpy=None, verbose=False,\n",
    "                     baseline_hazards_=None, eval_=True, num_workers=0):\n",
    "        \"\"\"Predict survival function for `input`. S(x, t) = exp(-H(x, t))\n",
    "        Require compueted baseline hazards.\n",
    "\n",
    "        Arguments:\n",
    "            input {np.array, tensor or tuple} -- Input x passed to net.\n",
    "\n",
    "        Keyword Arguments:\n",
    "            max_duration {float} -- Don't compute estimates for duration higher (default: {None})\n",
    "            batch_size {int} -- Batch size (default: {8224})\n",
    "            numpy {bool} -- 'False' gives tensor, 'True' gives numpy, and None give same as input\n",
    "                (default: {None})\n",
    "            baseline_hazards_ {pd.Series} -- Baseline hazards. If `None` used `model.baseline_hazards_` (default: {None})\n",
    "            eval_ {bool} -- If 'True', use 'eval' mode on net. (default: {True})\n",
    "            num_workers {int} -- Number of workers in created dataloader (default: {0})\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame -- Survival estimates. One columns for each individual.\n",
    "        \"\"\"\n",
    "        surv = self.predict_surv_df(input, max_duration, batch_size, verbose, baseline_hazards_,\n",
    "                                    eval_, num_workers)\n",
    "        surv = torch.from_numpy(surv.values.transpose())\n",
    "        return tt.utils.array_or_tensor(surv, numpy, input)\n",
    "\n",
    "    def save_net(self, path, **kwargs):\n",
    "        \"\"\"Save self.net and baseline hazards to file.\n",
    "\n",
    "        Arguments:\n",
    "            path {str} -- Path to file.\n",
    "            **kwargs are passed to torch.save\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        path, extension = os.path.splitext(path)\n",
    "        if extension == \"\":\n",
    "            extension = '.pt'\n",
    "        super().save_net(path+extension, **kwargs)\n",
    "        if hasattr(self, 'baseline_hazards_'):\n",
    "            self.baseline_hazards_.to_pickle(path+'_blh.pickle')\n",
    "\n",
    "    def load_net(self, path, **kwargs):\n",
    "        \"\"\"Load net and hazards from file.\n",
    "\n",
    "        Arguments:\n",
    "            path {str} -- Path to file.\n",
    "            **kwargs are passed to torch.load\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        path, extension = os.path.splitext(path)\n",
    "        if extension == \"\":\n",
    "            extension = '.pt'\n",
    "        super().load_net(path+extension, **kwargs)\n",
    "        blh_path = path+'_blh.pickle'\n",
    "        if os.path.isfile(blh_path):\n",
    "            self.baseline_hazards_ = pd.read_pickle(blh_path)\n",
    "            self.baseline_cumulative_hazards_ = self.baseline_hazards_.cumsum()\n",
    "\n",
    "    def df_to_input(self, df):\n",
    "        input = df[self.input_cols].values\n",
    "        return input\n",
    "    \n",
    "\n",
    "class _CoxPHBase(_CoxBase):\n",
    "    def _compute_baseline_hazards(self, input, df_target, max_duration, batch_size, eval_=True, num_workers=0):\n",
    "        if max_duration is None:\n",
    "            max_duration = np.inf\n",
    "\n",
    "        # Here we are computing when expg when there are no events.\n",
    "        #   Could be made faster, by only computing when there are events.\n",
    "        return (df_target\n",
    "                .assign(expg=np.exp(self.predict(input, batch_size, True, eval_, num_workers=num_workers)))\n",
    "                .groupby(self.duration_col)\n",
    "                .agg({'expg': 'sum', self.event_col: 'sum'})\n",
    "                .sort_index(ascending=False)\n",
    "                .assign(expg=lambda x: x['expg'].cumsum())\n",
    "                .pipe(lambda x: x[self.event_col]/x['expg'])\n",
    "                .fillna(0.)\n",
    "                .iloc[::-1]\n",
    "                .loc[lambda x: x.index <= max_duration]\n",
    "                .rename('baseline_hazards'))\n",
    "\n",
    "    def _predict_cumulative_hazards(self, input, max_duration, batch_size, verbose, baseline_hazards_,\n",
    "                                    eval_=True, num_workers=0):\n",
    "        max_duration = np.inf if max_duration is None else max_duration\n",
    "        if baseline_hazards_ is self.baseline_hazards_:\n",
    "            bch = self.baseline_cumulative_hazards_\n",
    "        else:\n",
    "            bch = self.compute_baseline_cumulative_hazards(set_hazards=False, \n",
    "                                                           baseline_hazards_=baseline_hazards_)\n",
    "        bch = bch.loc[lambda x: x.index <= max_duration]\n",
    "        expg = np.exp(self.predict(input, batch_size, True, eval_, num_workers=num_workers)).reshape(1, -1)\n",
    "        return pd.DataFrame(bch.values.reshape(-1, 1).dot(expg), \n",
    "                            index=bch.index)\n",
    "\n",
    "    def partial_log_likelihood(self, input, target, g_preds=None, batch_size=8224, eps=1e-7, eval_=True,\n",
    "                               num_workers=0):\n",
    "        '''Calculate the partial log-likelihood for the events in datafram df.\n",
    "        This likelihood does not sample the controls.\n",
    "        Note that censored data (non events) does not have a partial log-likelihood.\n",
    "\n",
    "        Arguments:\n",
    "            input {tuple, np.ndarray, or torch.tensor} -- Input to net.\n",
    "            target {tuple, np.ndarray, or torch.tensor} -- Target labels.\n",
    "\n",
    "        Keyword Arguments:\n",
    "            g_preds {np.array} -- Predictions from `model.predict` (default: {None})\n",
    "            batch_size {int} -- Batch size (default: {8224})\n",
    "            eval_ {bool} -- If 'True', use 'eval' mode on net. (default: {True})\n",
    "            num_workers {int} -- Number of workers in created dataloader (default: {0})\n",
    "\n",
    "        Returns:\n",
    "            Partial log-likelihood.\n",
    "        '''\n",
    "        df = self.target_to_df(target)\n",
    "        if g_preds is None:\n",
    "            g_preds = self.predict(input, batch_size, True, eval_, num_workers=num_workers)\n",
    "        return (df\n",
    "                .assign(_g_preds=g_preds)\n",
    "                .sort_values(self.duration_col, ascending=False)\n",
    "                .assign(_cum_exp_g=(lambda x: x['_g_preds']\n",
    "                                    .pipe(np.exp)\n",
    "                                    .cumsum()\n",
    "                                    .groupby(x[self.duration_col])\n",
    "                                    .transform('max')))\n",
    "                .loc[lambda x: x[self.event_col] == 1]\n",
    "                .assign(pll=lambda x: x['_g_preds'] - np.log(x['_cum_exp_g'] + eps))\n",
    "                ['pll'])\n",
    "\n",
    "\n",
    "class CoxPH(_CoxPHBase):\n",
    "    \"\"\"Cox proportional hazards model parameterized with a neural net.\n",
    "    This is essentially the DeepSurv method [1].\n",
    "\n",
    "    The loss function is not quite the partial log-likelihood, but close.    \n",
    "    The difference is that for tied events, we use a random order instead of \n",
    "    including all individuals that had an event at that point in time.\n",
    "\n",
    "    Arguments:\n",
    "        net {torch.nn.Module} -- A pytorch net.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        optimizer {torch or torchtuples optimizer} -- Optimizer (default: {None})\n",
    "        device {str, int, torch.device} -- Device to compute on. (default: {None})\n",
    "            Preferably pass a torch.device object.\n",
    "            If 'None': use default gpu if available, else use cpu.\n",
    "            If 'int': used that gpu: torch.device('cuda:<device>').\n",
    "            If 'string': string is passed to torch.device('string').\n",
    "\n",
    "    [1] Jared L. Katzman, Uri Shaham, Alexander Cloninger, Jonathan Bates, Tingting Jiang, and Yuval Kluger.\n",
    "        Deepsurv: personalized treatment recommender system using a Cox proportional hazards deep neural network.\n",
    "        BMC Medical Research Methodology, 18(1), 2018.\n",
    "        https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0482-1\n",
    "    \"\"\"\n",
    "    def __init__(self, net, optimizer=None, device=None, loss=None):\n",
    "        if loss is None:\n",
    "            loss = Loss.CoxPHLoss()\n",
    "        super().__init__(net, loss, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Update 7/1/2025\n",
    "class StratifiedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, durations, events, batch_ids):\n",
    "        self.x = x\n",
    "        self.durations = durations\n",
    "        self.events = events\n",
    "        self.batch_ids = batch_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.durations[idx], self.events[idx], self.batch_ids[idx]\n",
    "\n",
    "\n",
    "### UPDATE 11/05/2024\n",
    "class CoxPHStratified(_CoxPHBase):\n",
    "    \"\"\"Cox proportional hazards model parameterized with a neural net.\n",
    "\n",
    "    The loss function is not quite the partial log-likelihood, but close.    \n",
    "    The difference is that for we stratify events by batch (strata) when\n",
    "    calculating partial log likelihood.\n",
    "\n",
    "    Arguments:\n",
    "        net {torch.nn.Module} -- A pytorch net.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        optimizer {torch or torchtuples optimizer} -- Optimizer (default: {None})\n",
    "        device {str, int, torch.device} -- Device to compute on. (default: {None})\n",
    "            Preferably pass a torch.device object.\n",
    "            If 'None': use default gpu if available, else use cpu.\n",
    "        loss {function} -- Loss function to use, default is stratified_cox_ph_loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, net, optimizer=None, device=None, loss=None):\n",
    "        self.batch_ids = None\n",
    "        if loss is None:\n",
    "            # loss = Loss.CoxPHLossStratified()\n",
    "            loss = CoxPHLossStratified()\n",
    "        super().__init__(net, loss, optimizer, device)\n",
    "        \n",
    "    def compute_metrics(self, data, metrics=None):\n",
    "        if metrics is None:\n",
    "            metrics = self.metrics\n",
    "        if self.loss is None and self.loss in metrics.values():\n",
    "            raise RuntimeError(\"Need to set `self.loss`.\")\n",
    "        x, durations, events, batch_ids = data\n",
    "        log_h = self.net(x)\n",
    "        \n",
    "        if torch.isnan(log_h).any():\n",
    "            print(\"NaNs detected in log_h during compute_metrics()\")\n",
    "            return {name: float('nan') for name in metrics}\n",
    "        \n",
    "        return {name: metric(log_h, durations, events, batch_ids) for name, metric in metrics.items()}\n",
    "    \n",
    "    def fit_dataloader(self, dataloader, epochs=1, callbacks=None, verbose=True, metrics=None, val_dataloader=None):\n",
    "        \"\"\"\n",
    "        Custom training loop for CoxPHStratified that reads (x, duration, event, batch_id) from DataLoader.\n",
    "        \n",
    "        Args:\n",
    "            dataloader (DataLoader): training data loader returning 4-tuples.\n",
    "            epochs (int): number of training epochs.\n",
    "            callbacks (list): optional callbacks.\n",
    "            verbose (bool): print training progress.\n",
    "            metrics (dict): optional metrics.\n",
    "            val_dataloader (DataLoader): optional validation dataloader (can be normal 2-tuple).\n",
    "        \n",
    "        Returns:\n",
    "            TrainingLogger\n",
    "        \"\"\"\n",
    "        self._setup_train_info(dataloader)\n",
    "        self.metrics = self._setup_metrics(metrics)\n",
    "        self.log.verbose = verbose\n",
    "        self.val_metrics.dataloader = val_dataloader\n",
    "        \n",
    "        if callbacks is None:\n",
    "            callbacks = []\n",
    "        self.callbacks = cb.TrainingCallbackHandler(\n",
    "            self.optimizer, self.train_metrics, self.log, self.val_metrics, callbacks\n",
    "        )\n",
    "        self.callbacks.give_model(self)\n",
    "\n",
    "        stop = self.callbacks.on_fit_start()\n",
    "        for _ in range(epochs):\n",
    "            if stop:\n",
    "                break\n",
    "            stop = self.callbacks.on_epoch_start()\n",
    "            if stop:\n",
    "                break\n",
    "            \n",
    "            for x, durations, events, batch_ids in dataloader:\n",
    "                stop = self.callbacks.on_batch_start()\n",
    "                if stop:\n",
    "                    break\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                log_h = self.net(x)\n",
    "                loss = self.loss(log_h, durations, events, batch_ids)\n",
    "                self.batch_loss = loss\n",
    "                self.batch_metrics = {\"loss\": loss}\n",
    "                \n",
    "                loss.backward()\n",
    "                stop = self.callbacks.before_step()\n",
    "                if stop:\n",
    "                    break\n",
    "                self.optimizer.step()\n",
    "                stop = self.callbacks.on_batch_end()\n",
    "                if stop:\n",
    "                    break\n",
    "            else:\n",
    "                stop = self.callbacks.on_epoch_end()\n",
    "        self.callbacks.on_fit_end()\n",
    "        \n",
    "        return self.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Test: Debugging*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n",
      "batch 1 has no events\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1.])\n",
      "tensor([1., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5826)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PyTorch tensor\n",
    "batch_indices = torch.tensor([1, 2, 2, 2, 2, 2, 3, 3, 4, 4], dtype=torch.float32)\n",
    "durations = torch.tensor([169.5, 0.6, 12.3, 1.5, 3.8, 0.1, 0.1, 0.1, 0.6, 0.1], dtype=torch.float32)\n",
    "events = torch.tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.float32)\n",
    "log_h = torch.tensor([-4.1238, 2.1188, -1.5863, -1.2239, 0.9088, 5.6637, 1.2920, 4.5356, 1.5392, 5.0004], dtype=torch.float32)\n",
    "\n",
    "# Test out ufnction\n",
    "device = batch_indices.device\n",
    "unique_batches = torch.unique(batch_indices)\n",
    "losses = torch.zeros(len(unique_batches), device=device)\n",
    "\n",
    "for i, batch in enumerate(unique_batches):\n",
    "    # i = 1\n",
    "    # batch = 1\n",
    "    # print(i)\n",
    "    mask = (batch_indices == batch)\n",
    "    if mask.sum() == 0:\n",
    "        print(f\"batch {batch} is empty\")\n",
    "        continue\n",
    "    idx = torch.argsort(durations[mask], descending=True)\n",
    "    # idx = durations[mask].sort(descending=True)[1]\n",
    "    log_h_batch = log_h[mask][idx]\n",
    "    events_batch = events[mask][idx]\n",
    "    \n",
    "    print(events_batch)\n",
    "    if events_batch.sum() == 0:\n",
    "        print(f\"batch {int(batch)} has no events\")\n",
    "        continue\n",
    "    \n",
    "    losses[i] = cox_ph_loss_sorted(log_h_batch, events_batch, eps=1e-7)\n",
    "    \n",
    "losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5826)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratified_cox_ph_loss(log_h, durations, events, batch_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dimensions: (90000, 541)\n",
      "Testing data dimensions:  (10000, 541)\n"
     ]
    }
   ],
   "source": [
    "batchNormType='BE00Asso00_normNone'\n",
    "dataType = 'linear-moderate'\n",
    "keywords = ['061825']\n",
    "test_size=10000\n",
    "random_state=42\n",
    "time_col='time'\n",
    "status_col='status'\n",
    "batch_col='batch.id'\n",
    "\n",
    "train_df, test_df = load_simulate_survival_data(batchNormType=batchNormType,\n",
    "                                                dataName=dataType,\n",
    "                                                keywords=keywords, \n",
    "                                                keep_batch=True)\n",
    "\n",
    "print(f\"Training data dimensions: {train_df.shape}\")\n",
    "print(f\"Testing data dimensions:  {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_data(df, mapper=None, fit_scaler=True):\n",
    "    survival_cols = [time_col, status_col]\n",
    "    covariate_cols = [col for col in df.columns if col not in survival_cols]\n",
    "    \n",
    "    if fit_scaler or mapper is None:\n",
    "        standardize = [([col], StandardScaler()) for col in covariate_cols]\n",
    "        mapper = DataFrameMapper(standardize)\n",
    "        # Transform features (miRNA expression)\n",
    "        x = mapper.fit_transform(df[covariate_cols]).astype('float32')\n",
    "    else:\n",
    "        x = mapper.transform(df[covariate_cols]).astype('float32')\n",
    "    \n",
    "    # Prepare labels (survival data)\n",
    "    y = (df[time_col].values, df[status_col].values)\n",
    "    \n",
    "    return x, y, mapper\n",
    "\n",
    "train_sub,_ = train_test_split(train_df,\n",
    "                            train_size=1000, \n",
    "                            shuffle=True, random_state=42,\n",
    "                            stratify=train_df[[status_col, batch_col]])\n",
    "\n",
    "test_sub, _ = train_test_split(test_df,\n",
    "                            train_size=1000, \n",
    "                            shuffle=True, random_state=42,\n",
    "                            stratify=test_df[[status_col, batch_col]])\n",
    "\n",
    "batch_ids_train = train_sub[batch_col].to_numpy().reshape(-1)\n",
    "batch_ids_test = test_sub[[batch_col]].to_numpy().reshape(-1)\n",
    "\n",
    "train_sub = train_sub.drop(columns=[batch_col])\n",
    "test_sub = test_sub.drop(columns=[batch_col])\n",
    "\n",
    "x_train, y_train, mapper = _preprocess_data(train_sub, fit_scaler=True)\n",
    "x_test, y_test, _ = _preprocess_data(test_sub, mapper=mapper, fit_scaler=False)\n",
    "\n",
    "durations_train, events_train = y_train[0], y_train[1]\n",
    "durations_test, events_test = y_test[0], y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 538])\n",
      "torch.Size([1000, 2])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2743192/3116171740.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  y_train = torch.tensor(y_train).transpose(0,1).to(device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = torch.from_numpy(x_train).to(device)\n",
    "x_test = torch.from_numpy(x_test).to(device)\n",
    "y_train = torch.tensor(y_train).transpose(0,1).to(device)\n",
    "y_test = torch.tensor(y_test).transpose(0,1).to(device)\n",
    "\n",
    "durations_train = torch.from_numpy(durations_train).float().to(device)\n",
    "durations_test = torch.from_numpy(durations_test).float().to(device)\n",
    "events_train = torch.from_numpy(events_train).float().to(device)\n",
    "events_test = torch.from_numpy(events_test).float().to(device)\n",
    "\n",
    "batch_ids_train = torch.from_numpy(batch_ids_train).long().to(device)\n",
    "batch_ids_test = torch.from_numpy(batch_ids_test).long().to(device)\n",
    "\n",
    "print(x_test.shape)           # Should be [n_samples, n_features]\n",
    "print(y_test.shape)           # Should be [n_samples, n_features]\n",
    "print(durations_test.shape)   # Should be [n_samples]\n",
    "print(events_test.shape)      # Should be [n_samples]\n",
    "print(batch_ids_test.shape)   # Should be [n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.shape\n",
    "input_size = x_train.shape[1]\n",
    "output_size = 1\n",
    "num_nodes = [32]            # Default # layers & nodes\n",
    "dropout = 0.3                    # Default dropout rate\n",
    "learning_rate = 1e-4      # Default learning rate\n",
    "batch_size = 128               # Default batch size\n",
    "epochs = 100                      # Default number of epochs\n",
    "batch_norm = True             # Default batch normalization\n",
    "output_bias = True           # Default output bias\n",
    "weight_decay = 1e-4         # Default weight decay\n",
    "activation = torch.nn.ReLU\n",
    "\n",
    "net = tt.practical.MLPVanilla(\n",
    "    in_features=input_size,\n",
    "    out_features=output_size,\n",
    "    num_nodes=num_nodes,\n",
    "    dropout=dropout, \n",
    "    batch_norm=batch_norm,\n",
    "    activation=activation,\n",
    "    output_bias=output_bias\n",
    ").to(device)\n",
    "optimizer = tt.optim.Adam(weight_decay=weight_decay, lr=learning_rate)\n",
    "\n",
    "# Get default early stopping settings if not defined \n",
    "patience = 40\n",
    "min_delta = 1e-4\n",
    "callbacks = [tt.callbacks.EarlyStopping(patience=patience, min_delta=min_delta)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoxPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 3.8831,\tval_loss: 3.5626\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 3.5485,\tval_loss: 3.4992\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 3.4668,\tval_loss: 3.4378\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 3.3699,\tval_loss: 3.4001\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 3.2844,\tval_loss: 3.3809\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 3.2342,\tval_loss: 3.3483\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 3.1337,\tval_loss: 3.3225\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 3.0652,\tval_loss: 3.3020\n",
      "8:\t[0s / 0s],\t\ttrain_loss: 3.0529,\tval_loss: 3.2784\n",
      "9:\t[0s / 0s],\t\ttrain_loss: 2.9835,\tval_loss: 3.2556\n",
      "10:\t[0s / 0s],\t\ttrain_loss: 2.9500,\tval_loss: 3.2556\n",
      "11:\t[0s / 0s],\t\ttrain_loss: 2.9056,\tval_loss: 3.2324\n",
      "12:\t[0s / 0s],\t\ttrain_loss: 2.8455,\tval_loss: 3.2226\n",
      "13:\t[0s / 0s],\t\ttrain_loss: 2.8475,\tval_loss: 3.2211\n",
      "14:\t[0s / 0s],\t\ttrain_loss: 2.8370,\tval_loss: 3.2256\n",
      "15:\t[0s / 0s],\t\ttrain_loss: 2.7594,\tval_loss: 3.1857\n",
      "16:\t[0s / 0s],\t\ttrain_loss: 2.7236,\tval_loss: 3.1786\n",
      "17:\t[0s / 0s],\t\ttrain_loss: 2.7048,\tval_loss: 3.1772\n",
      "18:\t[0s / 0s],\t\ttrain_loss: 2.6561,\tval_loss: 3.1447\n",
      "19:\t[0s / 0s],\t\ttrain_loss: 2.6691,\tval_loss: 3.1363\n",
      "20:\t[0s / 0s],\t\ttrain_loss: 2.6558,\tval_loss: 3.1173\n",
      "21:\t[0s / 0s],\t\ttrain_loss: 2.6235,\tval_loss: 3.1367\n",
      "22:\t[0s / 0s],\t\ttrain_loss: 2.5972,\tval_loss: 3.1255\n",
      "23:\t[0s / 0s],\t\ttrain_loss: 2.5623,\tval_loss: 3.1227\n",
      "24:\t[0s / 0s],\t\ttrain_loss: 2.5936,\tval_loss: 3.1280\n",
      "25:\t[0s / 0s],\t\ttrain_loss: 2.4922,\tval_loss: 3.1197\n",
      "26:\t[0s / 0s],\t\ttrain_loss: 2.5496,\tval_loss: 3.1470\n",
      "27:\t[0s / 0s],\t\ttrain_loss: 2.4681,\tval_loss: 3.1201\n",
      "28:\t[0s / 0s],\t\ttrain_loss: 2.4541,\tval_loss: 3.1286\n",
      "29:\t[0s / 0s],\t\ttrain_loss: 2.4949,\tval_loss: 3.0957\n",
      "30:\t[0s / 0s],\t\ttrain_loss: 2.4841,\tval_loss: 3.0828\n",
      "31:\t[0s / 0s],\t\ttrain_loss: 2.4563,\tval_loss: 3.0653\n",
      "32:\t[0s / 0s],\t\ttrain_loss: 2.4461,\tval_loss: 3.0665\n",
      "33:\t[0s / 0s],\t\ttrain_loss: 2.3953,\tval_loss: 3.0680\n",
      "34:\t[0s / 0s],\t\ttrain_loss: 2.3988,\tval_loss: 3.0728\n",
      "35:\t[0s / 0s],\t\ttrain_loss: 2.4184,\tval_loss: 3.0803\n",
      "36:\t[0s / 0s],\t\ttrain_loss: 2.4066,\tval_loss: 3.0938\n",
      "37:\t[0s / 0s],\t\ttrain_loss: 2.3929,\tval_loss: 3.1025\n",
      "38:\t[0s / 0s],\t\ttrain_loss: 2.3966,\tval_loss: 3.1210\n",
      "39:\t[0s / 0s],\t\ttrain_loss: 2.4161,\tval_loss: 3.0673\n",
      "40:\t[0s / 1s],\t\ttrain_loss: 2.4161,\tval_loss: 3.0426\n",
      "41:\t[0s / 1s],\t\ttrain_loss: 2.3623,\tval_loss: 3.0267\n",
      "42:\t[0s / 1s],\t\ttrain_loss: 2.3203,\tval_loss: 3.0188\n",
      "43:\t[0s / 1s],\t\ttrain_loss: 2.3133,\tval_loss: 3.0296\n",
      "44:\t[0s / 1s],\t\ttrain_loss: 2.3549,\tval_loss: 3.0410\n",
      "45:\t[0s / 1s],\t\ttrain_loss: 2.3114,\tval_loss: 3.0260\n",
      "46:\t[0s / 1s],\t\ttrain_loss: 2.3007,\tval_loss: 3.0515\n",
      "47:\t[0s / 1s],\t\ttrain_loss: 2.3096,\tval_loss: 3.0948\n",
      "48:\t[0s / 1s],\t\ttrain_loss: 2.3048,\tval_loss: 3.0444\n",
      "49:\t[0s / 1s],\t\ttrain_loss: 2.2675,\tval_loss: 3.0576\n",
      "50:\t[0s / 1s],\t\ttrain_loss: 2.2567,\tval_loss: 3.1379\n",
      "51:\t[0s / 1s],\t\ttrain_loss: 2.3051,\tval_loss: 3.0825\n",
      "52:\t[0s / 1s],\t\ttrain_loss: 2.2646,\tval_loss: 3.0807\n",
      "53:\t[0s / 1s],\t\ttrain_loss: 2.2792,\tval_loss: 3.0283\n",
      "54:\t[0s / 1s],\t\ttrain_loss: 2.2696,\tval_loss: 3.0539\n",
      "55:\t[0s / 1s],\t\ttrain_loss: 2.2511,\tval_loss: 3.0976\n",
      "56:\t[0s / 1s],\t\ttrain_loss: 2.2512,\tval_loss: 3.0266\n",
      "57:\t[0s / 1s],\t\ttrain_loss: 2.2877,\tval_loss: 3.0144\n",
      "58:\t[0s / 1s],\t\ttrain_loss: 2.2200,\tval_loss: 3.0661\n",
      "59:\t[0s / 1s],\t\ttrain_loss: 2.2527,\tval_loss: 3.0409\n",
      "60:\t[0s / 1s],\t\ttrain_loss: 2.2064,\tval_loss: 3.0387\n",
      "61:\t[0s / 1s],\t\ttrain_loss: 2.2417,\tval_loss: 3.0033\n",
      "62:\t[0s / 1s],\t\ttrain_loss: 2.2186,\tval_loss: 3.0173\n",
      "63:\t[0s / 1s],\t\ttrain_loss: 2.2688,\tval_loss: 3.0045\n",
      "64:\t[0s / 1s],\t\ttrain_loss: 2.2328,\tval_loss: 3.0418\n",
      "65:\t[0s / 1s],\t\ttrain_loss: 2.2513,\tval_loss: 3.0114\n",
      "66:\t[0s / 1s],\t\ttrain_loss: 2.2664,\tval_loss: 3.0565\n",
      "67:\t[0s / 1s],\t\ttrain_loss: 2.2224,\tval_loss: 3.0222\n",
      "68:\t[0s / 1s],\t\ttrain_loss: 2.2069,\tval_loss: 3.0724\n",
      "69:\t[0s / 1s],\t\ttrain_loss: 2.1924,\tval_loss: 3.0402\n",
      "70:\t[0s / 1s],\t\ttrain_loss: 2.1852,\tval_loss: 3.0294\n",
      "71:\t[0s / 1s],\t\ttrain_loss: 2.2061,\tval_loss: 3.0251\n",
      "72:\t[0s / 1s],\t\ttrain_loss: 2.1576,\tval_loss: 3.0908\n",
      "73:\t[0s / 1s],\t\ttrain_loss: 2.1882,\tval_loss: 3.0884\n",
      "74:\t[0s / 1s],\t\ttrain_loss: 2.1473,\tval_loss: 3.1167\n",
      "75:\t[0s / 1s],\t\ttrain_loss: 2.1719,\tval_loss: 3.0515\n",
      "76:\t[0s / 1s],\t\ttrain_loss: 2.2015,\tval_loss: 3.1033\n",
      "77:\t[0s / 1s],\t\ttrain_loss: 2.1665,\tval_loss: 3.0069\n",
      "78:\t[0s / 1s],\t\ttrain_loss: 2.1687,\tval_loss: 3.0090\n",
      "79:\t[0s / 1s],\t\ttrain_loss: 2.1909,\tval_loss: 3.0195\n",
      "80:\t[0s / 1s],\t\ttrain_loss: 2.1871,\tval_loss: 3.0419\n",
      "81:\t[0s / 1s],\t\ttrain_loss: 2.1677,\tval_loss: 3.0540\n",
      "82:\t[0s / 1s],\t\ttrain_loss: 2.1505,\tval_loss: 3.0897\n",
      "83:\t[0s / 1s],\t\ttrain_loss: 2.1460,\tval_loss: 3.0520\n",
      "84:\t[0s / 1s],\t\ttrain_loss: 2.1617,\tval_loss: 3.1044\n",
      "85:\t[0s / 1s],\t\ttrain_loss: 2.1770,\tval_loss: 3.1042\n",
      "86:\t[0s / 1s],\t\ttrain_loss: 2.1632,\tval_loss: 3.0408\n",
      "87:\t[0s / 1s],\t\ttrain_loss: 2.1136,\tval_loss: 3.0309\n",
      "88:\t[0s / 1s],\t\ttrain_loss: 2.0933,\tval_loss: 3.0413\n",
      "89:\t[0s / 1s],\t\ttrain_loss: 2.1256,\tval_loss: 3.0595\n",
      "90:\t[0s / 1s],\t\ttrain_loss: 2.0886,\tval_loss: 3.0586\n",
      "91:\t[0s / 2s],\t\ttrain_loss: 2.1142,\tval_loss: 3.0799\n"
     ]
    }
   ],
   "source": [
    "# CoxPH model\n",
    "model = CoxPH(net, optimizer=optimizer)\n",
    "log = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks, \n",
    "    verbose=True,\n",
    "    val_data=(x_test, y_test),\n",
    "    val_batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9502699643045979, 0.8175973395539272)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================== Evaluation ====================\n",
    "_ = model.compute_baseline_hazards()\n",
    "\n",
    "# # Convert torch tensors back to numpy objects for evaluation\n",
    "# x_train_np = x_train.detach().cpu().numpy()\n",
    "# x_test_np = x_test.detach().cpu().numpy()\n",
    "\n",
    "# durations_train_np = durations_train.detach().cpu().numpy()\n",
    "# events_train_np    = events_train.detach().cpu().numpy()\n",
    "# durations_val_np   = durations_test.detach().cpu().numpy()\n",
    "# events_val_np      = events_test.detach().cpu().numpy()\n",
    "\n",
    "# Initialize EvalSurv objects \n",
    "tr_surv  = model.predict_surv_df(x_train)\n",
    "te_surv = model.predict_surv_df(x_test)\n",
    "\n",
    "tr_ev = EvalSurv(tr_surv, durations_train, events_train, censor_surv='km')\n",
    "te_ev = EvalSurv(te_surv, durations_test, events_test, censor_surv='km')\n",
    "\n",
    "# Concordance index ----------------\n",
    "tr_c_index = tr_ev.concordance_td() \n",
    "te_c_index = te_ev.concordance_td() \n",
    "tr_c_index, te_c_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified CoxPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = StratifiedDataset(x_train, durations_train, events_train, batch_ids_train)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_dataset = torch.utils.data.TensorDataset(x_test, durations_test, events_test, batch_ids_test)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # # Access batches\n",
    "# # for idx, (inputs, durations, events, batch_ids) in enumerate(train_loader):\n",
    "# #     print(f\"Batch {idx + 1}:\")\n",
    "# #     print(\"batch ids:\", batch_ids)\n",
    "# #     # print(\"Time:\", durations)\n",
    "# #     print(\"Events:\", events)\n",
    "# #     print()\n",
    "    \n",
    "# ## Test\n",
    "# for idx, (inputs, durations, events, batch_ids) in enumerate(test_loader):\n",
    "#     print(f\"Batch {idx + 1}:\")\n",
    "#     print(\"batch ids:\", batch_ids)\n",
    "#     # print(\"Time:\", durations)\n",
    "#     print(\"Events:\", events)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StratifiedDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get default early stopping settings if not defined \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# patience = 20\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# min_delta = 0\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# callbacks = [tt.callbacks.EarlyStopping(patience=patience, min_delta=min_delta)]\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mStratifiedDataset\u001b[49m(x_train, durations_train, events_train, batch_ids_train)\n\u001b[1;32m      7\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# test_dataset = torch.utils.data.TensorDataset(x_test, durations_test, events_test, batch_ids_test)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StratifiedDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Get default early stopping settings if not defined \n",
    "# patience = 20\n",
    "# min_delta = 0\n",
    "# callbacks = [tt.callbacks.EarlyStopping(patience=patience, min_delta=min_delta)]\n",
    "\n",
    "train_dataset = StratifiedDataset(x_train, durations_train, events_train, batch_ids_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_dataset = torch.utils.data.TensorDataset(x_test, durations_test, events_test, batch_ids_test)\n",
    "test_dataset = StratifiedDataset(x_test, durations_test, events_test, batch_ids_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Stratified CoxPH model\n",
    "model = CoxPHStratified(net, optimizer=optimizer)\n",
    "model.metrics = {'val_loss': model.loss}\n",
    "start = time.time() # Record iteration start time\n",
    "log = model.fit_dataloader(\n",
    "    train_loader,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=True,\n",
    "    val_dataloader=test_loader  # optional for now\n",
    ")\n",
    "stop = time.time() # Record time when training finished\n",
    "duration = round(stop - start, 2)\n",
    "print(f\"Training time: {duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8718146735088339, 0.7258435801802039)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================== Evaluation ====================\n",
    "_ = model.compute_baseline_hazards(input=x_train, target=(durations_train, events_train))\n",
    "\n",
    "# Convert torch tensors back to numpy objects for evaluation\n",
    "# x_train = x_train.detach().cpu().numpy()\n",
    "# x_test  = x_test.detach().cpu().numpy()\n",
    "# durations_train = durations_train.detach().cpu().numpy()\n",
    "# durations_test  = durations_test.detach().cpu().numpy()\n",
    "# events_train    = events_train.detach().cpu().numpy()\n",
    "# events_test     = events_test.detach().cpu().numpy()\n",
    "\n",
    "# Initialize EvalSurv objects \n",
    "tr_surv  = model.predict_surv_df(x_train)\n",
    "te_surv = model.predict_surv_df(x_test)\n",
    "tr_ev = EvalSurv(tr_surv, durations_train, events_train, censor_surv='km')\n",
    "te_ev = EvalSurv(te_surv, durations_test, events_test, censor_surv='km')\n",
    "\n",
    "# Concordance index ----------------\n",
    "tr_c_index  = tr_ev.concordance_td() \n",
    "te_c_index = te_ev.concordance_td() \n",
    "\n",
    "tr_c_index, te_c_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== Archive ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hsa.let.7a.2..1</th>\n",
       "      <th>hsa.let.7a.3..1</th>\n",
       "      <th>hsa.let.7a..2..1</th>\n",
       "      <th>hsa.let.7b.1</th>\n",
       "      <th>hsa.let.7b..1</th>\n",
       "      <th>hsa.let.7c.1</th>\n",
       "      <th>hsa.let.7c..1</th>\n",
       "      <th>hsa.let.7d.1</th>\n",
       "      <th>hsa.let.7d..1</th>\n",
       "      <th>hsa.let.7e.1</th>\n",
       "      <th>...</th>\n",
       "      <th>hsa.miR.96.1</th>\n",
       "      <th>hsa.miR.96..1</th>\n",
       "      <th>hsa.miR.98.1</th>\n",
       "      <th>hsa.miR.98..1</th>\n",
       "      <th>hsa.miR.99a.1</th>\n",
       "      <th>hsa.miR.99a..1</th>\n",
       "      <th>hsa.miR.99b.1</th>\n",
       "      <th>hsa.miR.99b..1</th>\n",
       "      <th>time</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.161142</td>\n",
       "      <td>16.139914</td>\n",
       "      <td>9.440727</td>\n",
       "      <td>12.377491</td>\n",
       "      <td>5.524234</td>\n",
       "      <td>11.493406</td>\n",
       "      <td>3.315916</td>\n",
       "      <td>13.211118</td>\n",
       "      <td>4.830156</td>\n",
       "      <td>10.275969</td>\n",
       "      <td>...</td>\n",
       "      <td>4.311983</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>11.930738</td>\n",
       "      <td>6.893816</td>\n",
       "      <td>9.471225</td>\n",
       "      <td>1.244932</td>\n",
       "      <td>1.820734</td>\n",
       "      <td>0.499726</td>\n",
       "      <td>17.195448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.737665</td>\n",
       "      <td>15.622746</td>\n",
       "      <td>7.350143</td>\n",
       "      <td>12.387646</td>\n",
       "      <td>4.147150</td>\n",
       "      <td>12.739532</td>\n",
       "      <td>3.693167</td>\n",
       "      <td>11.653065</td>\n",
       "      <td>6.348644</td>\n",
       "      <td>10.511325</td>\n",
       "      <td>...</td>\n",
       "      <td>5.211017</td>\n",
       "      <td>0.010937</td>\n",
       "      <td>10.018970</td>\n",
       "      <td>4.504616</td>\n",
       "      <td>12.851019</td>\n",
       "      <td>4.458570</td>\n",
       "      <td>9.536961</td>\n",
       "      <td>4.829052</td>\n",
       "      <td>1.190342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.137077</td>\n",
       "      <td>15.838051</td>\n",
       "      <td>7.474055</td>\n",
       "      <td>14.062855</td>\n",
       "      <td>5.124704</td>\n",
       "      <td>13.175972</td>\n",
       "      <td>3.549938</td>\n",
       "      <td>12.224586</td>\n",
       "      <td>5.851290</td>\n",
       "      <td>10.720255</td>\n",
       "      <td>...</td>\n",
       "      <td>6.218995</td>\n",
       "      <td>0.023063</td>\n",
       "      <td>11.032040</td>\n",
       "      <td>5.361192</td>\n",
       "      <td>12.062099</td>\n",
       "      <td>3.112622</td>\n",
       "      <td>9.023745</td>\n",
       "      <td>4.720672</td>\n",
       "      <td>6.049298</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.152887</td>\n",
       "      <td>18.113934</td>\n",
       "      <td>7.656181</td>\n",
       "      <td>12.879390</td>\n",
       "      <td>4.768000</td>\n",
       "      <td>13.147153</td>\n",
       "      <td>3.518407</td>\n",
       "      <td>12.940474</td>\n",
       "      <td>6.602133</td>\n",
       "      <td>11.160047</td>\n",
       "      <td>...</td>\n",
       "      <td>3.165273</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>11.539787</td>\n",
       "      <td>5.979588</td>\n",
       "      <td>13.096312</td>\n",
       "      <td>4.459175</td>\n",
       "      <td>9.323495</td>\n",
       "      <td>5.122238</td>\n",
       "      <td>8.483713</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.068358</td>\n",
       "      <td>14.510637</td>\n",
       "      <td>7.261556</td>\n",
       "      <td>11.505635</td>\n",
       "      <td>5.917256</td>\n",
       "      <td>12.665515</td>\n",
       "      <td>4.810049</td>\n",
       "      <td>11.739157</td>\n",
       "      <td>6.542291</td>\n",
       "      <td>11.056379</td>\n",
       "      <td>...</td>\n",
       "      <td>17.231062</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>10.274817</td>\n",
       "      <td>5.154520</td>\n",
       "      <td>13.246133</td>\n",
       "      <td>5.570715</td>\n",
       "      <td>11.376455</td>\n",
       "      <td>6.263673</td>\n",
       "      <td>23.514074</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1035 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hsa.let.7a.2..1  hsa.let.7a.3..1  hsa.let.7a..2..1  hsa.let.7b.1  \\\n",
       "0         0.161142        16.139914          9.440727     12.377491   \n",
       "1         0.737665        15.622746          7.350143     12.387646   \n",
       "2         1.137077        15.838051          7.474055     14.062855   \n",
       "3         2.152887        18.113934          7.656181     12.879390   \n",
       "4         3.068358        14.510637          7.261556     11.505635   \n",
       "\n",
       "   hsa.let.7b..1  hsa.let.7c.1  hsa.let.7c..1  hsa.let.7d.1  hsa.let.7d..1  \\\n",
       "0       5.524234     11.493406       3.315916     13.211118       4.830156   \n",
       "1       4.147150     12.739532       3.693167     11.653065       6.348644   \n",
       "2       5.124704     13.175972       3.549938     12.224586       5.851290   \n",
       "3       4.768000     13.147153       3.518407     12.940474       6.602133   \n",
       "4       5.917256     12.665515       4.810049     11.739157       6.542291   \n",
       "\n",
       "   hsa.let.7e.1  ...  hsa.miR.96.1  hsa.miR.96..1  hsa.miR.98.1  \\\n",
       "0     10.275969  ...      4.311983       0.000551     11.930738   \n",
       "1     10.511325  ...      5.211017       0.010937     10.018970   \n",
       "2     10.720255  ...      6.218995       0.023063     11.032040   \n",
       "3     11.160047  ...      3.165273       0.015713     11.539787   \n",
       "4     11.056379  ...     17.231062       0.005042     10.274817   \n",
       "\n",
       "   hsa.miR.98..1  hsa.miR.99a.1  hsa.miR.99a..1  hsa.miR.99b.1  \\\n",
       "0       6.893816       9.471225        1.244932       1.820734   \n",
       "1       4.504616      12.851019        4.458570       9.536961   \n",
       "2       5.361192      12.062099        3.112622       9.023745   \n",
       "3       5.979588      13.096312        4.459175       9.323495   \n",
       "4       5.154520      13.246133        5.570715      11.376455   \n",
       "\n",
       "   hsa.miR.99b..1       time  status  \n",
       "0        0.499726  17.195448       0  \n",
       "1        4.829052   1.190342       1  \n",
       "2        4.720672   6.049298       1  \n",
       "3        5.122238   8.483713       1  \n",
       "4        6.263673  23.514074       1  \n",
       "\n",
       "[5 rows x 1035 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare data\n",
    "folder = 'linear'\n",
    "keywords = ['moderate', \"latest\", 'RW']\n",
    "\n",
    "train_df, test_df = load_simulate_survival_data(folder=folder, keywords=keywords, test_size=0.2)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_cols = ['time', 'status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df, val_df = train_test_split(train_df, \n",
    "                                test_size=0.2,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                stratify=train_df['status'])\n",
    "\n",
    "# Transform data\n",
    "covariate_cols = [col for col in train_df.columns if col not in survival_cols]\n",
    "standardize = [([col], StandardScaler()) for col in covariate_cols]\n",
    "leave = [(col, None) for col in survival_cols]\n",
    "x_mapper = DataFrameMapper(standardize)\n",
    "\n",
    "# gene expression data\n",
    "x_train = x_mapper.fit_transform(tr_df[covariate_cols]).astype('float32')\n",
    "x_val = x_mapper.fit_transform(val_df[covariate_cols]).astype('float32')\n",
    "x_test = x_mapper.transform(test_df[covariate_cols]).astype('float32')\n",
    "\n",
    "# prepare labels\n",
    "get_target = lambda df: (df['time'].values, df['status'].values)\n",
    "y_train = get_target(tr_df)\n",
    "y_val = get_target(val_df)\n",
    "t_test, e_test = get_target(test_df)\n",
    "val = x_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net\n",
    "\n",
    "We create a simple MLP with two hidden layers, ReLU activations, batch norm and dropout. \n",
    "Here, we just use the `torchtuples.practical.MLPVanilla` net to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = x_train.shape[1]\n",
    "num_nodes = [32, 16]\n",
    "out_features = 1\n",
    "batch_norm = True\n",
    "dropout = 0.2\n",
    "output_bias = True\n",
    "\n",
    "net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm,\n",
    "                            dropout, output_bias=output_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "To train the model we need to define a `torch.optim` optimizer; here we instead use one from `tt.optim` as it has some added functionality.\n",
    "We use the `Adam` optimizer and set the desired learning rate with `model.lr_finder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tt.optim.Adam(weight_decay=0.01)\n",
    "\n",
    "be_model = CoxPHStratified(net, optimizer)\n",
    "\n",
    "# we  set it manually to 0.001\n",
    "be_model.optimizer.set_lr(1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the `EarlyStopping` callback to stop training when the validation loss stops improving. After training, this callback will also load the best performing model in terms of validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'batch_indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "\n",
      "File \u001b[0;32m~/dl-survival-miRNA/pycox/models/cox.py:509\u001b[0m, in \u001b[0;36mCoxPHStratified.fit\u001b[0;34m(self, input, target, batch_indices, batch_size, epochs, callbacks, verbose, num_workers, shuffle, metrics, val_data, val_batch_size, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`batch_indices` must be provided either at initialization or during fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    510\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    511\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/dl-survival-miRNA/pycox/models/cox.py:54\u001b[0m, in \u001b[0;36m_CoxBase.fit\u001b[0;34m(self, input, target, batch_size, epochs, callbacks, verbose, num_workers, shuffle, metrics, val_data, val_batch_size, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit  model with inputs and targets. Where 'input' is the covariates, and\u001b[39;00m\n",
      "\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m'target' is a tuple with (durations, events).\u001b[39;00m\n",
      "\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    TrainingLogger -- Training log\u001b[39;00m\n",
      "\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_data \u001b[38;5;241m=\u001b[39m tt\u001b[38;5;241m.\u001b[39mtuplefy(\u001b[38;5;28minput\u001b[39m, target)\n",
      "\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     55\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     56\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/base.py:294\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, input, target, batch_size, epochs, callbacks, verbose, num_workers, shuffle, metrics, val_data, val_batch_size, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (is_dl(val_data) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (val_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;32m    291\u001b[0m     val_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataloader(\n",
      "\u001b[1;32m    292\u001b[0m         val_data, val_batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mnum_workers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n",
      "\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;32m--> 294\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log\n",
      "\n",
      "File \u001b[0;32m~/dl-survival-miRNA/pycox/models/cox.py:480\u001b[0m, in \u001b[0;36mCoxPHStratified.fit_dataloader\u001b[0;34m(self, dataloader, epochs, callbacks, verbose, metrics, val_dataloader)\u001b[0m\n",
      "\u001b[1;32m    478\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 480\u001b[0m         stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    481\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_fit_end()\n",
      "\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/callbacks.py:128\u001b[0m, in \u001b[0;36mCallbackHandler.on_epoch_end\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;32m--> 128\u001b[0m     stop_signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stop_signal\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/callbacks.py:97\u001b[0m, in \u001b[0;36mCallbackHandler.apply_callbacks\u001b[0;34m(self, func)\u001b[0m\n",
      "\u001b[1;32m     95\u001b[0m stop_signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues():\n",
      "\u001b[0;32m---> 97\u001b[0m     stop \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     98\u001b[0m     stop \u001b[38;5;241m=\u001b[39m stop \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;32m     99\u001b[0m     stop_signal \u001b[38;5;241m=\u001b[39m stop_signal \u001b[38;5;129;01mor\u001b[39;00m stop\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/callbacks.py:128\u001b[0m, in \u001b[0;36mCallbackHandler.on_epoch_end.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n",
      "\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;32m--> 128\u001b[0m     stop_signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_callbacks(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stop_signal\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/callbacks.py:289\u001b[0m, in \u001b[0;36mMonitorFitMetrics.on_epoch_end\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    287\u001b[0m     scores \u001b[38;5;241m=\u001b[39m {name: np\u001b[38;5;241m.\u001b[39mnan \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mkeys()}\n",
      "\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 289\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_in_batches_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend_score(name, val)\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/base.py:436\u001b[0m, in \u001b[0;36mModel.score_in_batches_dataloader\u001b[0;34m(self, dataloader, score_func, eval_, mean, numpy)\u001b[0m\n",
      "\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n",
      "\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 436\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    438\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n",
      "\u001b[1;32m    439\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_func \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore_func\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m probably doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt work... Not implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    440\u001b[0m         )\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/base.py:182\u001b[0m, in \u001b[0;36mModel.compute_metrics\u001b[0;34m(self, data, metrics)\u001b[0m\n",
      "\u001b[1;32m    180\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;32m    181\u001b[0m out \u001b[38;5;241m=\u001b[39m tuplefy(out)\n",
      "\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {name: metric(\u001b[38;5;241m*\u001b[39mout, \u001b[38;5;241m*\u001b[39mtarget) \u001b[38;5;28;01mfor\u001b[39;00m name, metric \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torchtuples/base.py:182\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m    180\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;32m    181\u001b[0m out \u001b[38;5;241m=\u001b[39m tuplefy(out)\n",
      "\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {name: \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, metric \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/dl-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'batch_indices'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 64\n",
    "epochs = 500\n",
    "callbacks = [tt.callbacks.EarlyStopping(patience=20, min_delta=5e-2)]\n",
    "verbose = True\n",
    "\n",
    "batch_indices = np.ones(len(y_train[1]))\n",
    "log = be_model.fit(x_train, y_train,\n",
    "                batch_indices,\n",
    "                batch_size,\n",
    "                epochs,\n",
    "                callbacks, \n",
    "                verbose=verbose,\n",
    "                val_data=val, val_batch_size=batch_size\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-surv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
